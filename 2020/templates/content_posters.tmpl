
<div class="content-container">
   <h1>
         North East Database Day 2020 <br />
         <small>Monday January 27th, 2020</small><br/>
         <small>Samberg Conference Center at MIT</small>
   </h1>
   <p>The poster session will include drinks and appetizers. It will be held in the Samberg Conference Center at MIT.</p>
   <h3>Information for Poster Presenters</h3>
   <p>We will be able to provide poster board, easels, and mounting supplies for you.
   You will need to print your poster and bring it with you to the conference. We recommend
   posters to be either A0, A1, or ANSI D or E sizes (either 24" by 32" or 36" by 42"). Our
   poster boards are large enough to accommodate any of these (4' x 6'). You may orient your
   poster as you see fit. If you prefer to print out individual 8.5" x 11" pages, our board
   will be large enough to accommodate about 12 pages. If you prefer a single poster and
   you don't have access to a large format printer, Fedex/Kinkos can print your poster for you.
   </p>
   <h3>List of accepted posters:</h3>

<table class="table table-bordered posterTable">
<tbody>
<tr>
    <td>
    Anat Kleiman (Brandeis University)
    <br/>
    <strong>
    Shared Reads in Predicting Well-Behaved Query Combinations
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po1')">Toggle Abstract</a>
    <div id="po1" class="abstract" style="display: none;">
    <p>
    Systems can optimize for efficiency by selecting sets of queries that run faster concurrently compared to sequentially. As such, query pair selection is a vital component of a database management system.    In the past, the important problem of query combination selection has predominantly been approached with opaque machine learning techniques that require huge amounts of training data, or complex, hand-turned models. Many problems exist with these approaches, the largest being that they are generally difficult to maintain. Furthermore, these approaches are overfitted to a limited amount of scenarios, and cannot be used on a general scale. As such, an overarching solution to the problem of selecting queries to be run concurrently has not been found yet.  Our experimental study of this problem revealed that a simple observation that we believe can generalize a solution to this issue. Using two datasets, TPC-DS (synthetic), and JOB/IMDB (real world), we have found that the run-speed of concurrent queries is inversely proportional to shared reads. In other words, queries that run faster concurrently tend to also share more read data. In turn, they are able to make more cache hits when run together. As such, a huge predictor of whether a query mix is efficient or not is how many reads that mix shares.  Based on our observation, we propose a system that predicts the quality of a query mix by estimating their shared reads. In our work we show that a simple approach based on set-similarity is somewhat predictive, and can work to optimize query-pair selection to concurrently run. Going forward, we hope to implement an ML-driven prediction strategy to increase and maximize predictivity.  
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Andy Vidan (Composable Analytics, Inc.)
    <br/>
    <strong>
    Composable DataOps: Overcoming Challenges Moving Data and Analytics Products to Production
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po2')">Toggle Abstract</a>
    <div id="po2" class="abstract" style="display: none;">
    <p>
    The productionalization of data-driven products requires cross-functional teams of data professionals, ranging from data engineers and software developers to data scientists, IT operations and business leaders, to work collaboratively and unify around a set of best practices and tools for “Data OPerationS” or DataOps. Where DevOps allows organizations to modernize their approach to software development and deployment, DataOps establishes a modern framework for development and deployment of data-driven products. In this talk, we introduce the Composable DataOps Platform, and describe its implementation.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Peter Griggs (MIT- CSAIL)
    <br/>
    <strong>
    Indexing Methods for Large Scale Interactive Data Visualization
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po3')">Toggle Abstract</a>
    <div id="po3" class="abstract" style="display: none;">
    <p>
    Interactive visualization systems need to have low latency user interactions to maintain the user’s attention, but this can be difficult when the dataset being visualized is very large. While it is common for web-based data visualization tools like D3.js to store the entire dataset in memory, large datasets must be stored in a database. Kyrix is a system for creating pan-and-zoom visualizations that uses a database to store visualization objects as spatial data. As a user interacts with the visualization, the frontend dynamically fetches data from the database and renders it using D3.js. This architecture pushes the performance bottleneck from browser to database level. In a purely browser visualization system, the bottleneck comes from rendering the dataset. However, in a scalable visualization system, the bottleneck is fetching data from the database, which relies on spatial indexing. Typically, a 2D rtree is used for indexing bounding boxes. Due to the requirements of interactive visualization systems, 2D rtrees are not necessarily the best option. We compare the performance of using 2D rtrees, 3D rtrees, and quadtrees as indexes. This ongoing research aims to show that 3D rtrees are beneficial for zoom interactions and that quadtree-based indexes might handle highly-skewed data better than 2D rtrees. Additionally, we explore optimizations including spatial partitioning, row compression, and pre-sorting data in z-order to improve index scalability. As the datasets that data scientists want to interact with grow, interactive visualization systems need to scale to handle these needs. The results of this research will help these systems scale to handle changing workloads and help data scientists make sense of this data.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Andy Vidan (Composable Analytics, Inc.)
    <br/>
    <strong>
    Building End-to-end Scalable Machine Learning Data Pipelines
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po4')">Toggle Abstract</a>
    <div id="po4" class="abstract" style="display: none;">
    <p>
    Developing truly scalable, end-to-end machine learning data pipelines is challenging. In this talk, we begin by describing flow-based programming and its ability to facilitate distributed data and query integration. We then describe our use of flow-based programming to define end-to-end data pipelines with embedded analytics and machine learning. Patterns and reference architectures for these pipelines are presented. Finally, we demonstrate our approach by reviewing several successful implementations from real-world business use cases.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Jeremy Kepner (MIT Lincoln Laboratory)
    <br/>
    <strong>
    75,000,000,000 Streaming Inserts/Second Using Hierarchical Hypersparse GraphBLAS Matrices
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po5')">Toggle Abstract</a>
    <div id="po5" class="abstract" style="display: none;">
    <p>
    The SuiteSparse GraphBLAS C-library implements high performance hypersparse matrices with bindings to a variety of languages (Python, Julia, and Matlab/Octave). GraphBLAS provides a lightweight in-memory database implementation of hypersparse matrices that are ideal for analyzing many types of network data, while providing rigorous mathematical guarantees, such as linearity. Streaming updates of hypersparse matrices put enormous pressure on the memory hierarchy. This work benchmarks an implementation of hierarchical hypersparse matrices that reduces memory pressure and dramatically increases the update rate into a hypersparse matrices. The parameters of hierarchical hypersparse matrices rely on controlling the number of entries in each level in the hierarchy before an update is cascaded. The parameters are easily tunable to achieve optimal performance for a variety of applications. Hierarchical hypersparse matrices achieve over 1,000,000 updates per second in a single instance. Scaling to 31,000 instances of hierarchical hypersparse matrices arrays on 1,100 server nodes on the MIT SuperCloud achieved a sustained update rate of 75,000,000,000 updates per second. This capability allows the MIT SuperCloud to analyze extremely large streaming network data sets.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Yi Lu (MIT), Xiangyao Yu (University of Wisconsin, Madison), Samuel Madden (MIT)
    <br/>
    <strong>
    STAR: Scaling Transactions through Asymmetric Replication
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po6')">Toggle Abstract</a>
    <div id="po6" class="abstract" style="display: none;">
    <p>
    In this poster, we present STAR, a new distributed in-memory database with asymmetric replication. By employing a single- node non-partitioned architecture for some replicas and a partitioned architecture for other replicas, STAR is able to efficiently run both highly partitionable workloads and work- loads that involve cross-partition transactions. The key idea is a new phase-switching algorithm where the execution of single-partition and cross-partition transactions is separated. In the partitioned phase, single-partition transactions are run on multiple machines in parallel to exploit more concurrency. In the single-master phase, mastership for the entire data- base is switched to a single designated master node, which can execute these transactions without the use of expen- sive coordination protocols like two-phase commit. Because the master node has a full copy of the database, this phase- switching can be done at negligible cost. Our experiments on two popular benchmarks (YCSB and TPC-C) show that high availability via replication can coexist with fast serializable transaction execution in distributed in-memory databases, with STAR outperforming systems that employ conventional concurrency control and replication algorithms by up to one order of magnitude.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Jiyuan Zhang (CMU), Daniele Giuseppe Spampinato (Carnegie Mellon University), Franz Franchetti (Carnegie Mellon University)
    <br/>
    <strong>
    FESIA: A Fast and SIMD-Efficient Set Intersection Approach on Modern CPUs
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po7')">Toggle Abstract</a>
    <div id="po7" class="abstract" style="display: none;">
    <p>
    Set intersection is an important operation and widely used in both database and graph analytics applications. However, existing state-of-the-art set intersection methods only consider the size of input sets and fail to optimize for the case in which the intersection size is small. In real-world scenarios, the size of most intersections is usually orders of magnitude smaller than the size of the input sets, e.g., keyword search in databases and common neighbor search in graph analytics. In this paper, we present FESIA, a new set intersection approach on modern CPUs. The time complexity of our approach is $O(n/\sqrt{w} + r)$, in which $w$ is the SIMD width, and $n$ and $r$ are the size of input sets and intersection size, respectively. The key idea behind FESIA is that it first uses bitmaps to filter out unmatched elements from the input sets, and then selects suitable specialized kernels at runtime to compute the final intersection on each pair of bitmap segments. In addition, all data structures in FESIA are designed to take advantage of SIMD instructions provided by vector ISAs with various SIMD widths, including SSE, AVX, and the latest AVX-512. Our experiments on both real-world and synthetic datasets show that our intersection method achieves more than a order of magnitude better performance than conventional scalar implementations, and up to 4x better performance than state-of-the-art SIMD implementations.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Cen Wang (University of Massachusetts Amherst)
    <br/>
    <strong>
    NIM: GENERATIVE NEURAL NETWORKS FOR SIMULATION INPUT MODELING
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po8')">Toggle Abstract</a>
    <div id="po8" class="abstract" style="display: none;">
    <p>
    Neural Input Modeling (NIM) is a novel generative-neural-network framework that exploits modern data-rich environments to automatically capture complex simulation input distributions and then generate samples from them. Experiments show that our prototype architecture, which uses a variational autoencoder with LSTM components, can accurately and automatically capture complex stochastic processes. Outputs from queuing simulations based on known and learned inputs, respectively, are also statistically close.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Leonhard Spiegelberg (Brown University), Tim Kraska (MIT), Malte Schwarzkopf (Brown University)
    <br/>
    <strong>
    Tuplex
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po9')">Toggle Abstract</a>
    <div id="po9" class="abstract" style="display: none;">
    <p>
    Tuplex is a new big data framework which allows to execute python code at native speeds together with a robust exception handling model.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Malte Sandstede (RelationalAI), John Liagouris (Boston University), Vasiliki Kalavri (Boston University)
    <br/>
    <strong>
    ST2: Analyzing Performance Issues in Distributed Dataflows
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po11')">Toggle Abstract</a>
    <div id="po11" class="abstract" style="display: none;">
    <p>
    Large-scale data processing systems such as Apache Spark, Flink, and Tensorflow, employ a dataflow programming model and target distributed architectures. A dataflow job consists of workers executing tasks in parallel and communicating via messages over a network. Understanding the performance of such complex and dynamic systems is challenging. Traditional profiling tools like gprof, measure aggregate time per activity but fail to capture computation dependencies. Bottleneck detection methods for data analytics, such as blocked time analysis, operate offline and are only applicable to batch execution. In this poster, we present ST2, a tool that analyzes distributed dataflow traces at scale, operates online, and provides feedback to running applications in real-time. We leverage the fact that dataflow instrumentation can model any interaction between system components and that processing instrumentation traces can itself be expressed as a streaming computation.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Ji Zhang (Huazhong University of Science and Technology), Kun Wan (Huazhong University of Science and Technology), Sebastian Schelter (New York University), ke zhou (HUST)
    <br/>
    <strong>
    Exploring Monte Carlo Tree Search for Join Order Selection
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po12')">Toggle Abstract</a>
    <div id="po12" class="abstract" style="display: none;">
    <p>
    Query optimization remains a difficult problem, and existing database management systems (DBMSs) often miss good execution plans. Identifying an efficient join order is key to achieving good performance in database systems. A primary challenge in join order selection is enumerating a set of candidate orderings and identifying the most effective ordering. Searching in larger candidate spaces increases the potential of finding well working plans, but also increases the cost of query optimization. In this paper, we explore the benefits of Monte Carlo Tree Search (MCTS) for the join order selection problem. 
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Nikolaos Tziavelis (Northeastern University), Deepak Ajwani (University College, Dublin), Wolfgang Gatterbauer (Northeastern University), Mirek Riedewald (Northeastern University), Xiaofeng Yang (Northeastern University)
    <br/>
    <strong>
    Optimal Algorithms for Ranked Enumeration of Answers to Full Conjunctive Queries
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po13')">Toggle Abstract</a>
    <div id="po13" class="abstract" style="display: none;">
    <p>
    We study ranked enumeration of the results to a join query in order of decreasing importance, as imposed by an appropriate ranking function. Our main contribution is a framework for ranked enumeration over a class of Dynamic Programming problems that generalizes seemingly diﬀerent problems that to date had been studied in isolation. To this end, we study and extend classic algorithms that ﬁnd the k-shortest paths in a weighted graph. For full conjunctive queries, including cyclic ones, our approach is asymptotically optimal in terms of (1) time to return the top result, (2) delay between results, and (3) time until all results are returned in order. These optimality properties were derived for a complexity measure that has been widely used in the context of worst-case optimal join algorithms, but which abstracts away dependency on query size and a polylogarithmic cost factor in input size. By performing a more detailed cost analysis, we are able to uncover a previously unknown tradeoﬀ between two incomparable ranked enumeration approaches: one has lower complexity for small k, the other for large k. We demonstrate theoretically and empirically the superiority of our techniques to batch algorithms that produce the full result and then sort it. Interestingly, our technique is not only faster for returning the ﬁrst few results, but even when all results are produced.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Andy Huynh (Boston University), Manos Athanassoulis (Boston University)
    <br/>
    <strong>
    Tuning Data Systems Under Uncertainty in Resources and Workload
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po14')">Toggle Abstract</a>
    <div id="po14" class="abstract" style="display: none;">
    <p>
    Tuning databases for applications with hybrid workloads is a difficult and complex problem. Designers often rely on pre-existing tools in conjunction with workload and hardware knowledge to tune a database correctly. However, the increasing demands for resources has caused databases to migrate to the cloud. Due to virtualization and co-locating of applications in the cloud, designers often face uncertainty in resource availability. Additionally these applications may present ad hoc workloads, or deviate slightly from previous workload knowledge. This introduces potential performance cliffs due to workload drift. In this work we both explore how these uncertainties affect performance and propose a design paradigm that combats these uncertainties. Our design paradigm formulates tuning as a robust optimization problem which allows designers to trade off between performance and robustness to noise. We demonstrate our design in OakDB, a column-oriented database that exposes a robustness knob to the designer. 
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Ju Hyoung Mun (Boston University), Zichen Zhu (Boston University), Aneesh Raman (Boston University), Manos Athanassoulis (Boston University)
    <br/>
    <strong>
    Reducing hashing overhead of Bloom filters in LSM-trees
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po15')">Toggle Abstract</a>
    <div id="po15" class="abstract" style="display: none;">
    <p>
    A log-structured merge-tree (LSM-tree) is a key-value store that balances read performance and ingestion rate employing out-of-place updates. LSM-trees are  widely adopted as the storage backend by a range of systems including LevelDB and BigTable at Google, and RocksDB at Facebook. In order to improve the performance of point queries, LSM-trees often maintain in main memory Bloom filters which are used to avoid unnecessary accesses to durable storage. In practice, the Bloom filters help avoid the vast percentage of unnecessary disk reads, which is particularly beneficial when a large portion of the queries are expected to return an empty result set. Using Bloom filters, however, comes at a cost. In particular, a Bloom filter first hashes the queried value and then probes the corresponding bits. When compared with accessing the disk hashing and probing bits in memory is orders of magnitude faster, hence, Bloom filters are always beneficial.  Contrary to common perception, Bloom filters are not always beneficial. The rationale behind using Bloom filters is that there is a vast difference between the cost of accessing Bloom filters and the data. As the gap of accessing time between the Bloom filter and the data narrows, the advantage of using Bloom filter weakens.  In order to alleviate the hashing overhead, we propose to share the hash calculation between multiple Bloom filter queries under the same key and re-use it by bit-manipulation across the tree hierarchy. Furthermore, by sharing the hash calculations, the indices of multiple Bloom filters along with different hierarchy are acquired in advance. Using this property, we also propose to prefetch the necessary Bloom filter sections to improve the lookup performance further. The preliminary simulation result shows that the proposed scheme can lower the cost for the hash calculation by up to 75% for empty queries to an LSM-tree with four levels.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Jeff Fried (InterSystems)
    <br/>
    <strong>
    Multi-model databases: current state and future trends
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po17')">Toggle Abstract</a>
    <div id="po17" class="abstract" style="display: none;">
    <p>
    Relational, columnar, object, XML, and graph databases are flourishing, but many applications need all of these for different capabilities.    A variety of multi-model databases are now available, but practices for applying these vary, and the variety can be confusing.  This talk covers the range of options, and considers the pros and cons, what is possible, what is best for performance, and what is practical.    We'll explore the current state of technology and adoption as well as the future trends.   We'll go deep on InterSystems' multimodel implementations as an example.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Mengyuan Sun (MIT), Joana M. F. da Trindade (MIT), Samuel Madden (MIT), Julian Shun (MIT), Nesime Tatbul (Intel Labs and MIT)
    <br/>
    <strong>
    In-memory Graph Partitioning for Efficient Temporal Graph Analytics on NVRAM
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po19')">Toggle Abstract</a>
    <div id="po19" class="abstract" style="display: none;">
    <p>
    Routing EMS responders in busy city streets, airplanes in congested sky highways, and server requests on a network datacenter are tasks crucial to the daily operation of large-scale infrastructure services. These tasks can be naturally modeled as temporal paths over temporal graphs, i.e., edges have a time interval to denote their validity. Temporal graphs are increasingly ubiquitous (e.g., interaction networks from various domains, such as protein-protein interactions, data center traffic networks, and spatio-temporal dynamic networks). Due to their dynamic nature, they can also be orders of magnitude larger than their atemporal counterparts. A key challenge in making a temporal path query efficient over these graphs is in ensuring that the algorithm’s execution respects the application's constraints (e.g., monotonically increasing start times in a “earliest arrival time” temporal path), while still providing good cache-locality. In the TGDB project, so far we have designed efficient parallel temporal graph algorithms that operate over specialized in-memory data structures to provide better cache-locality than traditional parallel graph processing indexes. The TGDB framework, however, assumes that the entire graph fits in the available DRAM, which is not the case for temporal graphs where the user would like to pose queries over both recent and historical data. Here we explore alternative memory and storage designs for these queries. Specifically, we developed a NVRAM-aware in-memory graph partitioning scheme optimized for performance of temporal graph analytics workloads. Our system employs a novel algorithm that considers workload access patterns to devise optimal placement for in-memory graph data structures onto DRAM and NVRAM. We show that, when compared to a baseline that stores the graph entirely onto NVRAM, we are able to obtain reasonable speedups on workloads that include both synthetic and real-world temporal graph data.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Priya Deshpande (DePaul University), Alexander Rasin (DePaul University)
    <br/>
    <strong>
    Exploring Fine-Grained Access to Bio-medical Data using FAIR Principles
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po20')">Toggle Abstract</a>
    <div id="po20" class="abstract" style="display: none;">
    <p>
    As more data is created in healthcare domain, integration of electronic health records, pathology reports, and radiology teaching files is becoming a priority.  Digitization of health care data raises challenges in the biomedical domain, ranging from data generation to data management and data analysis. For the purposes of biomedical data analysis, researchers and doctors need to be able to to find relevant data and to integrate data sources. This outcome of data analysis over integrated data repositories will have many applications in diagnostic process, biomedical research, epidemiology, and education. However, these data is located at different hospitals or research institutes; each institution has their own security protocols, formats, and policies, making data integration a challenge. Integrating these data sources and providing a unified repository for doctors and researchers would be a great contribution towards growth of the medical research and drug industry. Biomedical data custodians expect guarantees with regard to data security and compliance with access policies in order to consider sharing their data. Researchers want access to data but they have to perform searches over geographically scattered data and only small amount data is available to authorized users for further analysis. It is currently difficult for researchers to find and access relevant data sources; gaining such access often requires a custom and involved registration process with the current owners of the data.  In this proposed work, we focus on integration of heterogeneous biomedical data sources, designing a workflow for researchers to share and reuse data with fine-grained access control. Our proposed system -- Data integration and indexing System (DiiS) will provide data source discovery and make data available for reproducibility. We plan to follow FAIR (Findable, Accessible, Interoperable, and Reusable) principles to deploy the DiiS framework. 
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Prashanth Menon (Carnegie Mellon Universiy), Andrew Pavlo (Carnegie Mellon University), Todd Mowry (Carnegie Mellon University)
    <br/>
    <strong>
    Regret-Free Compilation: An Optimization Framework for Recompiling SQL Queries on the Fly
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po21')">Toggle Abstract</a>
    <div id="po21" class="abstract" style="display: none;">
    <p>
    A popular technique to improve the performance of modern in-memory relational database management systems (RDBMS) is to compile SQL queries directly into machine code. This just-in-time compilation (JIT) removes all interpretation overhead and allows the DBMS to specialize both access methods and intermediate data structures such as hash tables. However, the compiler is at the mercy of the query plan optimizer: a bad plan will produce bad code. Moreover, compilation time is non-negligible, meaning that stopping a query to recompile a potentially better plan is expensive and often prohibitive.  To overcome the static nature of code-generation based RDBMS, we present a query compilation framework tailored for in-memory RDBMS that supports dynamic recompilation. Instead of compiling the whole query, the RDBMS uses domain knowledge to cleverly structure the program to make recompiling fragments easier. The execution engine instruments these fragments and can determine whether to optimize and recompile them at runtime.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Suman Karumuri (Slack), Franco Solleza (Brown University), Stan Zdonik (Brown University), Nesime Tatbul (Intel Labs and MIT)
    <br/>
    <strong>
    The Case for a Polystore Architecture for Observability Data Management
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po221')">Toggle Abstract</a>
    <div id="po221" class="abstract" style="display: none;">
    <p>
    With rapid advances in distributed computing technologies, software systems and applications are increasingly becoming more distributed, heterogeneous, and complex, making it challenging to predict their behavior in the face of failures and varying load. Observability has been gaining importance as a key capability for monitoring and maintaining cloud-native systems to ensure their quality of service to customers. Borrowed from control theory, the notion of observability aims at bringing better visibility into testing, debugging, and understanding the general behavior of software based on telemetry data collected from the internals of the system as it operates. Beyond simple black-box monitoring of known scenarios, observability requires gaining deeper contextual information and insight about operational semantics of systems for root-cause analysis of unforeseen problems. As such, providing observability is inherently a data-intensive, time-sensitive, and iterative process that also involves humans in the loop (e.g., DevOps teams). While time series is the core data type in this space, there are interconnected data categories that differ in their characteristics and needs. These include Metrics, Events, Logs, and Traces (MELT). Large volumes of these heterogeneous time series should be collected, stored, and indexed from instrumented systems in a way that enables their low-latency search, analysis, and visualization in real time and on demand. In this poster, we will present real-world experience with observability data and its use cases at Slack, a cloud-based workplace communication service with more than 10M daily active users. We hypothesize that a polystore architecture that unifies common lifecycle management issues of MELT data into a single infrastructure while supporting the varying requirements of each data type using a pluggable storage and query engine interface is the best way to address these requirements in a scalable manner.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Anil Shanbhag (MIT)
    <br/>
    <strong>
    Running Data Analytics Efficiently on GPUs
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po231')">Toggle Abstract</a>
    <div id="po231" class="abstract" style="display: none;">
    <p>
    There has been significant amount of excitement and recent work on GPU-based database systems. Previous work has claimed that these systems can perform orders of magnitude better than CPU-based database systems on analytical workloads such as those found in decision support and business intelligence applications. A hardware expert would view these claims with suspicion.  First, the empirical nature of past work, which often compares different code bases from different developers, makes it hard to assess the true nature of performance gains from  GPUs vs implementation artifacts. Second, given the general notion that database operators are memory-bandwidth bound, one would expect the maximum gain to be roughly equal to the ratio of the memory bandwidth of GPU to that of CPU.  In this paper, we adopt a model-based approach to understand when and why the performance gains of running queries on GPUs vs on CPUs varies from the bandwidth ratio (which is roughly 16$\times$ on modern hardware). We propose Crystal, a library of parallel routines that can be combined together to run full SQL queries on a GPU with minimal materialization overhead. We implement individual query operators to show that while the speedups for selection, projection, and sorts are near the bandwidth ratio, joins  achieve less speedup due to differences in hardware capabilities. Interestingly, we show on a popular analytical workload that full query performance gain from running on GPU exceeds the bandwidth ratio despite individual operators having speedup less than bandwidth ratio, as a result of limitations of vectorizing chained operators on CPUs, resulting in a 25$\times$ speedup for GPUs over CPUs on the benchmark. Finally, we do a cost/performance analysis to show that GPUs can be around 4$\times$ more cost-effective when the workload  fits into GPU memory.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Alan Piciacchio (IBM)
    <br/>
    <strong>
    Data Science & Engineering - Best Practices for Curation
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po241')">Toggle Abstract</a>
    <div id="po241" class="abstract" style="display: none;">
    <p>
    It has been said that "data is our new natural resource."  Often, the burgeoning data scientist will be tempted to use advanced techniques to process data. However - the first critical step in adding value to data is to validate and refine it. These tasks, which may seem burdensome, are necessary and mandatory, and if not addressed appropriately, can send businesses in the wrong direction, and can cost $10s of millions of dollars.  In the presentation, the basic critical factors of data curation will be reviewed, including:   - Guaranteeing complete, unfiltered data sets - Selective removal of data - Addressing errant data tactically and at its root  - Provisions for data maintenance - including feeds and linkage structures - Inclusion of agile principles  Several case studies will be reviewed, in which the basic principles were applied. In these case studies, the applied principles resulted in significant increases in revenue and reductions in operating costs. These case studies include the management of client requests, root-cause analysis of IT outages, and building server systems in an enterprise IT environment.   It is expected that the presentation will help audience members gain a better understanding of the data science profession. 
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Haneen Mohammed (Columbia University)
    <br/>
    <strong>
    Continuous Prefetch for Interactive Data Applications
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po251')">Toggle Abstract</a>
    <div id="po251" class="abstract" style="display: none;">
    <p>
    Interactive data visualization and exploration (DVE) applications are often network-bottlenecked due to bursty request patterns, large response sizes, and heterogeneous deployments over a range of networks and devices. This makes it difficult to ensure consistently low response times (<100ms).  Khameleon is a framework for DVE applications that uses a novel combination of prefetching and response tuning to dynamically trade-off response quality for low latency by combining progressive visualizations and resource-aware scheduling. Khameleon exploits DVE's approximation tolerance: immediate lower-quality responses are preferable to waiting for complete results. Our preliminary results show that, across a wide range of network and client resource conditions, Khameleon outperforms classic prefetching approaches that even use perfect prediction models.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Wilson Qin (Harvard), Stratos Idreos (Harvard)
    <br/>
    <strong>
    Navigating the CPU vs Data Movement Tradeoff for NoSQL Key-Value Stores
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po261')">Toggle Abstract</a>
    <div id="po261" class="abstract" style="display: none;">
    <p>
    For the past several decades research on data systems design focuses on minimizing data movement costs to improve throughput and response times. Moving data is much more expensive than computing and as such the focus on minimizing data movement is justified. However, in the past decade the growth in CPU instruction capacity per cycle has gone up on the order of 50%, even though storage and memory have scaled magnitudes more in capacity, memory especially has faced slower gains in latency acceleration (e.g. only on the order of 25% for DRAM). This economic trend means that data is increasingly resident in the now cheap memory (not just as a cache for disk), creating a new challenge as memory access becomes more of a critical path.  We show several new insights on CPU and Data Movement tradeoffs. While modern hardware decouples waiting for I/O from the CPU instruction flow, ultimately the wait on memory bandwidth creates ready available CPU cycles that can be further harnessed for more data processing. This creates a growing opening to utilize optimal latent CPU capacity by redesigning NoSQL key-value store systems in a way that not only minimizes data movement. In addition, even if key-value stores are primarily designed for big-data applications where data movement is naturally the bottleneck, we show that for modern key-value systems (RocksDB, WiredTiger, Redis) there are families of workloads and query patterns which strike varying levels of CPU intensiveness. For these workload patterns, key-value stores need again to be redesigned to balance to harness the newly available CPU capacities exposed by accessing growing amounts of data resident in main memory.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Aristotelis Leventidis (Northeastern University), Wolfgang Gatterbauer (Northeastern University), Cody Dunne (Northeastern University), Mirek Riedewald (Northeastern University)
    <br/>
    <strong>
    I See What This Query Does: Visualising SQL Queries
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po271')">Toggle Abstract</a>
    <div id="po271" class="abstract" style="display: none;">
    <p>
    Complex SQL queries can be hard to understand, even for experienced programmers. We present QueryViz, a novel tool that can transform a SQL query over a given database schema into a visual representation. QueryViz aims to enhance query interpretation with concrete minimal visual elements that help the user capture the intent of a query quickly. It relies on the concise translation of first order logic into visualization by identifying the equivalence of SQL to that of logic representations. QueryViz automatically decides which visual elements to use and how to arrange them. Intuitively, our system behaves like a traditional optimizer, but optimizes for visual clarity instead of performance. We argue that it would be fairly straightforward to integrate QueryViz into existing DBMS, where it could facilitate modification, re-use, and maintenance of non-trivial SQL queries. In addition, it is also ideally suited as a teaching tool. An initial user study confirms the feasibility and potential benefits of our system.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Nathan Ng (University of Massachusetts Amherst), Sandeep Polisetty (UMASS), Marco Serafini (University of Massachusetts Amherst)
    <br/>
    <strong>
    Challenges and opportunities in building a transactional graph storage system for efficient real-time analytics
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po281')">Toggle Abstract</a>
    <div id="po281" class="abstract" style="display: none;">
    <p>
    We are currently developing a new storage system that is graph-aware, like LiveGraph, but is much more space-efficient. In this poster,  we will describe the state-of-the-art system LiveGraph, the existing problems of the system, research opportunities that emerged when trying to optimize the existing system, and possible solutions in tackling the abovementioned problems.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Matthew Tolton (Facebook Inc), Cornel Rat (Facebook Inc), Mathew Eis (Facebook Inc), Chaomin Yu (Facebook Inc), Tiho Tarnavski (Facebook Inc)
    <br/>
    <strong>
    Synapse: Distributed Transactional Storage in Facebook
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po291')">Toggle Abstract</a>
    <div id="po291" class="abstract" style="display: none;">
    <p>
    Synapse is a highly-available, ACID compliant, horizontally-scaling data store in Facebook. The consistency model is serializability, with optional linearizability. Each transaction maps to one instance of hierarchical single decree Paxos, with the client as the initial proposer. Compared to 2 phase commit on top of multi-Paxos model, Synapse leaderless design optimizes for predictability, scalability and availability.  Synapse exposes a key -> object interface within an auto-shared key space. Object types include Log, Blob, Counter and more. Transactions understand type specific operations for pipelining and conflict detection.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Vasiliki Kalavri (Boston University), John Liagouris (Boston University)
    <br/>
    <strong>
    Evaluating State Management Approaches for Streaming Dataflows
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po301')">Toggle Abstract</a>
    <div id="po301" class="abstract" style="display: none;">
    <p>
    Modern stream processors rely on embedded key-value stores to manage larger-than-memory state accumulated by long-running computations. RocksDB is the most prominent store used by systems such as Apache Flink, Kafka, and Samza, as well as Facebook’s Stylus. However, its benefits come at considerable cost: Flink applications using the RocksDB backend reportedly suffer an order of magnitude latency increase. A more recent key-value store, FASTER, provides support for in-place updates and promises better performance for workloads with a hot set.  In this poster, we present an experimental testbed for evaluating current practices and promising alternatives to streaming state management. Using a flexible API, we compare RocksDB and FASTER on diverse streaming queries and we also measure the performance gap between out-of-the-box approaches and carefully optimized state stores. Finally, we show that workload-aware state management can improve p99 latency of streaming queries by an order of magnitude. Our results motivate the need for flexible and customizable state stores.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Amadou Ngom (Carnegie Mellon University)
    <br/>
    <strong>
    Strategies for Vectorized Execution
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po311')">Toggle Abstract</a>
    <div id="po311" class="abstract" style="display: none;">
    <p>
    DBMSs use vectorizations to speed up database algorithms (joins, aggregates, etc.) and reduce the overhead of individual operations. For each vector, these algorithms often need to track the set of valid tuples after a predicate has been applied. To do so, systems employ one of two methods: dense selection vectors, or sparse bitmaps. There is currently no comparison of the two approaches, no analysis of which approach works better under given scenarios. This work does precisely this analysis. For vectorized joins and scans, we determine the tradeoffs between the two approaches. Preliminary results show that sparse bitmaps work better most of the time thanks to a higher potential for SIMD vectorization. If the execution engine supports adaptive query processing, then adaptively switching between the two proves to be a better technique.  
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Lei Cao (MIT), Yizhou Yan (Worcester Polytechnic Institute), Samuel Madden (MIT), Elke Rundensteiner (WPI)
    <br/>
    <strong>
    An End-to-end Anomaly Detection System
    <a href="javascript: toggleVisibility ('#po321')">Toggle Abstract</a>
    <div id="po321" class="abstract" style="display: none;">
    <p>
    Staggering volumes of data sets collected by modern applications from financial transaction data to IoT sensor data contain critical insights from rare phenomena to anomalies indicative of fraud or failure. To decipher valuables from the counterfeit, analysts need to interactively sift through and explore the data deluge. By detecting anomalies, analysts may prevent fraud or prevent catastrophic sensor failures. While previously developed research offers a treasure trove of stand-alone algorithms for detecting particular types of outliers, they tend to be variations on a theme. There is no end-to-end paradigm to bring this wealth of alternate algorithms to bear in an integrated infrastructure to support anomaly discovery over potentially huge data sets while keeping the human in the loop.  This project is the first to design an integrated paradigm for end-to-end anomaly discovery. It aims to support all stages of anomaly discovery by seamlessly integrating outlier-related services within one integrated platform. The result is a database-system inspired solution that models services as first class citizens for the discovery of outliers. It integrates outlier detection processes with data sub-spacing, explanations of outliers with respect to their context in the original data set, feedback on the relevance of outlier candidates, and metric-learning to refine the effectiveness of the outlier detection process. The resulting system enables the analyst to steer the discovery process with human ingenuity, empowered by near real-time interactive responsiveness during exploration. Our solution promises to be the first to achieve the power of sense making afforded by outlier explanation services and human feedback integrated into the discovery process.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Hyungsik Caleb Kim (DePaul University), Alexander Rasin (DePaul University), James M Wagner (DePaul University)
    <br/>
    <strong>
    Optimizing query plans with expensive user-defined-predicates
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po331')">Toggle Abstract</a>
    <div id="po331" class="abstract" style="display: none;">
    <p>
    All major relational database management systems (RDBMS) support function calls (built-in or custom) incorporated directly into SQL. RDBMSes enable users to create user-deﬁned functions using embedded languages (e.g., PL/SQL) or general languages (e.g., C/Java). User-deﬁned functions are necessary for applications in diﬀerent domains, including geographical data analytics, software project intelligence), bioinformatics, and data-driven astronomy (e.g., the Sloan Digital Sky Survey).   In this poster presentation, we propose improvements to both of our algorithms: the binary search algorithm and the query plan pruning algorithm. We describe our strategies for improving the binary search algorithm to further reduce the optimization cost while maintaining the same query plan quality. Our improved strategy terminates the inner dimension of the 2-dimensional UDP search as our algorithm performs a binary search. The goal of our new algorithm is to reduce the optimization cost while improving the quality of plan as compared to our previous approach. Our strategy for the improved query plan pruning approach is to dynamically adjust sub-plan pruning threshold based on the schema complexity and the number of tables and UDPs in a query. Our current query plan pruning algorithm uses a fixed sub-plan pruning threshold. 
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Jeremy Kepner (MIT Lincoln Laboratory)
    <br/>
    <strong>
    Simple Data Architecture Best Practices for AI Readiness
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po341')">Toggle Abstract</a>
    <div id="po341" class="abstract" style="display: none;">
    <p>
    AI requires data. A core requirement for AI techniques to be successful is high quality data. Hence, preparing systems to be “AI Ready” involves collecting raw data and parsing it for subsequent ingest, scan, query, and analysis. There are simple techniques that can be applied during initial parsing of raw data that can dramatically reduce the effort of applying AI. This parsing is much more efficient to do during initial collection setup when the knowledge of the data exists with the programmer. Requiring an AI analyst to later deduce this knowledge is a primary reason why “data wrangling” is often 80% of the effort in building an AI system. This work provides a short list of a few of these best practices, which can be summarized in one word: tables.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Neil B Band (Harvard College), Wasay Abdul (Harvard), Stratos Idreos (Harvard)
    <br/>
    <strong>
    MemFlow: Memory-Aware Distributed Deep Learning
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po351')">Toggle Abstract</a>
    <div id="po351" class="abstract" style="display: none;">
    <p>
    The memory requirement in deep neural network (DNN) training is increasing due to a positive feedback loop in data generation. Data collection improves and users generate more training data, more expressive models capture increasing complexity, and higher model accuracy attracts more users. A major contributor to the memory bottleneck is storage of feature maps: intermediate results produced during the forward pass and consumed during the backward pass of gradient backpropagation. State-of-the-art optimization frameworks for training on distributed hardware, however, fail to model both training time and memory usage, and hence fail to find parallelization strategies that optimize memory usage.   In this work, we address this problem by introducing MemFlow, an optimization framework for distributed DNN training that considers the important metric of memory usage when searching for a parallelization strategy. MemFlow consists of: (i) a task graph generator that estimates computation time and memory usage; (ii) a memory-conscious execution simulator; and (iii) a Markov Chain Monte Carlo (MCMC) search algorithm that considers various degrees of recomputation i.e., discarding feature maps during the forward pass and recomputing them as needed during the backward pass. MemFlow produces per-device load statistics for a given parallelization strategy and permits optimization of arbitrary user-defined cost functions parametrized by both computation time and memory usage. Our experiments demonstrate that under memory constraints, MemFlow can readily locate valid and superior parallelization strategies unattainable with previous frameworks.         
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    ajay patel (EnterpriseDB)
    <br/>
    <strong>
    Zero to Hero: Managing Postgres through DevOps 
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po361')">Toggle Abstract</a>
    <div id="po361" class="abstract" style="display: none;">
    <p>
    With Devop's motion and powerful automation platforms, organizations are adopting a product-centric approach for Postgres deployment and management. Whether it's Public Cloud, Private Cloud, VM's, containers Infrastructure management as code solve a lot of problems.  It also helps organizations and database administrators to save, effort and cost to make the business successful. With new automation and best practices in place, database operation can focus on performance, tuning and capacity planning to scale the projects much faster.  This session we are going to cover  1. How DevOps play a key role in the Postgres world. 2. Open source Devop's tools and brief comparison. 3. Infrastructure deployment and Configuration management for Postgres. 4. Top database deployment strategies. 5. Successful playbook demo to set High scalability, connection pooling, and automatic failover. 
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Darryl Ho (MIT), Jialin Ding (MIT), Sanchit Misra (Intel), Nesime Tatbul (Intel Labs and MIT), Vikram Nathan (MIT); Vasimuddin Md (Intel), Tim Kraska (MIT)
    <br/>
    <strong>
    LISA: Towards Learned DNA Sequence Search
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po371')">Toggle Abstract</a>
    <div id="po371" class="abstract" style="display: none;">
    <p>
    Next generation sequencing (NGS) technologies have enabled affordable sequencing of billions of short DNA fragments at high throughput, paving the way for population-scale genomics. Genomics data analytics at this scale requires overcoming performance bottlenecks, such as searching for short DNA sequences over long reference sequences. In this paper, we introduce LISA (Learned Indexes for Sequence Analysis), a novel learning-based approach to DNA sequence search. As a first proof of concept, we focus on accelerating one of the most essential flavors of the problem, called exact search. LISA builds on and extends FM-index, which is the state-of-the-art technique widely deployed in genomics tool-chains. Initial experiments with human genome datasets indicate that LISA achieves up to a factor of 4X performance speedup against its traditional counterpart.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Erfan Zamanian (Brown University), Xiangyao Yu (MIT), Michael Stonebraker (MIT), Tim Kraska (MIT)
    <br/>
    <strong>
    Rethinking Database High Availability with RDMA Networks
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po381')">Toggle Abstract</a>
    <div id="po381" class="abstract" style="display: none;">
    <p>
    Highly available database systems rely on data replication to tolerate machine failures. Both classes of existing replication algorithms, active-passive and active-active, were designed in a time when network was the dominant performance bottleneck. In essence, these techniques aim to minimize network communication between replicas at the cost of incurring more processing redundancy; a trade-off that suitably fitted the conventional wisdom of distributed database design. However, the emergence of next-generation networks with high throughput and low latency calls for revisiting these assumptions.  In this poster, we first make the case that in modern RDMA-enabled networks, the bottleneck has shifted to CPUs, and therefore the existing network-optimized replication techniques are no longer optimal. We present Active-Memory Replication, a new high availability scheme that efficiently leverages RDMA to completely eliminate the processing redundancy in replication. Using Active-Memory, all replicas dedicate their processing power to executing new transactions, as opposed to performing redundant computation. Active-Memory maintains high availability and correctness in the presence of failures through an efficient RDMA-based undo logging scheme. Our evaluation against active-passive and active-active schemes shows that Active-Memory is up to a factor of 2 faster than the second-best protocol on RDMA-based networks.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Yoshinori Matsunobu (Facebook)
    <br/>
    <strong>
    MyRocks -- Space and Write optimized MySQL database
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po391')">Toggle Abstract</a>
    <div id="po391" class="abstract" style="display: none;">
    <p>
    MyRocks is our open source MySQL database engine on top of RocksDB, with the goal of improving space and write efficiency beyond what was possible with compressed InnoDB. Our objective was to migrate one of our main databases (UDB) from compressed InnoDB to MyRocks and reduce the amount of storage and number of servers used by half. We carefully planned and implemented the migration from InnoDB to MyRocks for our UDB tier, which manages data about Facebook’s social graph. We also migrated backend database of Facebook Messenger from HBase to MyRocks in 2017-2018, which improved reliability and performance on Flash devices. This poster describes MyRocks overview, comparison to InnoDB, and future plans.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Connor Luckett (Brown University), Alex Galakatos (Brown University), Andrew Crotty (Brown University), Ugur Cetintemel (Brown University)
    <br/>
    <strong>
    WimPi: In-Memory Analytics on a "Wimpy" Cluster
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po401')">Toggle Abstract</a>
    <div id="po401" class="abstract" style="display: none;">
    <p>
    Modern data processing infrastructures traditionally favor high-end hardware (e.g., the latest generation processors, abundant main memory), with deployments typically ranging from a single machine to small clusters. On the other end of the spectrum are microservers, which typically have lower-powered processors and limited main memory. Therefore, they are relatively inexpensive, with a price point of around $50 or less, despite the fact that they can serve as full-fledged computers.  In fact, microservers like the Raspberry Pi 3B+, have actually become quite competitive with high-end hardware. While older microserver designs included weak, single-core CPUs, currently available versions have multi-core CPUs with advanced features (e.g., superscalar processing, out-of-order execution). Surprisingly, our microbenchmarks even show that the single-threaded compute performance of a Raspberry Pi 3B+ is comparable to that of a high-end Intel Xeon processor that is two orders of magnitude more expensive.  Moreover, microservers consume substantially less power than traditional servers. For instance, at peak CPU utilization, we measured a power draw of only 5 Watts on a Raspberry Pi 3B+, whereas servers typically draw hundreds of Watts. In addition to drawing orders-of-magnitude less power, microservers generate significantly less heat, thereby eliminating associated cooling costs.   We believe that microservers are actually an interesting and viable alternative to high-end servers in the context of in-memory analytics. Therefore, we present WimPi, a cluster of inexpensive Raspberry Pi 3B+ microservers that offers a promising alternative to traditional high-end hardware deployments. Using a series of microbenchmarks and a detailed experimental evaluation with realistic workloads, we show that such an architecture is able tooer much more "bang for the buck", especially for applications that are not latency-sensitive (e.g., bulk processing jobs, offline machine learning).
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Meena Jagadeesan (Harvard University), Subarna Chatterjee (Harvard University), Stratos Idreos (Harvard)
    <br/>
    <strong>
    From Worst-Case to Average-Case Analysis: Accurate Latency Predictions for Key-Value Storage Engine
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po421')">Toggle Abstract</a>
    <div id="po421" class="abstract" style="display: none;">
    <p>
    Selecting the optimal storage engine and tuning for an application requires being able to cost the underlying data structures. For this, it is necessary to develop accurate latency predictions for diverse workloads executed on different data structures. In this work, we start to develop an average-case analysis of the performance of storage engines that can achieve significantly more accurate predictions than existing worst-case models. In particular, we propose a distribution-aware framework to predict the latency of diverse workloads executed on a vast number of data structures. Our framework can be used to find the optimal storage engine and tuning for an application, which vastly improves the application’s performance.
    </p>
    </div>
    </td>
    </tr>

</tbody>
</table>

   <h3>Spotlight Presentations will also present a poster:</h3>
<table class="table table-bordered posterTable">
<tbody>

<tr>
    <td>
    Maria Basmanova (Facebook), Orri Erling (Facebook)
    <br/>
    <strong>
    Evolving Big Data at Facebook
    </strong><br/>
    <a href="javascript: toggleVisibility ('#spo1')">Toggle Abstract</a>
    <div id="spo1" class="abstract" style="display: none;">
    <p>
    The Facebook Data Platform (Warehouse) handles all the mission critical big data at Facebook and holds exabytes of data across multiple data centers. Presto is the main interactive query engine and Spark is the main ETL/batch engine.  Additionally, Facebook has a number of in-house systems for stream processing, time series for operational data, in-memory analytics and real time query of aggregated ads statistics, to mention just a few. Beyond this, there are all the customer web-facing serving systems like TAO for graph caching and RocksDB for scalable key-value stores and MySQL backend.   In this talk we drill down on Presto efficiency advances, the limits of the Java platform and our work for next generation converged execution across multiple analytical data systems.   We see that Facebook scale data warehousing requires a holistic understanding of the space for overcoming the fragmentation and consequent maintenance burden that comes from multiple point solutions. We have presented some aspects of our answer to this challenge: - Aria: Do only the work that answering the business question requires - Folly4Data: Highest CPU efficiency and adaptivity for execution primitives, use in Presto and elsewhere, obtain consistent semantics, performance and user experience - Scalability: Adaptively handle resource allocation and workload placement. This stack of technologies will be open source and usable standalone or as part of PrestoDB. The Presto Foundation under Linux Foundation is the current Presto public facing entity. We welcome new members and participation in open source. 
    </p>
    </div>
    </td>
    </tr>

    <tr>
    <td>
    Ajay Patel (EnterpriseDB)
    <br/>
    <strong>
    Monitor Postgres for performance 
    </strong><br/>
    <a href="javascript: toggleVisibility ('#spo2')">Toggle Abstract</a>
    <div id="spo2" class="abstract" style="display: none;">
    <p>
    Postgres is really easy and simple to install and use on various platforms cloud or on-premises. Monitoring is a key element of the Database eco-system and to the overall database performance. It also helps to ensure Database is healthy and contributes towards long term stability.  Monitoring sounds really easy but it becomes tricky on what to monitor and how to monitor?  On this session we are going to talk about:   1. Why is it important to monitor Postgres?  2. What are the proactive monitoring and reactive monitoring approaches, and how do they help in handling future problems?  3. Top 10 monitoring points.  4. When not to monitor?  5. Deep dive into Open Source tools for monitoring.  6. Deployment categories & Check list 
    </p>
    </div>
    </td>
    </tr>

    <tr>
    <td>
    Subhadeep Sarkar (Boston University), Dimitris Staratzis (Boston University), Tarikul Islam Papon (Boston University), Manos Athanassoulis (Boston University)
    <br/>
    <strong>
    Lethe: A Delete-Aware LSM-Based Storage Engine
    </strong><br/>
    <a href="javascript: toggleVisibility ('#spo3')">Toggle Abstract</a>
    <div id="spo3" class="abstract" style="display: none;">
    <p>
    To support fast ingestion and fast query processing, modern data stores handle incoming data in an out-of-place fashion. Deletes in such data stores are realized logically by inserting additional metadata. We highlight that all out-of-place data stores treat deletes as second class citizens, and are not designed to efficiently realize deletes without hurting performance. To address this, we introduce Lethe, a new delete-aware out-of-place key-value store.
    </p>
    </div>
    </td>
    </tr>

     <tr>
    <td>
    sandeep polisetty (UMASS), Nathan Ng (University of Massachusetts Amherst), Marco Serafini (University of Massachusetts Amherst)
    <br/>
    <strong>
    Speeding up subgraph queries by constructing efficient intermediate structures
    </strong><br/>
    <a href="javascript: toggleVisibility ('#spo4')">Toggle Abstract</a>
    <div id="spo4" class="abstract" style="display: none;">
    <p>
    We study the problem of subgraph querying i.e finding instances of a given subgraph in a larger graph, a fundamental computation performed by many applications and supported by many software systems that process graphs. Example applications include triangles and larger clique-like structures for detecting related pages in the world wide web and finding diamonds in recommendation algorithms in social networks.  The underlying systems used by these applications could be graph databases, RDF engines or graph processing systems. These subgraph queries are typically evaluated one vertex at a time using the worst-case optimal generic joins.  Worst-case optimal joins heavily use set intersections which are sensitive to branch mispredictions.  These branches do not allow for proper utilization of SIMD acceleration.  However unlike SQL queries on multiple tables, graph queries consist of joins on the same table.  This repetitive structure of graph queries allows the existence of reusable intermediate structures.  We propose a novel bitvector to store these reusable structures in a SIMD friendly format which can later be processed rapidly using SIMD without any branches.  Our experiments show that we can achieve a reduction in branch misprediction by 50 percent and a speedup of 2-4x for n-clique queries
    </p>
    </div>
    </td>
    </tr>

     <tr>
    <td>
    Ryan C Marcus (MIT), Tim Kraska (MIT)
    <br/>
    <strong>
    Learning to Multiplex Simple Query Optimizers
    </strong><br/>
    <a href="javascript: toggleVisibility ('#spo5')">Toggle Abstract</a>
    <div id="spo5" class="abstract" style="display: none;">
    <p>
    Modern machine-learning based solutions to query optimization (e.g., learned cardinality estimators, reinforcement learning, etc.) suffer from large training data requirements, poor tail latency, and inevitable staleness as the data and schema shift. We propose a new class of learned optimizers based on multiplexing simple query optimizers in a contextual multi-armed bandit setting. We show preliminary results suggesting that such an approach could learn to outperform traditional query optimizers with very little training data, while never producing a catastrophic query plan. We are hopeful that structural properties of our proposed system will allow it to easily adapt to changes in data and schema.
    </p>
    </div>
    </td>
    </tr>

         <tr>
    <td>
    Catherine Macheret (MITRE), Arnie Rosenthal (MITRE), Peter Mork (MITRE)
    <br/>
    <strong>
    Toward Agile Harmonization of Specifications
    </strong><br/>
    <a href="javascript: toggleVisibility ('#spo6')">Toggle Abstract</a>
    <div id="spo6" class="abstract" style="display: none;">
    <p>
    Systems interoperate more effectively when different components employ similar data definitions. To achieve this, one needs to harmonize the systems’ specifications. This need arises not just with databases, but also with ontologies, survey instruments, GUIs, statistical queries, and other information conveyances describable by metadata structures. The database community employs two complementary approaches to harmonization: data standardization (for new and evolving systems and interfaces) and data integration/mapping (for systems and interfaces one cannot change). This talk has several objectives for advancing these approaches: 1. Propose harmonization as a general problem in a way that facilitates the transition of data integration lore into practice, on problems requiring harmonizing structured metadata. 2. Describe some surprising shortfalls in the state of the practice for metadata and data standards 3. Discuss how agile harmonization might be achieved, by adding decentralized, time-limited, bottom-up capabilities to the standards and harmonization armory We then describe questions where research progress might mitigate these difficulties.
    </p>
    </div>
    </td>
    </tr>

         <tr>
    <td>
    Matteo Brucato (UMass Amherst), Nishant  Yadav (University of Massachusetts Amherst), Azza Abouzied (New York University Abu Dhabi), Peter Haas (University of Massachusetts Amherst), Alexandra Meliou (University of Massachusetts Amherst)
    <br/>
    <strong>
    Stochastic Package Queries for In-Database Constrained Optimization Under Uncertainty
    </strong><br/>
    <a href="javascript: toggleVisibility ('#spo7')">Toggle Abstract</a>
    <div id="spo7" class="abstract" style="display: none;">
    <p>
    In this talk, we introduce methods for in-database support of constrained optimization under uncertainty. Many important decision problems correspond to selecting a ``package'' (bag of tuples in a relational database) in order to jointly satisfy a set of constraints (expressed as predicates) while minimizing some overall ``cost'' function; in most real-world problems, the data is uncertain. We provide methods for specifying---via a SQL extension---and processing stochastic package queries (SPQs). SPQs model optimization problems with uncertainty, and let users express and solve these problems inside the database system. In order to handle a broad class of uncertainty models, prior work by the stochastic programming community uses Monte Carlo methods where the original stochastic optimization problem is approximated by a large deterministic optimization problem that incorporates many ``scenarios'', i.e., sample realizations of the uncertain data values. For large database tables, however, a huge number of scenarios is required, leading to poor performance and, often, failure of the solver software. We therefore provide a novel SummarySearch algorithm that, instead of trying to solve a large deterministic problem, seamlessly approximates it via a sequence of problems defined over carefully crafted ``summaries'' of the scenarios that accelerate convergence to a feasible and near-optimal solution. Experimental results on our prototype system show that SummarySearch can be orders of magnitude faster than prior methods at finding feasible and high-quality packages.
    </p>
    </div>
    </td>
    </tr>

        <tr>
    <td>
    Christine F Reilly (Skidmore College)
    <br/>
    <strong>
    A Generic RDBMS Schema for Property Graphs
    </strong><br/>
    <a href="javascript: toggleVisibility ('#spo8')">Toggle Abstract</a>
    <div id="spo8" class="abstract" style="display: none;">
    <p>
    This talk describes on-going research focused on creating and evaluating a generic schema for storing property graphs in a relational database management system (RDBMS). We focus on transactional graph data, where there are updates (insertions, deletions, and modifications) to the data over time. We are motivated by related work that has demonstrated that an RDBMS performs as well as or better than a specialized graph system for graph analytics applications. This related work suggests that an RDBMS may be the preferred storage system for graph data, because RDBMSs are mature software systems that use well-tested approaches for reliable data storage. This talk will present the schema as implemented in MariaDB, and will discuss the design decisions made and challenges faced in the creation of this schema. The plan for evaluation of this approach and directions for future work will be discussed.
    </p>
    </div>
    </td>
    </tr>

    <tr>
    <td>
    Brian Hentschel (Harvard University), Stratos Idreos (Harvard University)
    <br/>
    <strong>
    Workload-Dependent Filtering
    </strong><br/>
    <a href="javascript: toggleVisibility ('#spo9')">Toggle Abstract</a>
    <div id="spo9" class="abstract" style="display: none;">
    <p>
    Filters are memory-efficient data structures that are computationally efficient and which give partial answers to queries or defer answers to a secondary more complete source of the data. In this poster, we overview different ways that workload characteristics and data characteristics can be used to create better filter data structures. We present 1) Column Sketches, a technique using filtering to improve predicate evaluation, 2) preliminary work on its extensions to general operators 3) Stacked Filters, a technique using workload skew to create filters which help in zero result point lookups and 4) preliminary work improving Learned Bloom Filters, which like Stacked Filters help in the evaluation of zero result point lookups. We additionally examine the overlying themes which connect all four filter data structures. 
    </p>
    </div>
    </td>
    </tr>


</tbody>
</table>

</div>
