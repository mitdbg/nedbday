
<div class="content-container">
   <h1>
         North East Database Day 2019 <br />
         <small>Thursday January 24th, 2019</small><br/>
         <small>Samberg Conference Center at MIT</small>
   </h1>
   <p>The poster session will include drinks and appetizers. It will be held in the Samberg Conference Center at MIT.</p>
   <h3>Information for Poster Presenters</h3>
   <p>We will be able to provide poster board, easels, and mounting supplies for you.
   You will need to print your poster and bring it with you to the conference. We recommend
   posters to be either A0, A1, or ANSI D or E sizes (either 24" by 32" or 36" by 42"). Our
   poster boards are large enough to accommodate any of these (4' x 6'). You may orient your
   poster as you see fit. If you prefer to print out individual 8.5" x 11" pages, our board
   will be large enough to accommodate about 12 pages. If you prefer a single poster and
   you don't have access to a large format printer, Fedex/Kinkos can print your poster for you.
   </p>
   <h3>List of accepted posters:</h3>

<table class="table table-bordered posterTable">
<tbody>
<tr>
    <td>
    Jeremy Kepner (MIT Lincoln Laboratory)
    <br/>
    <strong>
    TabulaROSA: Tabular Operating System Architecture
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po1')">Toggle Abstract</a>
    <div id="po1" class="abstract" style="display: none;">
    <p>
    The rise in computing hardware choices is driving a reevaluation of operating systems. The traditional role of an operating system controlling the execution of its own hardware is evolving toward a model whereby the controlling processor is distinct from the compute engines that are performing most of the computations. In this context, an operating system can be viewed as software that brokers and tracks the resources of the compute engines and is akin to a database management system. To explore the idea of using a database in an operating system role, this work defines key operating system functions in terms of rigorous mathematical semantics (associative array algebra) that are directly translatable into database operations. These operations possess a number of mathematical properties that are ideal for parallel operating systems by guaranteeing correctness over a wide range of parallel operations. The resulting operating system equations provide a mathematical specification for a Tabular Operating System Architecture (TabulaROSA) that can be implemented on any platform. Simulations of forking in TabularROSA are performed using an associative array implementation and compared to Linux on a 32,000+ core supercomputer. Using over 262,000 forkers managing over 68,000,000,000 processes, the simulations show that TabulaROSA has the potential to perform operating system functions on a massively parallel scale. The TabulaROSA simulations show 20x higher performance as compared to Linux while managing 2000x more processes in fully searchable tables.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Abdul Wasay (Harvard University)
    <br/>
    <strong>
    MotherNets: Rapid Deep Ensemble Learning
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po2')">Toggle Abstract</a>
    <div id="po2" class="abstract" style="display: none;">
    <p>
    Ensembles of deep neural networks with diverse architectures significantly improve generalization accuracy. However, training such ensembles requires a large amount of computational resources and time as every network in the ensemble has to be trained. In practice, this restricts the number of deep neural network architectures that can be included within an ensemble. We propose MotherNets to address this problem. A MotherNet captures the maximum structural similarity across different members of a deep neural network ensemble. To train an ensemble, we first train the MotherNet and subsequently, this knowledge (or mapping function) is transferred to all members of the ensemble. Then, we continue to train the ensemble networks which converge significantly faster as compared to training from scratch. We show through experiments that MotherNets can: (i) reduce ensemble training cost by training large and diverse ensembles of deep neural networks achieving comparable accuracy to existing approaches in a fraction of their training time (e.g., 2x to 6x faster in our experiments) or (ii) alternatively, improve ensemble accuracy by being able to train larger ensembles within the same time state-of-the-art approaches train smaller ones. This reduction in training cost is because MotherNets reduces the total number of epochs (or rounds of training) required to train the ensemble resulting in fewer data movements and computations. This improvement grows linearly with the size of the ensemble allowing the construction of larger and more diverse ensembles to attack more complex problems without extra cost.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Brian N Hentschel (Harvard University)
    <br/>
    <strong>
    Column Sketches: A Scan Accelerator for Rapid and Robust Predicate Evaluation
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po3')">Toggle Abstract</a>
    <div id="po3" class="abstract" style="display: none;">
    <p>
    While numerous indexing and storage schemes have been developed to address the core functionality of predicate evaluation in data systems, they all require specific workload properties (query selectivity, data distribution, data clustering) to provide good performance and fail in other cases. We present a new class of indexing scheme, termed a Column Sketch, which improves the performance of predicate evaluation independently of workload properties. Column Sketches work primarily through the use of lossy compression schemes which are designed so that the index ingests data quickly, evaluates any query performantly, and has small memory footprint. A Column Sketch works by applying this lossy compression on a value-by-value basis, mapping base data to a representation of smaller fixed width codes. Queries are evaluated affirmatively or negatively for the vast majority of values using the compressed data, and only if needed check the base data for the remaining values. Column Sketches work over column, row, and hybrid storage layouts.   We demonstrate that by using a Column Sketch, the select operator in modern analytic systems attains better CPU efficiency and less data movement than state-of-the-art storage and indexing schemes. Compared to standard scans, Column Sketches provide an improvement of 3X-6X for numerical attributes and 2.7X for categorical attributes. Compared to state-of-the-art scan accelerators such as Column Imprints and BitWeaving, Column Sketches perform 1.4 - 4.8X better.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Sebastian Schelter (New York University)
    <br/>
    <strong>
    Data-Related Challenges in End-to-End Machine Learning
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po4')">Toggle Abstract</a>
    <div id="po4" class="abstract" style="display: none;">
    <p>
    I moved back to academia after working on a set of real-world machine learning (ML) deployments at Amazon Core ML for some years. I would like to present a poster summarizing some of the data management challenges encountered during that time with the goal to provide directions for future research.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Pieter Cailliau (Redis Labs), Timothy A Davis (Texas A&M University), Vijay Gadepally (MIT Lincoln Laboratory), Jeremy Kepner (MIT Lincoln Laboratory), Roi Lipman (Redis Labs), Jeffrey Lovitz (Redis Labs), Keren Ouaknine (Redis Labs)
    <br/>
    <strong>
    RedisGraph GraphBLAS Enabled Graph Database
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po5')">Toggle Abstract</a>
    <div id="po5" class="abstract" style="display: none;">
    <p>
    RedisGraph is a Redis module developed by Redis Labs to add graph database functionality to the Redis database. RedisGraph represents connected data as adjacency matrices. By representing the data as sparse matrices and employing the power of GraphBLAS (a highly optimized library for sparse matrix operations), RedisGraph delivers a fast and efficient way to store, manage and process graphs. Initial benchmarks indicate that RedisGraph is significantly faster than comparable graph databases.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Richard Wang (MIT), Jeremy Kepner (MIT Lincoln Laboratory)
    <br/>
    <strong>
    Database Schema for Human Brain Scale Neural Networks
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po6')">Toggle Abstract</a>
    <div id="po6" class="abstract" style="display: none;">
    <p>
    Deep neural networks have shown great potential in solving various problems through machine learning. With recent high levels of accuracy, there is a new possibility to model and understand the human brain by running a neural network comparable to the number of neurons and connections in the human brain. In many cases, memory limits neural network size. Fortunately, the human brain’s number of neurons and sparse connections implies that we can model the human brain as a large, sparse neural network. With the Dynamic Distributed Dimensional Data Model (D4M) software and the Apache Accumulo database, it is possible to store a neural network, with as many neurons and connections as the human brain, through the help of parallel computing in supercomputers. This work describes a schema for storing a human brain scale neural network. Using Graphulo, the stored neural network will be able to execute large inference on the same scale as the human brain.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Yoshinori Matsunobu (Facebook)
    <br/>
    <strong>
    MyRocks -- Space and Write optimized MySQL database
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po7')">Toggle Abstract</a>
    <div id="po7" class="abstract" style="display: none;">
    <p>
    MyRocks is our open source MySQL database engine on top of RocksDB, with the goal of improving space and write efficiency beyond what was possible with compressed InnoDB. Our objective was to migrate one of our main databases (UDB) from compressed InnoDB to MyRocks and reduce the amount of storage and number of servers used by half. We carefully planned and implemented the migration from InnoDB to MyRocks for our UDB tier, which manages data about Facebook’s social graph. We also migrated backend database of Facebook Messenger from HBase to MyRocks in 2017-2018, which improved reliability and performance on Flash devices. This poster describes MyRocks overview, comparison to InnoDB, and future plans.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Kostas Zoumpatianos (Harvard University), Stratos Idreos (Harvard), Themis Palpanas (Paris Descartes University)
    <br/>
    <strong>
    T-Store: Tunable Storage for Large Sequential Data
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po8')">Toggle Abstract</a>
    <div id="po8" class="abstract" style="display: none;">
    <p>
    T-Store is the first data series storage engine that uses the query workload in order to tune its data layout, leading to significant performance improvements. It is able to efficiently re-organize and partition arbitrary collections of data series, to match a given workload specification. It intelligently partitions data series in groups based on the access patterns, minimizing query execution cost.   In addition, it allows for arbitrary index structures to be efficiently built above.  The main contributions of this work are: i) a novel workload characterization method for data series queries workloads, ii) a workload-aware data partitioning scheme that constructs optimal groups of data series, iii) an efficient data ingestion method based on write-optimized partitioning, and iv) how to provide support for summarizations and content-based search.   We perform a thorough experimental evaluation in multiple datasets and workloads, demonstrating that T-Store can improve query performance by more than one order of magnitude by choosing the correct data layout, while still being robust to random query workloads.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    sandeep polisetty (UMASS), Marco Serafini (University of Massachusetts Amherst)
    <br/>
    <strong>
    Opportunities in decoupling storage and compution in graph mining
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po9')">Toggle Abstract</a>
    <div id="po9" class="abstract" style="display: none;">
    <p>
    POSTER: Graph data exists in a broad range of domains such as social networks, mobile networks and semantic webs. Ever increasing graph data has led to growing importance for graph mining. Graph Mining algorithms tend themselves to either of the following programming abstractions. A vertex oriented approach where each step results in an update to the state of a vertex and outgoing messages given an initial state and incoming messages edges. While this approach works for algorithms such as PageRank and ShortestPath, it is quite to challenging to write pattern mining algorithms (FSM,CliqueFinding) with this paradigm. A task oriented approach is better suited where each subgraph to be explored is treated as a task and incrementally explored.While the vertex oriented approach could be easily scaled taking advantage of vertex locality as the graph is typically stored as contiguous vertex partitions and streaming in partitions(eg. RStream). The task based approach cannot be scaled as easily as each task involves random vertex access. Therefore task based platforms such as GMiner and Arabesque partition the graph and keep the partition in memory. However, maintaining the graph partition in memory leads to higher compute time, due to high preloading cost. Coupling the computation with storage implies they cannot be independently scaled. Therefore, even simple computation on large graphs would need a lot of machines to store the graph in memory. By Modifying Gminer, we attempt to show the performance gained as the cost of preloading the graph is amortized over the computation. We analyze other preformance bottlenecks that would arise in scaling such a system.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Aaron Elliot (University of Massachusetts Amherst )
    <br/>
    <strong>
    The Learned Index as a Constant Space Secondary Index : Range Search on Unsorted Data
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po10')">Toggle Abstract</a>
    <div id="po10" class="abstract" style="display: none;">
    <p>
    Secondary indices can be expensive in terms of storage, especially in case of high dimensional data. The gain from keeping a secondary index on a large number of attributes can often be outweighed by the storage cost of a traditional index’s O(n) space.  Learned index structures can be far more space efficient than traditional indexes, as they only require storing the function and weights. A constant space learned index structure may be valuable when a linear space index is too large. Along this vein of reasoning, we explore the plausibility of learned index structures as constant space secondary indexes. In particular, we build a learned index structure for range search on a secondary attribute assuming the data is sorted on a primary attribute. Our approach requires that values of the primary attribute not be independent of the values of the secondary attribute.  The relationship between primary and secondary attributes may be many to many. To solve this challenge, we develop a neural mixed regression model. Another challenge we face is that TPC benchmark data has independent columns. As a work around, we evaluate on a dataset of Los Angeles parking citations, and consider range searches along uniformly random intervals.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Niv Dayan (Harvard), Stratos Idreos (Harvard)
    <br/>
    <strong>
    Dostoevsky: Removing Superfluous Merging from LSM-Trees
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po11')">Toggle Abstract</a>
    <div id="po11" class="abstract" style="display: none;">
    <p>
    We show that all mainstream LSM-tree based key-value stores in the literature and in industry suboptimally trade between the I/O cost of updates on one hand and the I/O cost of lookups  and storage space on the other. The reason is that they perform equally expensive merge operations across all levels of LSM-tree to bound the number of runs that a lookup has to probe and to remove obsolete entries to reclaim storage space. With state-of-the-art designs, however, merge operations from all levels of LSM-tree but the largest (i.e., most merge operations) reduce point lookup cost, long range lookup cost, and storage space by a negligible amount while significantly adding to the amortized cost of updates.   To address this problem, we introduce Lazy Leveling, a new design that removes merge operations from all levels of LSM-tree but the largest. Lazy Leveling improves the worst-case complexity of update cost while maintaining the same bounds on point lookup cost, long range lookup cost, and storage space. We further introduce Fluid LSM-tree, a generalization of the entire LSM-tree design space that can be parameterized to assume any existing design. Relative to Lazy Leveling, Fluid LSM-tree can optimize more for updates by merging less at the largest level, or it can optimize more for short range lookups by merging more at all other levels.  We put everything together to design Dostoevsky, a key-value store that adaptively removes superfluous merging by navigating  the Fluid LSM-tree design space based on the application workload and  hardware. We implemented Dostoevsky on top of RocksDB, and we show that it strictly dominates state-of-the-art designs in terms of  performance and storage space.  
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Tabassum Kakar (Worcester Polytechnic Institute)
    <br/>
    <strong>
    ConText: Supporting the Pursuit and Management of Evidence in Text-based Reporting Systems 
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po12')">Toggle Abstract</a>
    <div id="po12" class="abstract" style="display: none;">
    <p>
    Incident Inquiry Analysis (IIA) requires investigators to analyze text-based reports, where each may be evidence of a larger pattern that requires regulatory action. Given the rise of reporting systems in organizations, there is a need for systems that aid IIA analysts in exploring, evaluating, and generalizing findings across independently produced reports. We describe ConText, an IIA tool designed in collaboration with Pharmacovigilance analysts. ConText aims to address IIA challenges by combining domain-informed models to process unstructured text, interactive visualizations for steering investigations, and a case-building workflow that aligns with known analytical operations such as evidence collection and management. We evaluate ConText through case-studies that illustrate IIA workflows, and through semi-structured usage interviews with n=10 Pharmacovigilance analysts.We discuss how the design and evaluation of ConText may point to emerging challenges for interactive analytics in life-critical workflows, including a growing need for approaches that support trustworthy and transparent model-driven analytics.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Raj Singh (IBM)
    <br/>
    <strong>
    THE URBAN MORPHOLOGY PROJECT: Collaboratively Building an Open Global Land Use Database
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po13')">Toggle Abstract</a>
    <div id="po13" class="abstract" style="display: none;">
    <p>
    The Urban Morphology Project is a collaborative effort to develop an open global urban land use database. Prior efforts at global land use mapping have focused on categorizing land cover by natural features such as forest, agriculture, and water, leaving urban use as a single, homogeneous category. This has been useful for environmental issues and looking at the spread of human settlement, but there has been little work on breaking that urban use category into its constituent parts such as residential, industrial, commercial, etc. Such a database would be of tremendous value to urban scientists and designers in a wide range of fields. Using open data sets in conjunction with the very high resolution satellite imagery (sub-30m) just becoming publicly available, we seek to develop machine learning models tuned to the distinct developmental patterns that exist across the globe that can identify these urban land use forms.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Mihin S Sumaria (Worcester Polytechnic Institute), Rodica Neamtu (Worcester Polytechnic Institute)
    <br/>
    <strong>
    Data Series Analytics Powered by Multiple Warped Distances using DAVIS
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po14')">Toggle Abstract</a>
    <div id="po14" class="abstract" style="display: none;">
    <p>
    With the continuous increase of availability of time-series data in medical, financial, weather forecasting, and many other domains, the need to visually explore time-series data is becoming vital to extracting trends that may not otherwise be accessible to analysts. The computational costs associated with the interactive exploration of misaligned and variable-length time-series using elastic distances such as dynamic time warping is very high and impact the scalability of data discovery tools. We propose the Data Analytics and Visualization system (DAVIS) for mining large data series sets. Our system enables analysts to get insights by retrieving k_similar sequences in real-time using rich visualizations such as element-wise DTW matchings, line, difference, stacked area, and radial charts. Our DAVIS interface also provides square plots for summarizing clusters within our datasets, while also capturing point-wise similarities in misaligned sequences. 
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Rebecca L Wild (James Madison University), Matthew Hubbell (MIT Lincoln Laboratory), Jeremy Kepner (MIT Lincoln Laboratory)
    <br/>
    <strong>
    Scaling Big Data Platform for Big Data Pipeline
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po15')">Toggle Abstract</a>
    <div id="po15" class="abstract" style="display: none;">
    <p>
    Monitoring and Managing High Performance Computing (HPC) systems and environments generate an ever growing amount of data. Making sense of this data and generating a platform where the data can be visualized for system administrators and management to proactively identify system failures or understand the state of the system requires the platform to be as efficient and scalable as the underlying database tools used to store and analyze the data. In this paper we will show how we leverage Accumulo, d4m, and Unity to generate a 3D visualization platform to monitor and manage the Lincoln Laboratory Supercomputer systems and how we have had to retool our approach to scale with our systems.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Yi Lu (MIT), Xiangyao Yu (MIT), Samuel Madden (MIT)
    <br/>
    <strong>
    SCAR: Strong Consistency using Asynchronous Replication with Minimal Coordination
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po16')">Toggle Abstract</a>
    <div id="po16" class="abstract" style="display: none;">
    <p>
    Data replication is crucial in modern distributed systems as a means to provide high availability. Many techniques have been proposed to utilize replicas to improve a system's performance, often requiring expensive coordination or sacrificing consistency. In this paper, we present SCAR, a new distributed and replicated in-memory database that allows serializable transactions to read from backup replicas with minimal coordination. SCAR works by assigning logical leases to database records so that a transaction can safely read from a backup replica without coordination, because the records cannot be changed up to a certain logical time. In addition, we propose two optimization techniques (timestamp synchronization and parallel locking and validation) to further reduce coordination. We show that SCAR outperforms systems with conventional concurrency control algorithms and replication strategies by up to a factor of 2 on two popular benchmarks. We also demonstrate that SCAR achieves higher throughput by running under reduced isolation levels and detects concurrency anomalies in real time.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Yan Li (UMass Lowell)
    <br/>
    <strong>
    VTeller: Telling the Values Somewhere, Sometime in a Dynamic Network of Urban Systems
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po17')">Toggle Abstract</a>
    <div id="po17" class="abstract" style="display: none;">
    <p>
    Dynamic networks are very common in urban systems today. As data are acquired, unfortunately, they are rarely complete observations of the whole system. It is important to reliably infer the unobserved attribute values anywhere in the graphs, at certain times—either in the past or in the future. Previous work does not sufficiently capture the correlations inherent with graph topology and with time. We propose a machine learning approach using a novel probabilistic graphical model. We devise a series of algorithms to efficiently group the vertices, to learn the model parameters, and to infer the unobserved values for query processing. Furthermore, we propose a method to incrementally and automatically update the model. Finally, we perform an extensive experimental study using two real-world dynamic graph datasets to evaluate our approach.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Erkang Zhu (University of Toronto)
    <br/>
    <strong>
    JOSIE: Overlap Set Similarity Search for Finding Joinable Tables in Data Lakes
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po18')">Toggle Abstract</a>
    <div id="po18" class="abstract" style="display: none;">
    <p>
    We present a new solution for finding joinable tables in massive data lakes: given a table and one join column, find tables that can be joined with the given table on the largest number of distinct values. The problem can be formulated as an overlap set similarity search problem by considering columns as sets and matching values as intersection between sets. Although set similarity search is well-studied in the field of approximate string search (e.g., fuzzy keyword search), the solutions are designed for and evaluated over sets of relatively small size (average set size rarely much over 100 and maximum set size in the low thousands) with modest dictionary sizes (the total number of distinct values in all sets is only a few million). We observe that modern data lakes (including an enterprise lake and two important public data lakes of Open Government Data and WebTables) typically have massive set sizes (with maximum set sizes that may be tens of millions) and dictionaries that include hundreds of millions of distinct values. Our new algorithm, JOSIE (JOining Search using Intersection Estimation) minimizes the cost of set reads and inverted index probes used in finding the top-k sets. We show that JOSIE completely out performs the state-of-the-art overlap set similarity search techniques on data lakes. More surprising, we also consider state-of-the-art approximate algorithm and show that our new exact search algorithm performs almost as well, and even in some cases better, on real data lakes.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Xuanming Liu (UMass Lowell), Tingjian Ge (University of Massachusetts, Lowell), Yinghui Wu (Washington State University)
    <br/>
    <strong>
    Finding Densest Lasting Subgraphs in Dynamic Graphs: a Stochastic Approach
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po19')">Toggle Abstract</a>
    <div id="po19" class="abstract" style="display: none;">
    <p>
    One important problem that is insufficiently studied is finding densest lasting-subgraphs in large dynamic graphs, which considers the time duration of the subgraph pattern. We propose a framework called Expectation-Maximization with Utility functions (EMU), a novel stochastic approach that nontrivially extends the conventional EM approach. EMU has the flexibility of optimizing any user-defined utility functions. We validate our EMU approach by showing that it converges to the optimum—by proving that it is a specification of the general Minorization-Maximization (MM) framework with convergence guarantees. We then devise EMU algorithms for the densest lasting subgraph problem. Using real-world graph data, we experimentally verify the effectiveness and efficiency of our techniques, and compare with two prior approaches on dense subgraph detection.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Fatemeh Nargesian (University of Toronto)
    <br/>
    <strong>
    Optimizing Organizations for Navigating Data Lakes
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po20')">Toggle Abstract</a>
    <div id="po20" class="abstract" style="display: none;">
    <p>
    Navigation is known to be an effective complement to search. In addition to data discovery, navigation can help users develop a conceptual model of what types of data are available. In data lakes, there has been considerable research on dataset or table discovery using search. We consider the complementary problem of creating an effective navigation structure over a data lake. We define an organization as a navigation structure (graph) containing nodes representing sets of attributes (from tables or from semi-structured documents) within a data lake. An edge represents a subset relationship. We propose a novel problem, the data lake organization problem where the goal is to find an organization that allows a user to most efficiently find attributes or tables. We present a new probabilistic model of how users interact with an organization and define the likelihood of a user finding an attribute or a table using the organization. Our approach uses the attribute values and metadata (when available). For data lakes with little or no metadata, we propose a way of creating metadata using metadata available in other lakes. We propose an approximate algorithm for the organization problem and show its effectiveness on a synthetic benchmark. Finally, we construct an organization on tables of a real data lake containing data from federal Open Data portals and show that the organization dramatically improves the expected probability of discovering tables over a baseline. Using a second real data lake with no metadata, we show how metadata can be inferred that is effective in enabling organization creation. 
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Jialin Ding (MIT)
    <br/>
    <strong>
    Learned Multi-Dimensional Index for Data Warehouses
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po21')">Toggle Abstract</a>
    <div id="po21" class="abstract" style="display: none;">
    <p>
    Given a multi-dimensional table in a data warehouse, an analyst will often want to run queries on subsets of those dimensions. For example, given a table that tracks employee attributes such as age, salary, level, and start date, one possible range query is, ``Return all employees between ages 25-30 with salary between $90K and $100K."  Current methods for indexing multi-dimensional data are unsatisfactory. Multi-dimensional indexes such as R trees are rarely used in practice because they are difficult to implement and spend unnecessary resources on dimensions that are infrequently queried. In practice, multi-dimensional data is often only partially indexed: a primary index is built on the most important dimension, secondary indexes are built on some other dimensions, and infrequently-queried dimensions are simply not indexed. However, the decision of which dimensions to index is ad hoc and based on hand-tuned parameters.  In this work, we create a multi-dimensional learned index that uses knowledge of the dataset and query workload to automatically create an optimal index. Specifically, the learned index optimally rearranges the data storage order to achieve low scan overhead, which is a primary contributor to faster query speeds. On a workload derived from the TPC-H benchmark, the multi-dimensional learned index achieves up to 34X faster queries than a clustered column index and 1000X faster than an index-less column scan, while also maintaining an index size many orders of magnitude smaller than the size of the dataset.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Kishori M  Konwar (MIT)
    <br/>
    <strong>
    Storage Efficient Erasure-coded Consistent Key-value Store
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po221')">Toggle Abstract</a>
    <div id="po221" class="abstract" style="display: none;">
    <p>
    Highly-available and cost-effective large-scale distributed key-value (KV) stores with consistency guarantees are at the heart of many Internet-based applications. To ensure high availability and scalability most KV stores replicate data across multiple servers. Recently, erasure-code based distributed KV stores have gained interest due to their low storage and bandwidth costs.  Existing erasure-code based implementations of strongly consistent KV stores either rely on a leader to propagate the coded elements or use expensive consensus protocols to handle concurrent operations. Furthermore, only a handful of strongly consistent  KV stores that uses  erasure-codes are available.  Here, we design a novel erasure-code based, strongly consistent, KV store algorithm and then implement it. Our algorithm  is designed with practical considerations in mind therefore, our KV-store algorithm embodies many desirable attributes of an effective distributed storage system such as,   the ability to tolerate client and server failures;  graceful handling of asynchrony, without assuming common clock or failure detector or using timeouts etc;  low communication rounds during  read (GET) and write (PUT) operations;  highly available and near-optimal storage and communication costs.  Our KV-store mechanism can handle any number of concurrent PUTs and GETs on the stored data. Strong consistency and availability properties  of our algorithm are analytically proven. We experimentally study the performance of the KV-store  under a wide range of scenarios and compare with a quorum based replicated system. Our implementation closely follows  our  algorithm  specification of and hence   does not rely on additional distributed computing services or primitives like leader-election or consensus protocol to achieve coordination and synchronization.  Our experiments show that our KV-store  achieves substantially lower storage and bandwidth costs and has a significantly lower latency of operations than the replication-based mechanisms.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Joana M. F. da Trindade (MIT), Carlo Curino (Microsoft), Konstantinos Karanasos (Microsoft), Samuel Madden (MIT), Julian Shun (MIT)
    <br/>
    <strong>
    Kaskade: Graph Views for Efficient Graph Analytics
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po231')">Toggle Abstract</a>
    <div id="po231" class="abstract" style="display: none;">
    <p>
    Graphs are an increasingly popular way to model real-world entities and relationships between them, ranging from social networks to data lineage graphs and biological datasets. Queries over these large graphs often involve expensive sub-graph traversals and complex analytical computations. These real-world graphs are often substantially more structured than a generic vertex-and-edge model would suggest, but this insight has remained largely unexplored by existing graph engines for graph query optimization purposes. Therefore, in this work, we focus on leveraging structural properties of graphs and queries to automatically derive materialized graph views that can dramatically speed up query evaluation. We present Kaskade, the first graph query evaluation engine that exploits materialized graph views for query optimization purposes. Kaskade employs a novel inference-based view enumeration technique that significantly reduces the search space of views that need to be considered. More- over, it introduces a graph view size estimator to pick the most beneficial views to materialize given a query set and to select the best query evaluation plan given a set of materialized views. We evaluate its performance over real-world graphs, including the provenance graph that we maintain at Microsoft to enable auditing, service analytics, and advanced system optimizations. Our results show that Kaskade substantially reduces the effective graph size and yields significant performance speedups (up to 50x), in some cases making otherwise intractable queries possible.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Amir Rahimzadeh Ilkhechi (Brown University), Yicong Mao (Brown University), Alex Galakatos (Brown University), Andrew Crotty (Brown University), Ugur Cetintemel (Brown University)
    <br/>
    <strong>
    DeepSqueeze: Using deep learning models to compress relational databases
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po241')">Toggle Abstract</a>
    <div id="po241" class="abstract" style="display: none;">
    <p>
    Abstract The rapid proliferation of enormous datasets has made efficient data compression more important than ever. Traditional columnar compression techniques (e.g., dictionary encoding, run-length encoding) have proven highly effective in modern OLAP systems, but they typically compress individual columns without considering potential relationships between columns, such as functional dependencies and attribute correlations. Semantic compression, on the other hand, directly leverages these relationships to store only a subset of the attributes necessary to infer the others, but this approach cannot efficiently model subtle or complex relationships between more than a few attributes at a time.  We, therefore, propose DeepSqueeze, a novel semantic compression framework that uses deep learning models to efficiently capture complex relationships in relational data. In contrast to existing semantic compression techniques that model individual relationships between attributes, DeepSqueeze captures all columnar relationships together by directly mapping tuples to a lower-dimensional representation through the use of deep auto-encoders, leading to significant compression rate gains. In fact, our initial results demonstrate that DeepSqueeze can achieve up to 20% reduction in data size compared to state-of-the-art alternatives.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Leonhard Spiegelberg (Brown University), Tim Kraska (MIT)
    <br/>
    <strong>
    [POSTER] Robust data centric code generation
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po251')">Toggle Abstract</a>
    <div id="po251" class="abstract" style="display: none;">
    <p>
    In the world of data preparation Python is winning the race nowadays. Not only because of its large universe of modules and its community, but also due to its ease of use, expressiveness and high level features. Another advantage is its near to zero startup time which especially supports fast prototyping and quick delivery of data analytics tasks. Besides, it became the ultimate glue language to bring different frameworks together; predominantly in the domain of both analytics and Machine Learning frameworks. It is not uncommon for large enterprises to build complex pipelines not only using Python at the prototype stage but also to deploy them eventually for production use. However, its flexibility and typing issues are also its curse preventing robust, competitive and native compilation largely. Modern big data frameworks like Apache Spark or Ray rely on the cloudpickle module for remote execution of Python UDFs using the Python interpreter. However, their execution speed is limited since they still use the interpreter ultimately and do not take advantage of both the monadic nature of MapReduce pipelines and the inherent patterns in the data. In this project, we first implemented a robust data centric compilation mode for Python UDFs. Then, we created a more robust big data analytics framework called Tuplex and used our novel execution model to provide overall speedups while allowing users to deal more efficiently with exceptions caused by ill-formatted input data. This poster shows our journey started by our wish to design a faster alternative to PySpark towards a more robust and efficient data analytics framework.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Rundong Li (Northeastern University), Mirek Riedewald (Northeastern University), Wolfgang Gatterbauer  (Northeastern University), Dhaval Patel (Northeastern University)
    <br/>
    <strong>
    Speeding Up Distributed Band Joins
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po261')">Toggle Abstract</a>
    <div id="po261" class="abstract" style="display: none;">
    <p>
    We consider the problem of speeding up band-join computation in the cloud. A simple example band-join is “SELECT * FROM S, T WHERE |S.A - B.A| <= b”, where b is called the bandwidth. To reduce running time in the cloud, one has to identify a partitioning that carefully trades input duplication for improved load balance across the worker machines. Band-join relaxes equi-join’s equality condition, allowing tuples with non-equal keys to match. The state of the art for this type of condition are recently-proposed theta-join algorithms. Those either suffer from excessive input duplication or rely on expensive geometric tiling algorithms. The latter could have higher computational complexity than the join itself when the condition involves multiple attributes. We propose a lightweight adaptive partitioning technique that maps a band-join to an equivalent equi-join. Optimization cost, even for complex band-joins with conditions on multiple attributes, does not exceed a few seconds. For band-joins on a single attribute, we provide optimality guarantees similar to those established for equi-joins. We also show that these guarantees do not carry over to band-joins on multiple attributes. Despite this negative analytical result, experiments with real and synthetic data demonstrate a reduction in running time over the state of the art by a factor of two or more.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Amin Saeidi (Microfocus Vertica), Ryan Roelke (Microfocus - Vertica)
    <br/>
    <strong>
    Vertica Eon mode and challenges
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po271')">Toggle Abstract</a>
    <div id="po271" class="abstract" style="display: none;">
    <p>
    Vertica has historically managed local disk for performance and reliability, at the cost of product complexity and scalability. Eon mode is a new Vertica architecture that puts the data on a shared storage with the promise of comparable performance on existing workloads and support for new types of workload. While the design reuses Vertica’s optimizer and execution engine, the metadata, storage, and fault tolerance mechanisms are rearchitected to reflect shared storage. Eon mode demonstrates promising performance, as well as, superior scalability and operational behavior.    This initial Eon mode design assumes that each Vertica node operates with a large local instance storage, which can be used for temporary data processing, as well as for caching files from the associated shared file system. We call this cache the “depot”. Using the depot means that the query executor only ever accesses files from a local file system. In the initial Eon mode deployments, several customers have preferred to use smaller depot instead. This results in most workloads reading files directly from shared storage rather than first fetching those files into the depot. This means file system operations comprise a much greater proportion of the query latency. This exposes some problems with Vertica’s query executor, such as:  - High cost of filesystem stat calls.  - Slow query planning and execution time when data/metadata files are not in the depot.  - Slow and costly merge-out operations with lots of small reads.  - Failure to load data sets larger than the depot.   We have improved Vertica’s design to resolve each of these issues. In our presentation, we’d like to discuss how we resolved the above issues by re-designing some key components of Vertica.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Yizhou Yan (Worcester Polytechnic Institute), Lei Cao (MIT), Samuel Madden (MIT), Elke Rundensteiner (WPI)
    <br/>
    <strong>
    SeqDB: A Database for Exploring Sequence Data
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po281')">Toggle Abstract</a>
    <div id="po281" class="abstract" style="display: none;">
    <p>
    Modern data intensive applications generate massive amounts of time-stamped data, much of it in the form of discrete, symbolic sequences produced from online customer transactions, sensors, and system logs. While valuable knowledge in such sequence data could be detected by discovering interesting sequential patterns, to date there is no suitable technology supporting interactive exploration of such patterns. In this work, we present a system, SeqDB, that efficiently supports a rich variety of pattern exploration requests with varying pattern semantics and input parameter settings. SeqDB pro-vides a sequence pattern query language (PQL) that enables the users to issue pattern-related query and mining requests. A query plan generator then produces an efficient execution plan for each PQL query. Further, SeqDB employs an innovative index structure that succinctly summarizes sequence data using a small set of sequential patterns. Using the index, PQL plans are mapped to effective execution strategies for pattern query and mining. As a result, SeqDB provides near-real time responsiveness even on large sequence datasets. Our experimental evaluation using several real world and synthetic sequence data sets demonstrates the versatility and efficiency of SeqDB, showing it to be 3 orders of magnitude faster or more than the current state-of-the-art pattern query and mining techniques.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Andy Vidan (Composable Analytics, Inc.)
    <br/>
    <strong>
    Composable Adaptive Decision Engine
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po291')">Toggle Abstract</a>
    <div id="po291" class="abstract" style="display: none;">
    <p>
    In this talk, we begin by describing dataflow programming and its ability to facilitate distributed data and query integration, allowing users to ingest an unlimited number of information sources in a variety of formats and integrate them using a flow-based application. In this way, entire datasets can be merged and mastered in a single system, while defining a layer of custom analytics (“rules”) as well as a visual web UI for user input and reporting. The analytics layer can include computer vision (for OCR and image processing), natural language processing (for text and document parsing), and other machine learning algorithms. We will then introduce the Composable Analytics platform, originally developed at MIT Lincoln Laboratory, and describe how we can use these methods to develop an adaptive decision and scoring engine.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Andy Vidan (Composable Analytics, Inc.)
    <br/>
    <strong>
    Composable DataOps
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po301')">Toggle Abstract</a>
    <div id="po301" class="abstract" style="display: none;">
    <p>
    Enter DataOps. This emerging trend is a way for enterprises to build processes around data management and analytics. At a high level, DataOps is similar to the more familiar DevOps. Where DevOps allows organizations to modernize their approach to software development, DataOps establishes a modern framework for data practices, which can help close the gap that companies discovered when pursuing Big Data. In this poster, we describe the technical features of the Composable DataOps Platform, originally developed at MIT Lincoln Laboratory, and its use as a full-stack analytics platform, with built-in services for data orchestration, automation and analytics.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Chi Zhang (Brandeis university)
    <br/>
    <strong>
    DQN-Join: A Learning-based Join Ordering Enumeration Techniques
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po311')">Toggle Abstract</a>
    <div id="po311" class="abstract" style="display: none;">
    <p>
    Query optimization is a process that a database or data warehouse takes an input from users, and reworks that question to deliver a quick response with as few computing resources as possible. The number of query plan choices reach far beyond human’s calculation. It is critical for query optimizer to select a physical execution plan that can improve performance and reduce costs in database systems. However, when query optimizer starts to select a good plan, it turns out that this so-called good plan is actually not very good. In other words, bad plans will always be on spot and potential better plans may be hidden forever.  Consequently, join order selection, as one of the most well-studied problems in database systems, plays a significant role in solving the challenge I mentioned above. Inspired by a new algorithm Deep Q-learning from Demonstration (DQFD), A new join ordering enumeration technique will be introduced in our poster. In support of learning from demonstration and DQFD, our self-designed DQN-Join algorithm is able to outperform traditional query optimizer and find better plans with lower latency.  DQN-Join consider join ordering enumeration as a reinforcement learning problem. First and foremost, when a query workload comes in, each query q will be executed and transformed into various states vectors s and actions a. We use latency as a reward signal to measure quality of each selected action on each state. latency will be recorded as L on each state after a query is successfully executed. Figure 1 shows how a query is transformed into pairs of states and actions. Next, our DQN-Join agent will use states as input and latency as output to mimic reward function. With the help of demonstration data, our DQN-Join agent will acquire similar behaviors(actions) and performance of traditional query optimizer. Last but not least, our DQN-Join agent will start to react from real world. For example, A new SQL query will firstly be transformed into a vector of the initial state. The initial state will then be sent to our neural network framework as input and a reward output including all actions will be generated. The DQN-Join agent will take an action with highest reward and move to a next state. The next state will be sent to our neural network again to get a new action. The process won’t stop until reaching terminal state.  It is beneficial for query optimizer to adapt DQN-Join algorithm and find a better query plan with lower cost and latency. Meanwhile In support of theory from learning from demonstration our model can largely reduce training time and reach a fast convergence. Finally, we can also prove that deep reinforcement learning can be widely applied in database systems especially in query optimization.  
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Dan Zhang (University of Massachusetts Amherst), Michael Hay (Colgate University), Gerome Miklau (University of Massachusetts Amherst), BRENDAN  O'CONNOR (University of Massachusetts Amherst ),  Ali Sarvghad (university)
    <br/>
    <strong>
    Challenges of visualizing Differentially Private Data
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po321')">Toggle Abstract</a>
    <div id="po321" class="abstract" style="display: none;">
    <p>
    Differential privacy has become a primary standard for protecting individual data while supporting flexible data analysis. Despite the adaptation of differential privacy to a wide variety of applications and tasks, visualizing the output of differentially private algorithms has rarely been considered. Visualization is one of the primary means by which humans understand and explore an unknown dataset and therefore supporting visualization is an important goal to advance the practical adoption of differential privacy.  We explore key challenges and propose solution approaches. Using two-dimensional location data as an example domain, we consider the challenges of plotting noisy output, the impact of visual artifacts caused by noise, and the proper way to present known uncertainty about private output. We propose the indistinguishability principle as an attempt to one of the major challenges: visualization under uncertainty. 
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Erfan Zamanian (Brown University), Julian Shun (MIT), Carsten Binnig (TU Darmstadt), Tim Kraska (MIT)
    <br/>
    <strong>
    Chiller: Contention-centric Transaction Execution and Data Partitioning for Fast Networks
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po331')">Toggle Abstract</a>
    <div id="po331" class="abstract" style="display: none;">
    <p>
    Distributed transactions on high-overhead TCP/IP-based networks were conventionally considered to be prohibitively expensive and thus were avoided at all costs. To that end, the primary goal of almost any existing partitioning scheme is to minimize the number of cross-partition transactions. However, with the next generation of fast RDMA-enabled networks, this assumption is no longer valid. In fact, recent work has shown that distributed databases can scale even when the majority of transactions are cross-partition. In this paper, we first make the case that the new bottleneck which hinders truly scalable transaction processing in modern RDMA-enabled databases is data contention, and that optimizing for data contention leads to different partitioning layouts than optimizing for the number of distributed transactions. We then present Chiller, a new approach to data partitioning and transaction execution, which minimizes data contention for both local and distributed transactions. Finally, we evaluate Chiller using TPC-C and a real-world workload, and show that our partitioning and execution strategy outperforms traditional partitioning techniques which try to avoid distributed transactions, by up to a factor of 2 under the same conditions.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Ani Kristo (Brown University), Kapil Vaidya (MIT), Ugur Cetintemel (Brown University), Tim Kraska (MIT)
    <br/>
    <strong>
    A Learned Sorting Algorithm
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po341')">Toggle Abstract</a>
    <div id="po341" class="abstract" style="display: none;">
    <p>
    Sorting is one of the most fundamental algorithms in computer science and a common operation in databases, appearing in indexing and query processing (e.g., ORDER BY clauses and sort-merge joins). In this work, we introduce a new type of distribution sort that leverages a learned model of the empirical CDF of the data in order to generate a sorted order of keys. The algorithm uses a Recursive Model Index (RMI) to efficiently get an approximation of the scaled CDF for each record key and map it to the corresponding position in the output array. We then apply a deterministic sorting algorithm that works well on nearly-sorted arrays (e.g., Insertion Sort) to establish a completely sorted order. In contrast to comparison-based sorts like Quicksort, this approach heavily reduces the number of conditional branches, hence resulting in fewer CPU stalls, branch mispredictions, and cache faults. We compared this algorithm against common sorting approaches and measured its running time performance for 50M randomly-distributed double-precision Gaussian keys. The results show that our approach yields a 2.6_ performance improvement over C++ STL sort, which is an optimized Quicksort hybrid, 3.88_ improvement over Timsort, which is the default sorting function for Java and Python, and 1.18_ over Radix sort, which is a common counting-based sort algorithm. We believe that these results reveal the potential for learned sorting approaches, which we expect to be the largest for parallel sorting, large key sizes, as well as with GPU, TPU, and FPGA accelerators. Our ongoing work is exploring several open research challenges that include building SIMD-optimized implementations of the algorithm to determining its complexity class.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Darshana Balakrishnan (State University of New York at Buffalo), Oliver A Kennedy (University at Buffalo, SUNY), Lukasz Ziarek (University at Buffalo, SUNY)
    <br/>
    <strong>
    Frankeinstein's Data Structure
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po351')">Toggle Abstract</a>
    <div id="po351" class="abstract" style="display: none;">
    <p>
    In most standard database systems creating or modifying a primary index is a very time-consuming process as indexes typically need to be rebuilt from scratch. Adaptive indexing is a form of incremental index organization in every query posted to the system is considered as a hint for incrementally building the index. This type of incremental indexing puts a little penalty on the initial few queries but they will eventually get answered faster when compared to building a full static index and then scanning over it. However, an adaptive index structure e.g. a sorted array or a B-Tree that is initially optimal for one type of workload becomes sub-optimal as the workload changes.  Just-In-Time Data Structures is a generalized approach to adaptive indexing that dynamically adapts to changing workloads even after index convergence. We show that it is possible to simulate the behavior of the Just-In-Time data Structure so as to help us estimate appropriate heuristics for dynamic and adaptive index creation.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Lucas P Hendren (SimplyVital health)
    <br/>
    <strong>
    DLT Enabled Database
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po361')">Toggle Abstract</a>
    <div id="po361" class="abstract" style="display: none;">
    <p>
    Based on research done by the MIT media lab group MedRec we have developed a blockchain enabled database that utilizes distributed ledger technology to govern and determine access to data and create an audit trail for the data.  This is currently deployed live on the Rinkeby Test Network.  We are continuing to develop and gather data on this.  This poster will showcase this setup and initial findings.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Nesime Tatbul (Intel Labs and MIT), Stan Zdonik (Brown University), Mejbah Alam (Intel Labs), Justin Gottschlich (Intel Labs)
    <br/>
    <strong>
    Novel Approaches for Time Series Anomaly Detection
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po371')">Toggle Abstract</a>
    <div id="po371" class="abstract" style="display: none;">
    <p>
    It is widely anticipated that time series data will dominate our future. From autonomous driving to industrial automation, the inevitable rise of the Internet of Things (IoT) has already exposed us to billions of data sources generating time varying data at unprecedented complexity and scale. A key capability that lies at the core of managing time series data is the identification of patterns that signify "interesting" phenomena, such as anomalies. Anomalies are patterns in data that do not conform to expected (or, normal) behavior. Detecting anomalies before, after, or as they occur not only finds use in many mission-critical domains, but also empowers systems and users with the ability to cope with large data volumes by guiding attention and resources to information that matters the most.  Although anomaly detection has been actively studied for many decades, there are several challenges to be overcome before it can become practical in real-world deployments of time series applications. In particular, anomaly detection is a highly domain-specific problem, making models and algorithms widely variable. Furthermore, anomalies are relatively rare or unique events, possibly occurring in noisy or complex data environments (e.g., IoT, multi-variate or multi-resolution time series). This not only limits access to high-quality training data, but also complicates the whole training process. In order to deal with such challenges, it is critical to build tools that will enable data scientists to productively develop and interact with time series anomaly detection systems.  This talk will give examples from our ongoing research in this direction. In particular, we would like to share our recent results about a new accuracy scoring model that we developed for time series classification [1]. Time permitting, we will also present our follow-up work in progress on Greenhouse, a novel ML system for time series anomaly detection [2].  [1] N. Tatbul, T. J. Lee, S. Zdonik, M. Alam, and J. Gottschlich. Precision and Recall for Time Series. In 32nd Annual Conference on Neural Information Processing Systems (NeurIPS’18), Montreal, Canada, December 2018.  [2] T. J. Lee, J. Gottschlich, N. Tatbul, E. Metcalf, and S. Zdonik. Greenhouse: A Zero-Positive Machine Learning System for Time Series Anomaly Detection. In Inaugural Conference on Systems and Machine Learning (SysML’18), Stanford, CA, February 2018. 
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Tianyu Li (Carnegie Mellon University), Matt Butrovich (Carnegie Mellon University), Amadou Ngom (Carnegie Mellon University), Andrew Pavlo (Carnegie Mellon University)
    <br/>
    <strong>
    Fast Columnar Multi-Versioned In-Memory Transactional Storage Engine for Lightweight Transformation to Apache Arrow
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po381')">Toggle Abstract</a>
    <div id="po381" class="abstract" style="display: none;">
    <p>
    In-memory columnar formats, such as Apache Arrow, are becoming more popular for modern data analytics. These formats, however, are designed to be read-only. This means that a OLTP DBMS generates the data and then a data scientist need to use a heavy-weight transformation process to load the data into the desired columnar format. We aim to reduce or even eliminate this process by building a transactional DBMS that is aware of the eventual usage of its data and operates directly on Arrow-style storage blocks. We introduce several relaxations of Arrow’s invariants to make it amenable to transactional access, and rely on a lightweight in-memory transformation process to convert the data to Arrow when it is no longer active transactionally (i.e., cold). We built our system based on the concurrency control scheme of HyPer, and preliminary results show comparable performance with dedicated OLTP DBMS and much reduced overhead when exporting to Arrow.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Anil Shanbhag (MIT)
    <br/>
    <strong>
    How good are GPU-based databases really ?
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po391')">Toggle Abstract</a>
    <div id="po391" class="abstract" style="display: none;">
    <p>
    There has been significant amount of excitement and recent work on GPU-based database systems. Previous work has claimed that these systems can perform orders of magnitude better than CPU-based database systems on analytical workloads such as those found in decision support and business intelligence applications. A hardware expert would view these gains with suspicion. Given the general notion that database operators are memory bandwidth bound, one would expect the maximum gain to be roughly the bandwidth ratio i.e.: the ratio of the memory bandwidth of GPU to that of CPU. Also, given the restrictive GPU programming model which requires running program over a large number of threads, one would expect additional materializations leading to reduction in gain. The empirical nature of past works makes it hard to access the true nature of performance gains from using GPUs vs implementation artifacts.  In this work, we adopt a model-based approach to understand why the performance gain got from running the query on GPU vs on CPU deviates from the bandwidth ratio. We propose Blockwise, a library of parallel routines that can be combined together to run full SQL queries on the GPU with minimal materializations. First, we implement individual query operators to show that while speedup for selection and projection are around bandwidth ratio, join and sort achieve speedup smaller than the bandwidth ratio due to difference in hardware capabilities. We present a new join algorithm called Hybrid Join for GPUs that works around limitation of running radix join on GPUs. We show on a popular analytical workload that full query performance gain from running on GPU exceeds the bandwidth ratio despite individual operators having speedup less than bandwidth ratio due to limitation of vectorizing chained operators on CPUs. Finally, we do a performance/cost analysis to show that GPUs can be around 3x more cost-effective when the critical section of the computation fits in GPU memory.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Matthew Perron (MIT), Raul Castro Fernandez (MIT), Samuel Madden (MIT), David DeWitt (MIT)
    <br/>
    <strong>
    Analytics on Cloud Function Services
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po401')">Toggle Abstract</a>
    <div id="po401" class="abstract" style="display: none;">
    <p>
    Recent research demonstrates the benefit of cloud function services, like AWS Lambda, for parallel workloads like video encoding. Some are skeptical that cloud function services, as they exist today, are are a useful tool for workloads with more complex communication patterns. We demonstrate that TPC-H queries, executed on cloud function services and cloud storage, like Amazon S3, can be cost and performance competitive with the best provisioned systems reading from cloud storage, including Apache Hive, Presto, Amazon Redshift Spectrum, and Amazon Athena.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Michael Gubanov (Florida State University)
    <br/>
    <strong>
    Nested Dolls: Towards Unsupervised Clustering of Web Tables
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po41')">Toggle Abstract</a>
    <div id="po41" class="abstract" style="display: none;">
    <p>
    Here we discuss our initial efforts towards unsupervised clustering of a large-scale Web tables dataset. We improve our previous approach of weakly-supervised clustering, where an operator would provide a few descriptive keywords to generate an entity-identifying classifier, which is applied to the corpora to form a cohesive entitycentric cluster [1]. Here, we make a next step towards fully unsupervised algorithm by automatically generating these descriptive keywords. These keywords then can be used to generate high-precision training data and train a classifier to form a cluster. Here, we describe and evaluate this new unsupervised keyword generation algorithm and apply it to a large-scale Web tables corpus to form initial small high-precision clusters. The clusters are based on attributes, characterizing an entity, for example artist, title, album for Songs.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Walter Gerych (Worcester Polytechnic Institute), Emmanuel Agu (WPI), Elke Rundensteiner (WPI)
    <br/>
    <strong>
    Classifying  Depression  in  Imbalanced  Datasets  using  an Autoencoder-Based  Anomaly  Detection  Approach
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po421')">Toggle Abstract</a>
    <div id="po421" class="abstract" style="display: none;">
    <p>
    Depression is the most prevalent mental health ailment in the United States, affecting 15% of the population. Untreated depression can significantly decrease quality of life, physical health, and has significant economic and societal costs. The traditional method of diagnosing depression requires the patient to respond to medical questionnaires and is subjective. Passive methods to autonomously detect depression are desirable. Prior work on smartphone sensing of depression has utilized machine learning classification of smartphone sensor data. However, as with many ailments, the percentage of afflicted users in most populations is small compared with those unaffected, leading to severe class imbalance. In this work, we explore  anomaly detection methods as a method for mitigating class imbalance for depression detection. Our approach adopts a multi-stage machine learning pipeline. First, using autoencoders, we project the mobility features of the majority class (undepressed users). Thereafter, the trained autoencoder  then  classifies a test set of users as either depressed (anomalous) or not depressed (inliers) using a One Class SVM algorithm. Our method, when applied to the real-world StudentLife data set shows that even with an extremely imbalanced dataset, our method is able to detect individuals with depression symptoms with an AUC-ROC of 0.92, significantly outperforming traditional machine learning classification approaches.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Hyungsik Caleb Kim (DePaul University), Alexander Rasin (DePaul University), James Wagner (DePaul University)
    <br/>
    <strong>
    The art of pruning query plan search space: optimization of queries with user-defined  predicates 
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po431')">Toggle Abstract</a>
    <div id="po431" class="abstract" style="display: none;">
    <p>
    The use of user-defined functions in today’s relational databases are common especially as they are frequently integrated with other relational or non-relational databases. When a query contains expensive user-defined predicates, early pruning of vast search space by applying greedy heuristics may result in suboptimal execution plan. On the other hand, dynamic programming provides a thorough enumeration of search space yet is too expensive. In our presentation, we describe a dynamic approach that skips similar enumerations unit of joins and user-defined predicates. Our solution significantly reduces the search space while maintaining quality of the plan. As the number of tables and the number of user-defined predicates in a query grows, the pruning algorithms reduces the search space by greater proportion. We evaluate our algorithm for query plan enumeration based on search space enumeration complexity and the resulting quality of query plans.  
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Aristotelis Sigiouan Leventidis (Northeastern University), Wolfgang Gatterbauer (Northeastern University), Cody Dunne (Northeastern University), Mirek Riedewald (Northeastern University)
    <br/>
    <strong>
    I See What This Query Does: Visualising SQL Queries
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po441')">Toggle Abstract</a>
    <div id="po441" class="abstract" style="display: none;">
    <p>
    Complex SQL queries can be hard to understand, even for experienced programmers. We present QueryViz, a novel tool that can transform a SQL query over a given database schema into a visual representation. QueryViz aims to enhance query interpretation with concrete minimal visual elements that help the user capture the intent of a query quickly. It relies on the concise translation of first order logic into visualization by identifying the equivalence of SQL to that of logic representations. QueryViz automatically decides which visual elements to use and how to arrange them. Intuitively, our system behaves like a traditional optimizer, but optimizes for visual clarity instead of performance. We argue that it would be fairly straightforward to integrate QueryViz into existing DBMS, where it could facilitate modification, re-use, and maintenance of non-trivial SQL queries. In addition, it is also ideally suited as a teaching tool. An initial user study confirms the feasibility and potential benefits of our system.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Dmytro Bogatov (Boston University), George Kollios (Boston University), Leo Reyzin (Boston University)
    <br/>
    <strong>
    A Comparative Evaluation of Order-Preserving and Order-Revealing Schemes, and Secure Range-Query Protocols
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po451')">Toggle Abstract</a>
    <div id="po451" class="abstract" style="display: none;">
    <p>
    Database query evaluation over encrypted data has received a lot of attention recently. Order Preserving Encryption (OPE) and Order Revealing Encryption (ORE) are two important encryption schemes that have been proposed in this area. These schemes can provide very efficient query execution but at the same time may leak some information to adversaries. We present the first comprehensive comparison among a number of important OPE and ORE schemes using a framework that we developed. We evaluate protocols that are based on these schemes as well. We analyze and compare them both theoretically and experimentally and measure their performance over database indexing and query evaluation techniques using not only execution time but also I/O performance and usage of cryptographic primitive operations. Our comparison reveals some interesting insights concerning the relative security and performance of these approaches in database settings. This work is based on our original paper submitted to VLDB.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Alexander Rasin (DePaul University), Priya Deshpande (DePaul University)
    <br/>
    <strong>
    Data Integration and Information Retrieval -- Two sides of the same coin:  Case Study using Biomedical Data Sources
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po461')">Toggle Abstract</a>
    <div id="po461" class="abstract" style="display: none;">
    <p>
    Data integration and information retrieval are two sides of the same coin which have their own individual identity. In data integration, design of the database schema needs to consider data retrieval to best use the integrated database system. Data integration and information retrieval modules can work individually, and at a glance one can see them as two independent entities. However, in practice one needs to consider both together. We therefore think that database community should place more emphasis on information retrieval research in database conferences. Here we will be presenting our work showing how these two techniques interact. We believe, based on a literature survey, that the interaction between integration and retrieval of data is similar across other domains, such as: collecting data from different sources, data cleaning, designing schemas, fetching data from integrated repository. One needs to apply domain specific knowledge to learn about data sources and write retrieval algorithm (e.g., using domain specific ontologies). We will be presenting a case study with biomedical data integration, IRIS (Integrated Radiology Image Search) engine we developed to query radiology data sources. Our system demonstrates how integration and retrieval algorithms support each other in an application that can be used by doctors, radiologists, and researchers in the medical domain.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Philipp Eichmann (Brown University), Franco Solleza (Brown University), Nesime Tatbul (Intel Labs and MIT), Stan Zdonik (Brown University)
    <br/>
    <strong>
    Metro-Viz: Visual Exploration of Time Series Anomalies
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po471')">Toggle Abstract</a>
    <div id="po471" class="abstract" style="display: none;">
    <p>
    Vast amounts of time series data are being generated by a variety of sources from IoT sensors to systems logs. Anomaly detection (AD) is a critical capability for detecting rare, interesting, or abnormal events over this data. While there has been much recent progress in machine learning models and algorithms for AD, these techniques by themselves are not enough to characterize the results or behaviors of anomaly detectors in a systematic and human-interpretable way. In this poster, we present Metro-Viz, a visual tool to help data scientists to detect, explore, and reason about time series anomalies and anomaly detectors over large datasets. Metro-Viz aims to efficiently support a number of interactive tasks from exploring anomalies at different time granularities to hypothesis testing, using memory management techniques specifically tailored to data access patterns required by these tasks.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Peter Ivanov (Northeastern University), Wolfgang Gatterbauer (NEU)
    <br/>
    <strong>
    Minimal Factorizations of the Lineage for Conjunctive Queries
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po481')">Toggle Abstract</a>
    <div id="po481" class="abstract" style="display: none;">
    <p>
    Probabilistic databases are databases where the existence of each entry is uncertain and has a certain probability associated with it. Data in the real world often contains some errors or is uncertain and we can model this using probabilistic databases.  When we evaluate a query on a probabilistic database, the result is a boolean expression (called a lineage) that we would like to calculate. This problem is #P-Hard for general queries.   We try to approximate the value of the boolean expression by finding the minimal factorization (minFac) of the lineage (i.e. finding an equivalent Boolean expression that minimizes the number of repetitions of the variables). We do this for self-join-free conjunctive queries, for which the general problem of calculating the value of the lineage is still #P-Hard.  Query evaluation over probabilistic databases where (i) read-once formulas are known allow exact PTIME solutions and (ii) non read-once formulas are known allow approximate solutions with an error that depends on the number of repetitions of the variables (i.e. the penalty is the number of occurrences minus number of distinct variables). Our problem is to minimize this penalty.  We found out that minFac(q,D) is NP-Complete for most queries, but there are special cases where it is in P. For the hard queries, we found a 2(k-1)-factor approximation of the minimal penalty where k is the number of atoms in the query. There is a class of queries (which we call linear queries) for which we have not decided the complexity yet. 
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Matteo Brucato (UMass Amherst), Nishant  Yadav (U Mass, Amherst), Azza Abouzied (New York University Abu Dhabi), Peter Haas (University of Massachusetts Amherst), Alexandra Meliou (University of Massachusetts Amherst)
    <br/>
    <strong>
    Stochastic Package Queries
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po491')">Toggle Abstract</a>
    <div id="po491" class="abstract" style="display: none;">
    <p>
    We must often make decisions in the face of uncertain data. For example, investors must decide which assets to sell or buy without knowing their future value. While probabilistic databases can manage uncertainty, they lack support for decision making, and package queries can only support deterministic decisions. In this talk, we introduce stochastic package queries to model optimization problems with uncertainty, close to the data. We introduce SPaQL, our declarative language for the specification of package queries with stochastic constraints and objectives, and our stochastic query evaluation method SummarySearch based on Monte Carlo summaries that can produce feasible and close-to-optimal packages with high probability.
    </p>
    </div>
    </td>
    </tr>
<tr>
    <td>
    Manasi Vartak (MIT)
    <br/>
    <strong>
    Data Science is growing up: building the DS/ML Infrastructure Backbone
    </strong><br/>
    <a href="javascript: toggleVisibility ('#po51')">Toggle Abstract</a>
    <div id="po51" class="abstract" style="display: none;">
    <p>
    Machine learning and data science applications have become ubiquitous in a variety of domains ranging from voice assistants to self-driving cars. As a result, data science and machine learning have gone from an offline, research-oriented activity to one that drives core product features and revenue. While investments in DS/ML have risen exponentially, we find that the process of developing and deploying DS/ML and the associated tools remain ad-hoc. In the “wild-west” of DS/ML tools, reproducibility, diagnosis, and deployment of production-ready models are massive battles for small and large companies alike. As we depend more and more on DS/ML models to run products and businesses, the stack supporting the development and deployment of DS/ML also needs to develop to be as robust and dependable as the software development stack.  A key component of this new DS/ML stack is the model management system. Just as databases provide the source of truth for data, model management systems provide the source of truth for models --- they track models starting with feature engineering and training, all the way to model deployment and debugging. At MIT, we developed ModelDB, the first open-source system for managing machine learning models. Since its release, our GitHub repository has garnered > 500 stars, has been cloned over a thousand times, and has been forked >100 times. Moreover, ModelDB has been adopted at multiple financial institutions, insurance companies and retail companies, and has served as inspiration for commercial model management systems. Most recently, we have expanded ModelDB to enable model deployment as well as debugging.   In this talk, I will give an overview of the need for a new DS/ML tool stack, the key role played by model management, our work on ModelDB, and new functions enabled by model management such as model deployment and debugging.
    </p>
    </div>
    </td>
    </tr>


</tbody>
</table>

</div>
