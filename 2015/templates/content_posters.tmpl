<div class="content-container">
   <h1>
      New England Database Day Posters
   </h1>
   <p>The poster session will include drinks and appetizers. It will be held in Building 32, R&D Commons, 4th Floor, Gates Tower. Take either set of elevators to the 4th floor.</p>
   <h3>Information for Poster Presenters</h3>
   <p>We will be able to provide poster board, easels, and mounting supplies for you.
      You will need to print your poster and bring it with you to the conference. We recommend posters to be either A0, A1, or ANSI D or E sizes  (either 24" by 32" or 36" by 42"). 
      Our poster boards are large enough to accommodate any of these. You may orient your poster as you see fit. If you prefer to print out individual 8.5" x 11" pages, our board will be large enough to 
      accommodate about 12 pages. However, we recommend making a single poster -- if you don't have access to a large format printer Fedex/Kinkos can print your poster for you.
   </p>
   <p>We will have a storage area for poster tubes at the conference. You will be able to set up your posters during the lunch break.</p>
   <h3>List of accepted posters:</h3>
   <table class="table table-bordered posterTable">
      <tbody>
         <tr>
   <td>
      Ramoza Ahsan, WPI; Rodica Neamtu, WPI; Elke Rundensteiner, WPI<br/>
      <strong>Harnessing the Power of Big Data:Using a Spatial Temporal Data Model to answer Big Questions for economic prosperity</strong><br/>
      <a href="javascript: toggleVisibility ('#po1A')">Click to toggle abstract.</a>
      <div id="po1A" class="abstract" style="display: none;">
         <p><i>Big data is the key to answer questions facing the society: economic, social, political, financial and educational. Despite its indisputable power or potential and its Omni presence, big data continues to be a challenge because of its complexity, diversity, disperse nature and most of all because of the difficulty in creating the appropriate tools to collect, integrate, clean and analyze it. In our approach, we are creating a spatial temporal data model to automatically integrate and transform data contained in spreadsheets collected from heterogeneous public data sources. We introduce the Data Integration through Object Modelling  (DIOM) framework which successfully overcomes some of the well-known challenges related to leveraging data from diverse sources, like diversity in schema formats and extracting processes.First we generalize and template the data from the spreadsheets and represent it into a unified data model. Secondly we automatically extract and transform the data from the spreadsheets, while profiling the meta data in the process. Lastly we provide users with the opportunity to interactively perform repairs before loading the data in the database.The experimental results confirm the merit of our approach and mark a huge step in enabling individuals and organizations and advocate for decisions for economic prosperity. The reduction in the cost of collecting processing and reporting data is tremendous.</i></p>
      </div>
   </td>
</tr>
<tr>
   <td>
      Caitlin Kuhlman, Worcester Polytechnic Institut<br/>
      <strong>Solving Data Integration Challenges for Economic Competitiveness Analytics: MATTERS Dashboard</strong><br/>
      <a href="javascript: toggleVisibility ('#po2A')">Click to toggle abstract.</a>
      <div id="po2A" class="abstract" style="display: none;">
         <p><i>In our increasingly data driven world, having the tools to find meaningful insights in data collected from across diverse sources is essential. The Massachusetts Technology, Talent, and Economy Reporting System is an online analytics dashboard which allows users to explore a large data set of high fidelity cost and talent competitive metrics aggregated from publicly available online sources. Enabling the comparison of metrics across sectors, states and years, the system provides a powerful platform for reasoning about past trends in the technology economy in Massachusetts, the current economic climate for STEM industries, and the future of science and technological discovery in our area through predictive 'what if' analytics. This research addresses some of the many challenges in building a robust, stable and scalable system such as MATTERS for performing analytics across multiple heterogeneous data sources. Techniques are evaluated for resolving discrepancies between formats and cleaning noisy or erroneous data. Pipelines for the automatic extraction, transformation and loading of data have been developed, and the potential benefit of using data integration workflow tools to facilitate the addition of new sources has been explored. The impact of the White House's Open Data Initiative which promises to make more government data available in machine readable formats is also investigated.  </i></p>
      </div>
   </td>
</tr>
<tr>
   <td>
      Yue Wang, UMass Amherst; Alexandra Meliou, ; Gerome Miklau, <br/>
      <strong>Lifting the haze off the cloud: a market-based approach to resource allocation in the cloud</strong><br/>
      <a href="javascript: toggleVisibility ('#po3A')">Click to toggle abstract.</a>
      <div id="po3A" class="abstract" style="display: none;">
         <p><i>The availability of public computing resources in the cloud has revolutionized data analysis, but requesting cloud resources often involves complex decisions for consumers. Under the current pricing mechanisms, cloud service providers offer several service options and charge consumers based on the resources they use. Before they can decide which cloud resources to request, consumers have to estimate the completion time and cost of their computational tasks for different service options and possibly for different service providers. This estimation is challenging even for expert cloud users.   We propose a new market-based framework for pricing computational tasks in the cloud. Our framework introduces an entity called an agent, who acts as a broker between consumers and cloud service providers. The agent takes data and computational tasks from users, estimates time and cost for evaluating the tasks, and returns to consumers contracts that specify the price and completion time for each task. Our framework can be applied directly to existing cloud markets, as it does not alter the way cloud providers offer and price services. In addition, it simplifies cloud use for consumers by allowing them to compare contracts, rather than choose resources directly. We propose three types of contracts, we explain how agents price them, and we analyze their properties. Finally, we apply our market framework to a real-world cloud service to demonstrate and experimentally validate three key properties:  (a) competitiveness among agents ensures that consumers benefit from using the market,  (b) agents have incentive to price contracts fairly, and  (c) inaccuracies in estimates do not pose a significant risk to agents' profits.  </i></p>
      </div>
   </td>
</tr>
<tr>
   <td>
      Zheng Li, UMass Lowell; Tingjian Ge, UMass Lowell<br/>
      <strong>Order Approximate Interleaving Event  Matching</strong><br/>
      <a href="javascript: toggleVisibility ('#po4A')">Click to toggle abstract.</a>
      <div id="po4A" class="abstract" style="display: none;">
         <p><i>Most of the recent complex event semantics is based on regular expressions, extended with additional filters such as window constraints. We observe that many applications today require parallel  (interleaving) event patterns. Moreover, we observe that this matching needs to be approximate in terms of event orders and missing events. We first propose the query semantics. Then we devise a foundation algorithm, on top of which two optimization techniques are proposed. Finally, we perform a comprehensive experimental evaluation using three real-world datasets in different domains and synthetic datasets.</i></p>
      </div>
   </td>
</tr>
<tr>
   <td>
      Salah Ahmed, WPI; Olga Poppe, WPI; Yun Lu, WPI<br/>
      <strong>Event Sequence Analytics over High-Rate Event Streams</strong><br/>
      <a href="javascript: toggleVisibility ('#po5A')">Click to toggle abstract.</a>
      <div id="po5A" class="abstract" style="display: none;">
         <p><i>Many stream-based applications such as stock trend analytics, infection spread prevention and check fraud detection involve construction of longest event sequences over event streams arriving at high rate and possibly out-of-order. If events arrive in-order, they can be simply appended to all event sequences computed so far. In contrast to that, every out-of-order event can provoke re-computation of all event sequences to construct new once. To tackle this challenge, we represent an event stream as a Directed Acyclic Graph  (DAG) the nodes of which are events and the edges of which are immediate successor relations between these events. Furthermore, we propose a set of DAG-search-based algorithms computing all longest event sequences and compare their complexity with respect to CPU time and memory consumption. Based on this analysis and thorough experimental study, we conclude that a combination of two algorithms  (namely DFS memorizing intermediate results computed on DAG partitions and DFS combining these intermediate results to final results without memorizing them) achieves the best performance.</i></p>
      </div>
   </td>
</tr>
<tr>
   <td>
      Mingrui Wei, WPI<br/>
      <strong>Online Outlier Exploration Over Big Data</strong><br/>
      <a href="javascript: toggleVisibility ('#po6A')">Click to toggle abstract.</a>
      <div id="po6A" class="abstract" style="display: none;">
         <p><i>Traditional outlier detection systems process each individual outlier detection request specified with particular parameter setting one at a time. This is not only prohibitively time-consuming for big data sets, but also tedious for analysts as they explore the data to hone in on the most appropriate parameter setting and desired results. In this work, we present the first online outlier exploration platform, called ONION, that enables analysts to effectively explore and understand anomalies even in big data sets. First, ONION features an innovative interactive anomaly exploration model that offers an "outlier-centric panorama" into big data sets along with rich classes of new exploration types. Second, ONION proposes an online processing paradigm. Via a preprocessing phase, ONION compresses down the row big data into a knowledge-rich O-Space abstraction that pre-codes critical interrelationships of outlier candidates so to support subsequent interactive outlier analysis. Furthermore upon this abstracted O-Space our ONION infrastructure provides three classes of data structures and associated online processing strategies. As demonstrated by our extensive experiments with big real datasets, each of the advanced exploration types can be supported with milliseconds response time.</i></p>
      </div>
   </td>
</tr>
<tr>
   <td>
      CHUAN LEI, WPI; Zhongfang Zhuang, Worcester Polytechnic Institut; Elke Rundensteiner, WPI; Mohamed Eltabakh, WPI<br/>
      <strong>Shared Execution of Recurring Workloads in MapReduce</strong><br/>
      <a href="javascript: toggleVisibility ('#po7A')">Click to toggle abstract.</a>
      <div id="po7A" class="abstract" style="display: none;">
         <p><i>With the increasing complexity of data-intensive MapReduce workloads, Hadoop must often accommodate hundreds or even thousands of recurring analytics queries that periodically execute over frequently updated datasets, e.g., latest stock transactions, new log files, or recent news feeds. For many applications, such recurring queries come with user-specified service-level agreements  (SLAs), commonly expressed as the maximum allowed latency for producing results before their merits decay. The recurring nature of these emerging workloads combined with their SLA constraints make it challenging to share and optimize their execution. While some recent efforts on multi-job optimization in the MapReduce context have emerged, they focus on sharing work among ad-hoc jobs on static datasets. Unfortunately, these sharing techniques neither take the recurring nature of the queries into account nor guarantee the satisfaction of SLA requirements. In this work, we propose the first scalable multi-query sharing engine tailored for recurring workloads in the MapReduce infrastructure, called Helix. First, Helix deploys sliced window alignment techniques to reveal sharing opportunities among recurring queries without introducing additional MapReduce overhead. Based on the aligned slices, Helix leverages an integrated model of sharing groups and execution ordering to efficiently produce an optimized shared execution plan for all recurring queries in a single pass. Our experimental results over real-world datasets confirm that Helix significantly outperforms the state-of-art techniques by an order of magnitude.</i></p>
      </div>
   </td>
</tr>
<tr>
   <td>
      Vijay Gadepally, Massachusetts Institute of Tec; Sherwin Wu, Quora; Jeremy Kepner, ; Samuel Madden, MIT<br/>
      <strong>Sifter: A Generalized, Efficient, and Scalable Big Data Corpus Generator</strong><br/>
      <a href="javascript: toggleVisibility ('#po8A')">Click to toggle abstract.</a>
      <div id="po8A" class="abstract" style="display: none;">
         <p><i>Big data has reached the point where the volume, velocity, and variety of data place significant limitations on the computer systems which process and analyze this data. Working with large data sets is becoming increasingly difficult and researchers look for simple scalable systems that can be used to automatically develop a data corpus that one can use for testing or training models. Sifter is a system that supports efficient extraction of data subsets to a size that can be manipulated on a single machine. Sifter was developed as a big data corpus generator for scientists to generate these smaller datasets from an original larger one. Sifter's three-layer architecture allows for client users to easily create their own custom data corpus jobs, while allowing administrative users to easily integrate additional core data sets into Sifter. This poster presents the implemented Sifter system deployed on a Twitter dataset. </i></p>
      </div>
   </td>
</tr>
<tr>
   <td>
      Lei Cao, WPI<br/>
      <strong>Detecting Moving Object Outliers In Massive-Scale</strong><br/>
      <a href="javascript: toggleVisibility ('#po9A')">Click to toggle abstract.</a>
      <div id="po9A" class="abstract" style="display: none;">
         <p><i>The detection of abnormal moving objects over high-volume trajectory streams is critical for real time applications ranging from military surveillance to transportation management. Yet this problem remains largely unexplored. In this work, we first propose classes of novel trajectory outlier definitions that model the anomalous behavior of moving objects for a large range of real time applications. Our theoretical analysis and empirical study on the Beijing Taxi and GMTI  (Ground Moving Target Indicator) datasets demonstrate its effectiveness in capturing abnormal moving objects. Furthermore we propose a general strategy for efficiently detecting the new outlier classes. It features three fundamental optimization principles designed to minimize the detection costs. Our comprehensive experimental studies demonstrate that our proposed strategy drives the detection costs 100-fold down into practical realm for applications producing high volume trajectory streams to utilize.</i></p>
      </div>
   </td>
</tr>
<tr>
   <td>
      Eirik Bakke, MIT<br/>
      <strong>SIEUFERD Episode 2: A Visual Query Language</strong><br/>
      <a href="javascript: toggleVisibility ('#po10A')">Click to toggle abstract.</a>
      <div id="po10A" class="abstract" style="display: none;">
         <p><i>SIEUFERD is the Schema-Independent End-User Front-End for Relational Databases. In the first installment, "Automatic Layout of Structured Hierarchical Reports"  (InfoVis 2013), we showed how to take arbitrary nested relational data and generate table-, form-, and report-style data displays of the kind often found in FileMaker- or Microsoft Access-style database applications. This year, the system adds the interaction features needed to fully specify such views from a graphical user interface, including provisions for sorting, filtering, and joining, and for specifying formulas and aggregation. The result is a general-purpose tool that lets a user connect to an arbitrary Postgres, MySQL, or Oracle database and construct a wide range of useful view queries in only a few clicks.  This project will be presented as a live demo on a monitor stand  (no poster).</i></p>
      </div>
   </td>
</tr>
<tr>
   <td>
      Zhibo Peng, Brandeis University; Mitch Cherniack, <br/>
      <strong>A Tool For Query Optimizer Engineering</strong><br/>
      <a href="javascript: toggleVisibility ('#po11A')">Click to toggle abstract.</a>
      <div id="po11A" class="abstract" style="display: none;">
         <p><i>Devel-Op  (A [Devel]opment Environment for Query [Op]timizers) is designed to facilitate the rapid prototyping, profiling and benchmarking of query optimizers. Rapid prototyping support is provided by way of declarative specification languages and corresponding generators for query optimizer components. Profiling support is provided with a set of tools for profiling generated components.  The current version of Devel-Op includes support for two key components of the query optimizer: the Logical Plan Enumerator  (LPE) and the Physical Plan Generator  (PPG). We have defined declarative specification languages LSL and PSL for specifying LPE and PPG components respectively. LSL specifications resemble attribute grammars as found in compiler generation tools such as Yacc and Ox. PSL specifications resemble Tree Attribute Grammars  (TAGs) that are commonly used in compiler generation to specify language translators. We have implemented component generators that generate LPE and PPG components from their corresponding specifications that can be used to process complex queries  (e.g., queries with group by clauses and aggregate functions). We also provide debugging and visualization profiling tools for tracing the operation of these components over queries.  On the poster, we will show the project's architecture and goals, introduce the LSL and PSL specification languages with example component specifications for each, and demostrate the the usage of the debugging and visualization tools using screenshots from a debugging trace of a query optimized using the components generated from the specification examples above. We will also have a demostration of the tool available for those who are interested.</i></p>
      </div>
   </td>
</tr>
<tr>
   <td>
      Dongqing Xiao, Worcester Polytechnic Inst; Mohamed Eltabakh, WPI<br/>
      <strong>Scalable Triangle Counting in Big Graphs using MapReduce</strong><br/>
      <a href="javascript: toggleVisibility ('#po12A')">Click to toggle abstract.</a>
      <div id="po12A" class="abstract" style="display: none;">
         <p><i>Triangle Counting play a important role of network analysis. Various network analysis metric can be obtained directly from graph triangulation.Most of exact triangle counting  methods is computationally expensive and based on main-memory model that makes them not scalable to new emerging network graph with billions of edges.To efficiently handle web-scale graphs for triangle counting, serval scalable systems and algorithms have been proposed. By exploiting of distributed parallel computing, distributed community detection approaches have significant scalability. However, distributed systems such as Hadoop are generally slow for the communication cost bottleneck. To address this limitation,we propose a new "Communicate as group" architecture. With this architecture, the partition structure is open to data processing, and be utilized to detect redundant network message across clusters of computing node. In communication stage, this architecture will avoid redundant network message production and transfer. In computation stage, this architecture will notify each computation to use its local alive copy. This architecture would keep flexibility and speed up execution. </i></p>
      </div>
   </td>
</tr>
<tr>
   <td>
      Ashley Conard, MIT Lincoln Laboratory; stephanie Dodson, Brown University; Jeremy Kepner, MIT; Darrell Ricke, MIT Lincoln Laboratory<br/>
      <strong>Using a Big Data Database to Identify Pathogens in Protein Data Space </strong><br/>
      <a href="javascript: toggleVisibility ('#po13A')">Click to toggle abstract.</a>
      <div id="po13A" class="abstract" style="display: none;">
         <p><i>Current metagenomic analysis algorithms require significant computing resources, can report excessive false positives  (type I errors), may miss organisms  (type II errors / false negatives), or scale poorly on large datasets. This paper explores using big data database technologies to characterize very large metagenomic DNA sequences in protein space, with the ultimate goal of rapid pathogen identification in patient samples.  Our approach uses the abilities of a big data databases to hold large sparse associative array representations of genetic data to extract statistical patterns about the data that can be used in a variety of ways to improve identification algorithms.</i></p>
      </div>
   </td>
</tr>
<tr>
   <td>
      Ligia Nistor, Oracle Corporation; Mashhood Ishaque, Oracle Corporation; Kevin Backhouse, Oracle Corporation<br/>
      <strong>Applying Streaming Algorithms to Data at Rest  (poster)</strong><br/>
      <a href="javascript: toggleVisibility ('#po14A')">Click to toggle abstract.</a>
      <div id="po14A" class="abstract" style="display: none;">
         <p><i>In the era of big data, giving fast answers to queries is of critical importance.  We are improving the performance of guided navigation by applying two streaming algorithms, SpaceSaving and HyperLogLog, to data at rest. We trade accuracy for speed. However, the error is provably small and on typical examples non-existent.</i></p>
      </div>
   </td>
</tr>
<tr>
   <td>
      Kayhan Dursun, Brown University; Ugur Cetintemel, Brown University; Tim Kraska, Brown University; Carsten Binnig, Brown University; Stan Zdonik, Brown University<br/>
      <strong>HashStash: An Abstraction to Share and Reuse Intermediate Hash Tables for In-Memory Analytics</strong><br/>
      <a href="javascript: toggleVisibility ('#po15A')">Click to toggle abstract.</a>
      <div id="po15A" class="abstract" style="display: none;">
         <p><i>With the emergence of big data frameworks, data scientists can scale their analytical tasks to increasingly larger data sets. Since queries in analytical workloads often share similar work and touch common data, there are many possibilities to optimize the execution of a batch of tasks. Current approaches for multi-query optimization are designed for traditional disk-based systems where I/O is the primary performance bottleneck. As such, they ignore issues such as code and cache efficiency that dominate the performance of main-memory systems.  In this work, we introduce a new abstraction called HashStash, which makes it easier to reuse and share data structures between related queries. It provides another form of intermediate result sharing by managing internal data structures used by individual operators between query sessions. With the help of an underlying cost model, it assigns existing or new data structures to individual operators for the most efficient execution. </i></p>
      </div>
   </td>
</tr>
<tr>
   <td>
      Rebecca Taft, MIT<br/>
      <strong>E-Store: Fine-Grained Elastic Partitioning for Distributed Transaction Processing Systems</strong><br/>
      <a href="javascript: toggleVisibility ('#po16A')">Click to toggle abstract.</a>
      <div id="po16A" class="abstract" style="display: none;">
         <p><i>On-line transaction processing  (OLTP) database management systems  (DBMSs) often serve time-varying workloads due to daily, weekly or seasonal fluctuations in demand, or because of rapid growth in demand due to a company's business success. In addition, many OLTP workloads are heavily skewed to "hot" tuples or ranges of tuples. For example, the majority of NYSE volume involves only 40 stocks. To deal with such fluctuations, an OLTP DBMS needs to be elastic; that is, it must be able to expand and contract resources in response to load fluctuations and dynamically balance load as hot tuples vary over time.   This poster presents E-Store, an elastic partitioning framework for distributed OLTP DBMSs. It automatically scales resources in response to demand spikes, periodic events, and gradual changes in an application's workload. E-Store addresses localized bottlenecks through a two-tier data placement strategy: cold data is distributed in large chunks, while smaller ranges of hot tuples are assigned explicitly to individual nodes. This is in contrast to traditional single-tier hash and range partitioning strategies. Our experimental evaluation of E-Store shows the viability of our approach and its efficacy under variations in load across a cluster of machines. Compared to single-tier approaches, E-Store improves throughput by up to 130% while reducing latency by 80%.</i></p>
      </div>
   </td>
</tr>
<tr>
   <td>
      Olga Poppe, WPI; Elke Rundensteiner, WPI<br/>
      <strong>Transactional context-aware event query manager</strong><br/>
      <a href="javascript: toggleVisibility ('#po17A')">Click to toggle abstract.</a>
      <div id="po17A" class="abstract" style="display: none;">
         <p><i>We address the problem of enabling complex analytics of high-rate event streams in near real time. Instead of randomly omitting some queries and/or data to meet strict low latency requirement,  we present CEQMA  (Context-aware Event Query Managing Automaton) that during particular time periods extracts certain data portions to process them in an appropriate way. Time periods are defined by the states of the automaton.  Data portions are determined by relevance for runs of the automaton. The way the data is processed depends on the applicaiton logic. Furthermore, we consider three alternative run processing strategies which differ in their  responsiveness to critical situations and state changes and introduce a novel transaction model  that facilitates high-responsiveness of streaming applications. We illustrate the effectiveness of  CEQMA by the Linear Road benchmark.</i></p>
      </div>
   </td>
</tr>
<tr>
   <td>
      Aaron Elmore, MIT CSAIL<br/>
      <strong>DataHub: Collaborative Data Science & Dataset Version Management at Scale</strong><br/>
      <a href="javascript: toggleVisibility ('#po18A')">Click to toggle abstract.</a>
      <div id="po18A" class="abstract" style="display: none;">
         <p><i>Relational databases have scant support for collaborative data science, i.e., enabling teams of individuals collaborate on, curate, and analyze large datasets. Taking a cue from source-code version control systems like git, we propose  (a) a dataset version control system, giving users the ability to seamlessly manage, collaborate on, and reason about many divergent versions of a collection of datasets, and  (b) a platform, DataHub, giving users the ability to perform collaborative data analysis building on this version control system. We outline the challenges in providing dataset version control at scale. </i></p>
      </div>
   </td>
</tr>
<tr>
   <td>
      Medhabi Ray, WPI<br/>
      <strong>M&M: Match & Mine Sequence Patterns over Event Streams</strong><br/>
      <a href="javascript: toggleVisibility ('#po19A')">Click to toggle abstract.</a>
      <div id="po19A" class="abstract" style="display: none;">
         <p><i>Continuous event streams are generated by a great many different sources today. Extracting meaningful information out of these streams at real-time is of utmost importance to businesses. For instance an online retailer might be keen on improving the customer¹s user experience on his website. The question he might be asking here is ³What is the most frequent sequence of events for buyers who search for items and leave the site without transacting?² The above is an example of integration of mining query along with pattern detection queries. Thus we need an engine that supports both "complex pattern detection" as well as ³"sequential pattern mining.  We propose Match & Mine:  a technique for integrating Sequence Mining queries with CEP patterns in order to express queries like the one mentioned above. Sequential Mining in conjunction with CEP poses novel technical challenges beyond those addressed in the state-of-the-art incremental Sequential Mining over stream. We propose techniques to handle these challenges and further execute them in an efficient and scalable manner.</i></p>
      </div>
   </td>
</tr>
<tr>
   <td>
      Xiaolan Wang, Umass Amherst; Xin Luna Dong, Google; Alexandra Meliou, <br/>
      <strong>Diagnosing errors in data systems</strong><br/>
      <a href="javascript: toggleVisibility ('#po20A')">Click to toggle abstract.</a>
      <div id="po20A" class="abstract" style="display: none;">
         <p><i>A lot of systems and applications are data-driven, and the correctness of their operation relies heavily on the correctness of their data. While existing data cleaning techniques can be quite effective at purging datasets of errors, they disregard the fact that a lot of errors are systematic, inherent to the process that produces the data, and thus will keep occurring unless the problem is corrected at its source. In contrast to traditional data cleaning, in this work we focus on data diagnosis: explaining where and how the errors happen in a data generative process.  We develop a large-scale diagnostic framework that makes three important contributions. First, we transform the diagnosis problem to the problem of finding common properties among erroneous elements, with minimal domain-specific assumptions. Second, we use Bayesian analysis to derive a cost model that implements three intuitive principles of good diagnoses. Third, we design an efficient, highly-parallelizable algorithm for performing data diagnosis on large-scale data. We evaluate our cost model and algorithm using both real-world and synthetic data, and show that our diagnostic framework produces better diagnoses and is orders of magnitudes more efficient than other existing techniques.</i></p>
      </div>
   </td>
</tr>
<tr>
   <td>
      Liudmila Elagina, UMASS Amherst; Gerome Miklau, ; Michael Hay, Colgate University<br/>
      <strong>Rank Aggregation under Differential Privacy.</strong><br/>
      <a href="javascript: toggleVisibility ('#po21A')">Click to toggle abstract.</a>
      <div id="po21A" class="abstract" style="display: none;">
         <p><i>Given a database of rankings of a set of entities, the problem of rank aggregation is to compute a single ranking that can serve as a single best representative ranking. The input rankings are often "votes" by individuals about the relative quality of, for example, restaurants in Boston, or graduate programs in computer science, or movies released in 2014.    Rank aggregation is a well-studied problem and many alternative algorithms have been proposed in the literature. However, the aggregate ranking may reveal sensitive information about individuals' preferences. Concerns that an adversary could learn personal preferences may prevent users from sharing their opinions.   We study differentially private algorithms for computing aggregate rankings.  In this initial work we compare the performance of private algorithms based on noisy Borda scores with private algorithms based on noisy Markov chain approaches, and evaluate the distortion introduced by the privacy mechanism as a function of the database size. </i></p>
      </div>
   </td>
</tr>
<tr>
   <td>
      Maryam Hasan, WPI; Elke Rundensteiner, WPI; Emmanuel Agu, <br/>
      <strong>Using Hashtags as Labels for Supervised Learning of Emotions in Twitter Messages</strong><br/>
      <a href="javascript: toggleVisibility ('#po22A')">Click to toggle abstract.</a>
      <div id="po22A" class="abstract" style="display: none;">
         <p><i>Many college students experience depression or anxiety but do not seek help due to the social stigma associated with psychological counseling services.  Automatic techniques to classify social media messages based on the emotions they express can assist in the early detection of students in need of counseling.  Supervised machine learning methods yield accurate results but require training datasets of text messages that have been labelled with the classes of emotions they express. Manually labeling a large corpus of Twitter messages is labor-intensive, error prone and time-consuming. In this research, we investigate using hashtags inserted into Twitter  messages by their authors as emotion labels. We evaluate them  through two user studies, one with  psychology experts and  the other with the general crowd. The study showed that the labels created by general crowd was inconsistent and unreliable. However, the labels generated by experts matched with hashtag labels in over 88% of Twitter messages, which indicates that hashtags are indeed good emotion labels. Leveraging the concept of hashtags as   emotion labels, we develop Emotex, a supervised learning approach that classifies Twitter messages into the emotion classes they express.  We show that Emotex correctly classifies the emotions expressed in over 90% of text messages. </i></p>
      </div>
   </td>
</tr>
<tr>
   <td>
      Bahar Qarabaqi, Northeastern University; Mirek  Riedewald, Northeastern University<br/>
      <strong>Exploratory Refinement of Imprecise Queries</strong><br/>
      <a href="javascript: toggleVisibility ('#po23A')">Click to toggle abstract.</a>
      <div id="po23A" class="abstract" style="display: none;">
      <p>
         <i>We propose Merlin, a system for exploratory search in large databases. The user interacts with Merlin by specifying probability distributions over attributes, which express imprecise conditions about the entities of interest. Merlin aids the user in homing in on the right query conditions by addressing three key challenges:  (1) efficiently computing results for an imprecise query,  (2) providing feedback about the sensitivity of the result to changes of individual conditions, and  (3) suggesting new conditions. We formally introduce the notion of sensitivity and prove structural properties that enable efficient algorithms for quantifying the effect of uncertainty in user-specified conditions. To support interactive responses, we also develop techniques that can deliver probability estimates needed for the query result within a given hard realtime limit and are able to adapt automatically as the interactive query refinement proceeds.</i></p>
</div>
</td>
</tr>
<tr>
<td>Leilani Battle, MIT<br/>
<strong>Making Sense of Temporal Queries with Fine-Grained Provenance</strong><br/>
<a href="javascript: toggleVisibility ('#po24A')">Click to toggle abstract.</a>
<div id="po24A" class="abstract" style="display: none;">
<p><i>As real-time monitoring and analysis becomes increasingly popular, more researchers are turning to data stream management systems  (DSMS's) for fast, efficient ways to pose temporal questions over their datasets. However, these systems are inherently complex, and even expert database users find it difficult to understand the resulting behavior of DSMS queries. To help users better understand their temporal DSMS queries, we developed a visualization tool that allows users to see how a temporal query manipulates a given dataset, step-by-step. The key design features of our visualization tool are: 1) per-record-level  (i.e. fine- grained) provenance analysis to track how individual records are transformed after each database operation; 2) automatic query rewriting to allow users to enable provenance tracking with a single click; and 3) a mapping from the resulting provenance data to interactive visualizations.</i></p>
</div>
</td>
</tr>
<tr>
   <td>
      Xiao Qin, WPI; Ramoza Ahsan, WPI; Xika Lin, WPI; Elke Rundensteiner, WPI; Matthew Ward, WPI<br/>
      <strong>EPSTAR: An Evolving Parameter Space Framework for Interactive Temporal Association Rule Mining</strong><br/>
      <a href="javascript: toggleVisibility ('#po25A')">Click to toggle abstract.</a>
      <div id="po25A" class="abstract" style="display: none;">
         <p><i>Analytics of time-variant big data sets to derive actionable insights has become critical to gain a competitive edge for many modern applications. While recent advances to support interactive exploration of the associations and correlations derived from static data exist, the opportunity for interactive analysis of the time-variant patterns of association has been overlooked. In this work, we introduce the the first technology, called EPSTAR  (Evolving Parameter Space Framework for Interactive Temporal Association Rule Mining), that enables analysts to effectively explore the evolution of the associations across time at multiple levels of abstraction, called temporal association analytics. EPSTAR features three core innovations for temporal association analytics. First, EPSTAR offers an innovative interactive temporal association model that provides a `rule-centric panorama' into evolving data sets along with rich exploration operations.  Second, the EPSTAR preprocessing phase extracts the time-variant associations from big evolving data sets and compresses them into a knowledge-rich yet compact EP-Space  (Evolving Parameter Space).  This EP-Space pre-codes the core interrelationships of temporal associations, which are critical for efficient on-line temporal association analytics. Third, supported by this EP-Space structure, our EPSTAR infrastructure incorporates online processing strategies that efficiently implement the on-line temporal association analytics.  As demonstrated by our extensive experiments on real-world data sets, temporal association analytics is supported within a milliseconds response time.</i></p>
      </div>
   </td>
</tr>
<tr>
   <td>
      Abhishek Roy, UMass Amherst; Yanlei Diao, ; Toby Bloom, <br/>
      <strong>GEnomic Scalable Analysis with Low Latency  (GESALL)</strong><br/>
      <a href="javascript: toggleVisibility ('#po26A')">Click to toggle abstract.</a>
      <div id="po26A" class="abstract" style="display: none;">
         <p><i>Next-generation sequencing has transformed genomics into a new paradigm of data-intensive computing. Today, the large sequencing centers produce genomic data at the rate of 10 terabytes a day. However, the data processing pipelines take up to weeks to transform the noisy raw data into rich biological information. In this poster, we will outline the vision of our new project, GEnomic Scalable Analysis with Low Latency  (GESALL), which aims to reduce the latency of results. We will present the initial evaluation results of parallel execution of the pipeline. We will also explore the further optimization opportunities available in the pipeline. </i></p>
      </div>
   </td>
</tr>
<tr>
   <td>
      Haopeng Zhang, Umass Amherst<br/>
      <strong>Explaining Anomalies in CEP Queries and Proactive Monitoring</strong><br/>
      <a href="javascript: toggleVisibility ('#po27A')">Click to toggle abstract.</a>
      <div id="po27A" class="abstract" style="display: none;">
         <p><i>Complex event processing techniques are used to monitor the status of applications. However, due to highly complexity of monitored applications, users often cannot specify the query for anomalies in advance. We propose a new system to proactively monitor users' interests. Users start by specifying a known query for monitoring purposes, then users can annotate anomalies on the visualization of monitoring results. Our new system searches the context subset for user annotated anomalies, and find the most affecting features of the subset as the explanations for those anomalies. Also, the generated explanations are translated into CEP queries for proactively monitoring future anomalies. </i></p>
      </div>
   </td>
</tr>
<tr>
   <td>
      Wenzhao Liu, Univ of Massachusetts Amherst; Liping Peng, University of Massachusetts Amherst; Yanlei Diao, <br/>
      <strong>Query Steering for Interactive Data Exploration</strong><br/>
      <a href="javascript: toggleVisibility ('#po28A')">Click to toggle abstract.</a>
      <div id="po28A" class="abstract" style="display: none;">
         <p><i>The need of effective interactive data exploration  (IDE) will only increase as data are being collected at an unprecedented rate. We envision a data exploration system that supports a diverse set of discovery-oriented applications by steering the user towards interesting "trajectories" through the data. The system aims to automatically learn user interests and infer classification queries that retrieve relevant data. To achieve this, we rely on an interactive learning approach that iteratively requests user feedback on strategically collected data samples. Our goal is not only to provide accurate results for queries with both linear and non-linear patterns, but also to minimize the number of samples presented to the user  (which determines the amount of user effort) as well as the cost of sample acquisition from the database  (which amounts to the user wait time). In this poster, we will present the framework of our IDE system, the key techniques proposed for sample selection to achieve our goal and some initial results.</i></p>
      </div>
   </td>
</tr>
<tr>
   <td>
      Yunmeng Ban, UMass Amherst; Yanlei Diao, ; Boduo Li, <br/>
      <strong>A Comparison of Stream Processing Systems</strong><br/>
      <a href="javascript: toggleVisibility ('#po29A')">Click to toggle abstract.</a>
      <div id="po29A" class="abstract" style="display: none;">
         <p><i>As the scale for data analytics continues to grow, there has been an enormous interest in building a system that can handle not only "big data" but also "fast data". While MapReduce systems provide a batch processing model to scale to large amounts of data, the latency of such batch processing is too high for many real world applications. Motivated by such real world use cases, we proposed SCALLA, a scalable low-latency data analytics system that combines batch processing and stream processing in one system. To evaluate the performance of our system, we compare it with two popular stream processing system, Twitter Storm and Spark Streaming, over a collection of workloads designed for large continuous data streams. The experiment results reveal that our system outperforms the other two systems by 1-2 orders of magnitude when combining both latency and throughput measures.</i></p>
      </div>
   </td>
</tr>
<tr>
   <td>
      Yiqun Zhang, University of Houston; Wellington Cabrera, University of Houston; Carlos Ordonez, University of Houston & ATT<br/><strong>The Gamma Operator to Summarize Dense and Sparse Data Sets on the SciDB Array Database System</strong><br/>
      <a href="javascript: toggleVisibility ('#po30A')">Click to toggle abstract.</a>
      <div id="po30A" class="abstract" style="display: none;">
         <p><i>We propose a generalized matrix operator based on a special form of matrix multiplication that computes a comprehensive summarization matrix for a family of linear models. By deriving equivalent equations based on such summarization matrix, many statistical methods can work in two phases:  (1) Summarization of a data set in one pass with a highly optimized matrix multiplication;  (2) Iteration in main memory, exploiting the summarization matrix. Therefore, the summarization matrix allows iterative algorithms to work faster in main memory, decreasing the number of times the data set is scanned, without sacrificing model accuracy. Our summarization matrix operator benefits a big family of linear models, including PCA, linear regression and variable selection, three fundamental and complementary models widely used in statistical learning. From a systems perspective, we carefully study the optimization of our summarization operator on the SciDB parallel array DBMS and how to exploit the summarization matrix in R, the most popular open-source statistical system. We justify it is necessary to develop two specialized versions of the summarization operator for dense and sparse matrices, respectively.</i></p>
      </div>
   </td>
</tr>
<tr>
   <td>
      Wellington Cabrera, University of Houston; Yiqun Zhang, University of Houston; Carlos Ordonez, University of Houston & ATT<br/><strong>Fast Recursive Query Evaluation in a Columnar DBMS to Analyze Large Graphs</strong><br/>
      <a href="javascript: toggleVisibility ('#po31A')">Click to toggle abstract.</a>
      <div id="po31A" class="abstract" style="display: none;">
         <p><i>We study two deeply inter-related graph problems: transitive closure and graph adjacency matrix multiplication. Based on these two problems, we discuss main differences and opportunities optimizing recursive queries on a columnar DBMS, compared to a row DBMS. We explain how the optimization problem can be broken down and solved by more efficient recursive joins and pushing recursive aggregations. From a generalization perspective, we explain how many graph analytic problems like triangle counting and clustering coefficient can be reduced to specific multiplications involving the graph adjacency matrix. That is, transitive closure and iterative multiplication of the graph adjacency matrix represent more fundamental and general graph problems subsuming many graph problems. Nevetheless, we also characterize graph problems which are harder than transitive closure and thus they are not a good fit for linearly recursive queries.</i></p>
      </div>
   </td>
</tr>
<tr>
   <td>
      Kai Zhang, The Ohio State University; Kaibo Wang, ; Yuan Yuan, ; Lei Guo, ; Rubao Lee, ; Xiaodong Zhang, <br/><strong>Mega-KV: A Case for GPU to Maximize the Throughput of In-Memory Key-Value Store</strong><br/>
      <a href="javascript: toggleVisibility ('#po32A')">Click to toggle abstract.</a>
      <div id="po32A" class="abstract" style="display: none;">
         <p><i>In-memory key-value stores play a critical role in data processing to provide high throughput and low latency data accesses. In-memory key-value stores have several unique properties that include  (1) data intensive operations demanding high memory bandwidth for fast data accesses,  (2) high data parallelism and simple computing operations demanding many slim parallel computing units, and  (3) a large working set. As data volume continues to increase, our experiments show that conventional and general-purpose multicore systems are increasingly mismatched to the special properties of key-value stores because they do not provide massive data parallelism based on high memory bandwidth; the powerful but the limited number of computing cores do not satisfy the demand of the unique data processing task; and the cache hierarchy may not well benefit to the large working set.  In this paper, we make a strong case for GPU to serve as a special-purpose device to greatly accelerate the operations of in-memory key-value stores. Specifically, we present the design and implementation of Mega-KV, a GPU-based in-memory key-value store system that achieves high performance and high throughput. Effectively utilizing the high memory bandwidth and latency hiding capability of GPU, Mega-KV provides fast data accesses and significantly boosts overall performance. Running on a commodity PC installed with two CPUs and two GPUs, Mega-KV can pro- cess up to 200+ million key-value operations per second, which is 2.7 times faster than the state-of-the-art key-value store systems on a conventional CPU-based platform.</i></p>
      </div>
   </td>
</tr>
<tr>
   <td>
      Andy Vidan, Composable Analytics<br/><strong>Systems and Methods for Composable Analytics</strong><br/>
      <a href="javascript: toggleVisibility ('#po33A')">Click to toggle abstract.</a>
      <div id="po33A" class="abstract" style="display: none;">
         <p><i>In this paper, we describe a new data and process integration platform called Composable Analytics, originally developed at MIT Lincoln Laboratory. Composable Analytics is a web-based software platform that enables researchers, data analysts, and decision makers to collaboratively explore complex, information-based problems through the creation and use of customized analysis applications. One goal of this new platform is to extend the power of adaptability beyond the realm of software developers and into the capacity of data scientists and analysts.</i></p>
      </div>
   </td>
</tr> 
<tr>
   <td>
     Carsten Binnig; Ugur Cetintemel; Tim Kraska; Stan Zdonik, Brown University<strong>Human-in-the-Loop Data Management</strong><br/>
      <a href="javascript: toggleVisibility ('#po34A')">Click to toggle abstract.</a>
      <div id="po34A" class="abstract" style="display: none;">
         <p><i>
Data-centric applications in which data scientists of varying skill levels explore large data sets are becoming more and more relevant to make sense of the data, identify interesting patterns, and bring aspects of interest into focus for further analysis. Enabling these applications with ease of use and at "human speeds" is key to democratizing data science and maximizing human productivity.
<br>
Traditional database technologies are ill-suited to serve this purpose. Historically, databases assumed (1) text-based input (e.g., SQL) and output, (2) a one-shot (i.e., stateless) query-response paradigm, (3) batch results, and (4) simple analytics. We drop these fundamental assumptions and propose a new breed of database systems that instead support visual input and output, "conversational" interactions, early and progressive results, and complex analytics. Building a system that integrates these features requires a complete rethinking of the full database stack, from the interface to the "guts". We call these new systems "Human-in-the-Loop" Data Management Systems} (HIL-DMS). This work is an an early yet transformative step in this direction.
</i></p>
      </div>
   </td>
</tr>

<tr>
   <td>

John Meehan (Brown), Nesime Tatbul (Intel, MIT), Stan Zdonik (Brown), Hawk Wang (MIT), Cansu Aslantas (Brown), Andy Pavlo (Carnegie Mellon), Michael Stonebraker (MIT), Sam Madden (MIT), Ugur Cetintemel (Brown), Tim Kraska (Brown)
<strong>S-Store: Streaming Meets Transaction Processing</strong><br/>
      <a href="javascript: toggleVisibility ('#po35A')">Click to toggle abstract.</a>
      <div id="po35A" class="abstract" style="display: none;">
         <p><i>
Stream processing addresses the needs of real-time applications. Transaction processing addresses the coordination and safety of short atomic computations. Heretofore, these two modes of operation existed in separate, stove-piped systems. In this work, we attempt to fuse the two computational paradigms in a single system called S-Store, which can simultaneously accommodate OLTP and streaming applications. S-Store is built as an extension of an open-source, in-memory, distributed OLTP database system to make use of the transaction processing facilities already supported.  This way, we can concentrate on what is needed to support streaming, such as devising a simple transaction model for streams that integrates seamlessly with a traditional OLTP system.
</i></p>
      </div>
   </td>
</tr>
  </tbody>
   </table>
</div>
