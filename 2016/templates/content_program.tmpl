<div class="content-container">
   <h1>
      North East Database Day Program
   </h1>
   <h2 class="sponsorCallout">Sponsors</h2>
   <p class="sponsorCallout">Special thanks to this year's sponsors:<br/> <br/> <img src="images/logos/microsoft.jpg" width="180" /></p>
   <p class="programDate">January 30, 2015</p>
   <table class="table table-bordered table-hover table-condensed programTable">
      <tbody>
         <tr>
            <th width="15%">Time</th>
            <th width="85%">Event</th>
         </tr>
         <tr>
            <td>8:15-9:00am</td>
            <td><strong>Registration</strong></td>
         </tr>
         <tr>
            <td>9:00-9:10am</td>
            <td><strong>Welcome</strong> (Sam Madden and Tingjian Ge)</td>
         </tr>
         <tr>
            <td>9:10-9:20am</td>
            <td>Sponsor Acknowledgements</td>
         </tr>
         <tr>
            <td>9:20-10:10</td>
            <td><strong>Keynote1: </strong>
               Jignesh Patel  (University of Wisconsin). <i>From Data to Insights @ Bare Metal Speed</i> <a href="javascript: toggleVisibility ('#keynote1A')">Click to toggle abstract.</a>
               <div id="keynote1A" class="abstract" style="display: none;">
                  <h3>Abstract</h3>
<p>Big data platforms today largely employ data processing kernels that were developed for a now bygone hardware era. Hardware has made a fundamental shift in recent years, driven by the need to consider energy as a first class design parameter. Consequently, across the processor-IO hierarchy, the hardware paradigm today looks very different than just a few years ago. I argue that because of this shift, we are now building a "deficit" between the pace at which the hardware is evolving and the pace that is demanded of data processing kernels to keep up with the growth of big data. This deficit is unsustainable in the long run. One way to "pay off" this deficit is to have hardware and software co-evolve to exploit the full potential of the hardware. I will provide some examples of recent work from our Quickstep project that demonstrates the merit of this line of thinking. I'll focus on analytical data processing environments, and argue that our new way of flattening databases into WideTable and storing them using BitWeaving provides a dramatic new way to build "sustainable" analytical query processing system. Time permitting I'll also discuss the implications of our approach on future hardware.</p>
                  <h3>Bio</h3>
<p>Jignesh Patel is a Professor in Computer Sciences at the University of Wisconsin-Madison, where he also earned his Ph.D. He has worked in the area of databases (now fashionably called "big data") for over two decades. He has won several best paper awards, industry research awards, and is the recipient of the Wisconsin COW teaching award, and the U. Michigan College of Engineering Education Excellence Award. He has a strong interest in seeing research ideas transition to actual products. His Ph.D. thesis work was acquired by NCR/Teradata in 1997, and he also co-founded Locomatix - a startup that built a platform to power real-time data-driven mobile services. Locomatix became part of Twitter in 2013. He is an ACM Fellow, serves on the board of Lands'End, and advises a number of startups.</p>
               </div>
            </td>
         </tr>
         <tr>
            <td>10:10-10:30</td>
            <td>Coffee Break</td>
         </tr>
         <tr class="success">
            <td colspan="2" align="center"><strong>Session 1: New Hardware and Applications</strong></td>
         </tr>
         <tr>
            <td>10:30-10:50</td>
            <td>Kristin Tufte (Portland State University) <i>Adventures in Transportation from a (Big) Data Perspective</i> <br/><a href="javascript: toggleVisibility ('#pa1A')">Click to toggle abstract.</a> | <a href="./pdf/Paper40.pdf" target="_new">Click to view full paper.</a>
               <div id="pa1A" class="abstract" style="display: none;">
                  <h3>Kristin Tufte*, Portland State University</h3>
                  <p>Transportation is a part of our daily lives. It is well known that transportation is an important part of our lives and economy and has a large impact on the environment. What may be less well known is how diverse and interesting the data generated by transportation infrastructure is.  This talk gives a tour through transportation data sources from a data management perspective. Data sources are described and issues in obtaining and managing that data are discussed. Transportation data is an excellent example of Big Data Variety - we describe the variations in transportation data, ranging from simple format differences to more complex semantic differences and the need to combine data across different, varied sources to provide a complete picture of the transportation system. In addition to the tour of data sources, we discuss how the transportation data is used in practice and provide ideas of how it may be used in the future. We discuss very preliminary solutions for archiving this data, but more importantly we highlight areas where (big data) computer science research is applicable in the transportation data domain.</p>
               </div>
             </td>
         </tr>
         <tr>
            <td>10:50-11:10</td>
            <td>Paul Brown (Paradigm4 Inc) <i>A Survey of Scientific Applications using SciDB</i> <br/><a href="javascript: toggleVisibility ('#pa2A')">Click to toggle abstract.</a> | <a href="./pdf/Paper41.pdf" target="_new">Click to view full paper.</a>
               <div id="pa2A" class="abstract" style="display: none;">
                  <h3>Paul Brown*, Paradigm4 Inc</h3>
                  <p>While SciDB is gaining popularity among commercial software developers and data analysts charged with building ``Big Data'' applications, the platform was conceived as a tool for scientists. Here, we review a number of purely scientific applications of SciDB; bioinformatics at the NIH, experimental physics at NERSC, and remote sensing at INPE. We describe each project in terms of its scientific objectives, the quantity and character of the data involved, the analytic workload, and its hardware architecture. We conclude the talk with a general summary of what these projects have in common, and comment on what experience building these applications implies for data management in the Internet-of-Things (IoT); a computing environment where all interesting data is generated by machines.</p>
               </div>
             </td>
         </tr>
         <tr>
            <td>11:10-11:30</td>
            <td>Carsten Binnig, Ugur Cetintemel, Tim Kraska, Stan Zdonik (Brown) <i>I-Store: Data Management for Fast Networks</i> <br/><a href="javascript: toggleVisibility ('#pa3A')">Click to toggle abstract.</a> | <a href="./pdf/Paper51.pdf" target="_new">Click to view full paper.</a>
               <div id="pa3A" class="abstract" style="display: none;">
                  <h3>Carsten Binnig*, Brown University; Ugur Cetintemel, Brown University; Tim Kraska, Brown University; Stan Zdonik, Brown University</h3>
                  <p>Existing distributed data management systems typically assume that the network is a major bottleneck.  Consequently, avoiding remote data transfers is an important design aspect of existing systems. With modern RDMA-capable networks such as Infiniband FDR/EDR the before mentioned design decisions become obsolete. Modern interconnects allow data to transfer across machines almost as fast as from the CPU to memory.  In this paper, we revisit design decisions for distributed data management systems for OLTP workloads as well as for OLAP workloads under the assumption that communication between servers is essentially free.</p>
               </div>
             </td>
         </tr>
         <tr>
            <td>11:30-11:50</td>
            <td>Xiangyao Yu (MIT), George Bezerra (MIT), Andrew Pavlo (CMU), Srinivas Devadas (MIT), Michael Stonebraker (MIT) <i>Staring into the Abyss: An Evaluation of Concurrency Control with One Thousand Cores</i> <br/><a href="javascript: toggleVisibility ('#pa4A')">Click to toggle abstract.</a> | <a href="./pdf/Paper17.pdf" target="_new">Click to view full paper.</a>
               <div id="pa4A" class="abstract" style="display: none;">
                  <h3>Xiangyao Yu*, MIT; George Bezerra, MIT; Andrew Pavlo, CMU; Srinivas Devadas, MIT; Michael Stonebraker, MIT</h3>
                  <p>Computer architectures are moving towards an era dominated by many-core machines with dozens or even hundreds of cores on a single chip. This unprecedented level of on-chip parallelism introduces a new dimension to scalability that current database management systems (DBMSs) were not designed for. In particular, as the number of cores increases, the problem of concurrency control becomes extremely challenging. With hundreds of threads running in parallel, the complexity of coordinating competing accesses to data will likely diminish the gains from increased core counts.  To better understand just how unprepared current DBMSs are for future CPU architectures, we performed an evaluation of concurrency control for on-line transaction processing (OLTP) workloads on many-core chips. We implemented seven concurrency control algorithms on a main-memory DBMS and using computer simulations scaled our system to 1024 cores. Our analysis shows that all algorithms fail to scale to this magnitude but for different reasons. In each case, we identify fundamental bottlenecks that are independent of the particular database implementation and argue that even state-of-the-art DBMSs suffer from these limitations. We conclude that rather than pursuing incremental solutions, many-core chips may require a completely redesigned DBMS architecture that is built from ground up and is tightly coupled with the hardware.</p>
               </div>
             </td>
         </tr>
         <tr>
            <td colspan="2">&nbsp;</td>
         </tr>
         <tr>
            <td>11:50-12:50</td>
            <td><strong>Lunch (Outside 32-123)</strong></td>
         </tr>
         <tr>
            <td colspan="2">&nbsp;</td>
         </tr>
         <tr>
            <td>1:00-1:50</td>
            <td><strong>Keynote2: </strong>
               Andrew W. Lo (MIT). <i>Law as Code </i> <a href="javascript: toggleVisibility ('#keynote2A')">Click to toggle bio.</a>
               <div id="keynote2A" class="abstract" style="display: none;">
                  <h3>Bio</h3>
                  <p>Andrew W. Lo is the Charles E. and Susan T. Harris Professor, a Professor of Finance, and the Director of the Laboratory for Financial Engineering at the MIT Sloan School of Management.</p>
                  <p>Lo is the director of MIT's Laboratory for Financial Engineering, a research associate of the National Bureau of Economic Research,[4] a member of the NASD's Economic Advisory Board, 
                  and founder and chief scientific officer of AlphaSimplex Group,[5] a quantitative investment management company based in Cambridge, Massachusetts. AlphaSimplex specializes in quantitative 
                  global macro and global tactical asset allocation strategies, beta-replication products, and absolute-return risk analytics. He is an associate editor of the Financial Analysts Journal, 
                  The Journal of Portfolio Management, the Journal of Computational Finance, and Statistica Sinica. He is a former governor of the Boston Stock Exchange. He previously taught at the University of 
                  Pennsylvania's Wharton School. He testified in front of the Committee on Oversight and Government Reform of the U.S. House of Representatives.[6] With Lars Peter Hansen, he co-directs the Macro
                   Financial Modeling group at the Becker Friedman Institute, a network of macroeconomists working to develop improved models of the linkages between the financial and real sectors of the economy 
                   in the wake of the 2008 financial crisis.</p>
                   <p>His awards include the Alfred P. Sloan Foundation Fellowship, the Paul A. Samuelson Award, the American Association for Individual Investors Award, the Graham and Dodd Award, 
                   the 2001 IAFE-SunGard Financial Engineer of the Year award, a Guggenheim Fellowship, the CFA Institute's James R. Vertin Award, and awards for teaching excellence from both Wharton and MIT.</p>
               </div>
            </td>
         </tr>
         <tr class="success">
            <td colspan="2" align="center"><strong>Session 2: Graphs and DB Design</strong></td>
         </tr>
         <tr>
            <td>1:50-2:10</td>
            <td>Antonio Maccioni (Roma Tre Uni / Yale Uni), Daniel Abadi (Yale University) <i>On Compressing Graph Databases</i> <br/><a href="javascript: toggleVisibility ('#pa5A')">Click to toggle abstract.</a> | <a href="./pdf/Paper12.pdf" target="_new">Click to view full paper.</a>
               <div id="pa5A" class="abstract" style="display: none;">
                  <h3>Antonio Maccioni*, Roma Tre Uni / Yale Uni; Daniel Abadi, Yale University</h3>
                  <p>Distributed and parallel databases are not always a scalable solution for graphs because of the low locality in the  data matching the query. A different approach for scalability is to consider compressed versions of the input graph in order to reduce the cost of join operations at query time. These methods proved to be effective when the queries are given in advance but are not general enough to be adopted since we cannot reconstruct the original graph with decompression. This means that the results of the query are approximations of the exact answers. We want to discuss with the audience a novel compression technique that enables the database engine to evaluate graph queries (e.g., graph pattern matching queries) in an exact way. Preliminary experiments show that our approach is scalable with respect to the size of the graph and that this method can be employed as a layer over existing database systems with minimal effort.</p>
               </div>
             </td>
         </tr>
         <tr>
            <td>2:10-2:30</td>
            <td>Jeremy Kepner (MIT) <i>Associative Arrays: Unified Mathematics for Spreadsheets, Databases, Matrices, and Graphs</i> <br/><a href="javascript: toggleVisibility ('#pa6A')">Click to toggle abstract.</a> | <a href="./pdf/Paper7.pdf" target="_new">Click to view full paper.</a>
               <div id="pa6A" class="abstract" style="display: none;">
                  <h3>Jeremy Kepner*, MIT</h3>
                  <p>Data processing systems impose multiple views on data as it is processed by the system.  These views include spreadsheets, databases, matrices, and graphs.  The common theme amongst these views is the need to store and operate on data as whole sets instead of as individual data elements. This work describes a common mathematical representation of these data sets (associative arrays) that applies across a wide range of applications and technologies.  Associative arrays unify and simplify these different approaches for representing and manipulating data into common two-dimensional view of data. Specifically, associative arrays (1) reduce the effort required to pass data between steps in a data processing system, (2) allow steps to be interchanged with full confidence that the results will be unchanged, and (3) make it possible to recognize when steps can be simplified or eliminated.  Most database system naturally support associative arrays via their tabular interfaces.  The D4M implementation of associative arrays uses this feature to provide a common interface across SQL, NoSQL, and NewSQL databases.</p>
               </div>
             </td>
         </tr>
         <tr>
            <td>2:30-2:50</td>
            <td>Barzan Mozafari (Michigan) <i>CliffGuard: Towards Workload-Resilient Database Designs</i> <br/><a href="javascript: toggleVisibility ('#pa7A')">Click to toggle abstract.</a> | <a href="./pdf/Paper50.pdf" target="_new">Click to view full paper.</a>
               <div id="pa7A" class="abstract" style="display: none;">
                  <h3>Barzan Mozafari*, University of Michigan</h3>
                  <p>A fundamental problem in database systems is choosing the best physical design, i.e., a small set of auxiliary structures that en- able the fastest execution of future queries. Almost all commer- cial databases come with designer tools that create a number of indices or materialized views (together comprising the physical de- sign) that they exploit during query processing. Existing designers are what we call nominal; that is, they assume that their input pa- rameters are precisely known and equal to some nominal values. For instance, since future workload is often not known a priori, it is common for these tools to optimize for past workloads in hopes that future queries and data will be similar. In practice, however, these parameters are often noisy or missing. Since nominal de- signers do not take the influence of such uncertainties into account, they find designs that are sub-optimal and remarkably brittle. Of- ten, as soon as the future workload deviates from the past, their overall performance falls off a cliff, leading to customer discontent and expensive re-designs. Thus, we propose a new type of database designer that is robust against parameter uncertainties, so that over- all performance degrades more gracefully when future workloads deviate from the past. Users express their risk tolerance by de- ciding on how much nominal optimality they are willing to trade for attaining their desired level of robustness against uncertain sit- uations. To the best of our knowledge, this project is the first to adopt the recent breakthroughs in the theory of robust optimization to build a practical framework for solving some of the most fundamental problems in databases, replacing today's brittle designs with a principled world of robust designs that can guarantee predictable and consistent performance.</p>
               </div>
             </td>
         </tr>
         <tr>
            <td>2:50-3:10</td>
            <td>Coffee Break</td>
         </tr>
         <tr class="success">
            <td colspan="2" align="center"><strong>Session 3: Big Data and Real-time Analytics</strong></td>
         </tr>
         <tr>
            <td>3:10-3:30</td>
            <td> Alekh Jindal, Samuel Madden (MIT) <i>Preparing Data For The Data Lake</i> <br/><a href="javascript: toggleVisibility ('#pa8A')">Click to toggle abstract.</a> | <a href="./pdf/Paper45.pdf" target="_new">Click to view full paper.</a>
               <div id="pa8A" class="abstract" style="display: none;">
                  <h3>Alekh Jindal*, MIT; Samuel Madden, MIT</h3>
                  <p>Data preparation is increasingly becoming one of the biggest challenges in processing big data. While recent tools such as Tamer and Trifacta address the problem of integrating and cleaning the datasets as they come in, preparing these datasets for efficient processing over a variety of query workloads is still challenging. In this talk, I will discuss these challenges and describe our tool which allows for fine-grained data preparation, via a data preparation plan, and efficiently runs this plan while uploading the data to HDFS.</p>
               </div>
             </td>
         </tr>
         <tr>
            <td>3:30-3:50</td>
            <td>Yuan Yuan, Kaibo Wang, Rubao Lee, Xiaoning Ding, Xiaodong Zhang (Ohio State)<i>BCC: Reducing False Aborts in Optimistic Concurrency Control with Affordable Cost for In-Memory Databases</i> <br/><a href="javascript: toggleVisibility ('#pa9A')">Click to toggle abstract.</a> | <a href="./pdf/Paper32.pdf" target="_new">Click to view full paper.</a>
               <div id="pa9A" class="abstract" style="display: none;">
                  <h3>Yuan Yuan*, ; Kaibo Wang, ; Rubao Lee, ; Xiaoning Ding, New Jersey Institute of Techno; Xiaodong Zhang</h3>
                  <p>The Optimistic Concurrency Control (OCC) method has been demonstrated to provide high transaction throughputs in several recent in-memory database studies. OCC will abort a transaction if the transaction's read set has been changed by other concurrent transactions. OCC works well as long as transaction contention is low. However, it will cause unnecessary transaction aborts and significantly degrade the database's throughput when transaction contention is high. In this work, we propose a new concurrency control method BCC that is able to more precisely determine whether a transaction should be aborted. We implement BCC in Silo. Our evaluations demonstrate that BCC have much better transaction throughput  compared to OCC when transaction contention is high while having comparable performance with OCC for low contention workloads.</p>
               </div>
             </td>
         </tr>
         <tr>
            <td>3:50-4:10</td>
            <td>John Hugg (VoltDB) <i>Friends don't let Friends use the Lambda Architecture</i> <br/><a href="javascript: toggleVisibility ('#pa10A')">Click to toggle abstract.</a> | <a href="./pdf/Paper39.pdf" target="_new">Click to view full paper.</a>
               <div id="pa10A" class="abstract" style="display: none;">
                  <h3>John Hugg*, VoltDB Inc.</h3>
                  <p>Recently, there have been a number of systems developed to tackle the intersection of streaming and big data. Nathan Marz has championed the Lambda Architecture as a recipe for combining speed and batch processing to tackle these problems. Today, it's hard to go to a trendy big data conference without hearing a talk about how a complex, interdependent Lambda system solved a problem in heroic fashion with only a modicum of war scars and hefty AWS bill.  In this talk, we will prove, by counterexample, that the Lambda Architecture is bad and should feel bad. We will examine an recent, popular and non-fictional Lambda conference talk. We will then show how the same problem can be solved using VoltDB in a tiny fraction of the code, with many fewer nodes and with drastically reduced operational complexity. Finally we will argue the results are generally generalizable, i.e. not specific to the one example shown.</p>
               </div>
             </td>
         </tr>
         <tr>
            <td>4:10-4:30</td>
            <td>Jennie Duggan (Northwestern), Aaron Elmore (U. Chicago), Tim Kraska (Brown), Samuel Madden (MIT), Tim Mattson (Intel), Michael Stonebraker (MIT) <i>The BigDawg Architecture and Reference Implementation</i> <br/><a href="javascript: toggleVisibility ('#pa11A')">Click to toggle abstract.</a> | <a href="./pdf/Paper54.pdf" target="_new">Click to view full paper.</a>
               <div id="pa11A" class="abstract" style="display: none;">
                  <h3>Jennie Duggan*, Northwestern University; Aaron Elmore, Univ. of Chicago; Tim Kraska, Brown University; Samuel Madden, MIT; Tim Mattson, Intel Corp; Michael Stonebraker, MIT</h3>
                  <p>This paper presents the reference implementation of a new architecture for future "Big Data" applications.  Such applications require "big analytics" as one might expect, but they also require real-time streaming support, real-time analytics, data visualization, and cross-storage queries.  We are guided by the principle "one size does not fit all", and we build on top of three storage engines, each designed for specialized use cases.  In addition, we demonstrate novel support for querying across multiple storage engines as well as pioneering solutions to data visualization.  In the remainder of this short paper, we describe the first of three BigDawg reference implementations, Bulldog.  In the next two years we expect to follow with Pitbull and Rottweiler releases.</p>
               </div>
             </td>
         </tr>
         <tr>
            <td colspan="2">&nbsp;</td>
         </tr>
         <tr>
            <td>4:30 PM</td>
            <td><a href="#posters"><strong>Poster Session</strong></a> and Appetizers / Drinks (Building 32, R&D Commons, 4th Floor, Gates Tower)</td>
         </tr>
         <tr>
            <td>6:30 PM</td>
            <td>Adjourn</td>
      </tbody>
   </table>
</div>
