<div class="content-container">
   <h1>
         North East Database Day 20l6 <br />
         <small>Thursday January 28th, 2016</small><br/>
         <small>MIT CSAIL Bulding 32 Room 123</small>
   </h1>
   <style>
   i {
   font-weight: bold;
   display: block;
   }
   </style>
   <h2 class="sponsorCallout">Special thanks to this year's sponsors</h2>
   <p class="sponsorCallout">
    <!--Special thanks to this year's sponsors:<br/> <br/>-->
    <img src="images/logos/microsoft.jpg" width="180" />
  </p>
  <p>
    <a href="https://www.dropbox.com/sh/mhd43y61pf9gbrv/AAD3Sr6VZKL73kN-AwdX9qiia?dl=0"
      id="registerbtn">
      Dropbox link to speaker slides</a>
  </p>
   <table class="table table-bordered table-hover table-condensed programTable">
      <tbody>
         <tr>
            <th width="15%">Time</th>
            <th width="85%">Event</th>
         </tr>
         <tr>
            <td>8:15-9:00am</td>
            <td><strong>Registration</strong></td>
         </tr>
         <tr>
            <td>9:00-9:10am</td>
            <td><strong>Welcome</strong> (Eugene Wu/Samuel Madden)</td>
         </tr>
         <tr>
            <td>9:10-9:20am</td>
            <td>Sponsor Acknowledgements</td>
         </tr>
         <tr>
            <td>9:20-10:10</td>
            <td><strong>Keynote1: </strong>
              D. Richard Hipp (SQLite).  <i>SQLite, The Database at the Edge of the Network</i><br/>
               <a href="javascript: toggleVisibility ('#keynote1A')">Click to toggle bio.</a>
               <div id="keynote1A" class="abstract" style="display: none;">
                  <h3>Abstract</h3>
                  <p>
                  SQLite is the mostly widely deployed and used database engine in the
                  world.  But it is also, perhaps, the most widely misunderstood.
                  Unlike most other database engines that are designed to run in the
                  datacenter, SQLite is an embedded database engine designed for use on
                  devices at the edge of the network.  Consequently, SQLite embodies
                  several non-traditional and sometimes controversial design choices.

                  This talk will overview the history of SQLite and describe its unusual
                  use-case.  The talk will also summarize the design and implementation
                  of SQLite, with emphasis on those aspects of SQLite that differ from
                  common practice and on lessons learned from supporting SQLite for over
                  15 years.</p>
                  <h3>Bio</h3>
                  <p>
                  Richard Hipp is the creator of SQLite and the technical lead for a
                  small team of developers supporting and enhancing SQLite and related
                  software for companies around the world.  Richard is also the creator
                  and lead maintainer for the Fossil distributed version control system.
                  Richard is the founder and Head of Research for Hipp, Wyrick &
                  Company, Inc., a software consultancy that employs the key SQLite
                  committers.  Richard holds degrees from Duke University (Ph.D. 1992)
                  and Georgia Tech (MSEE, 1984) and currently resides in Charlotte, NC.
                  </p>
               </div>
            </td>
         </tr>
         <tr>
            <td>10:10-10:30</td>
            <td>Coffee Break</td>
         </tr>
         <tr class="success">
            <td colspan="2" align="center"><strong>Session 1: Interesting Topics</strong></td>
         </tr>
         <tr>
            <td>10:30-10:45</td>
            <td>Peter Bailis (Stanford) <i>Outlier Detection and Explanation in MacroBase, a Macroscope for the Internet of Things</i> 
            <br/>
            <a href="javascript: toggleVisibility ('#pa1A')">Click to toggle abstract.</a> <!--| <a href="./pdf/Paper40.pdf" target="_new">Click to view full paper.</a>-->
               <div id="pa1A" class="abstract" style="display: none;">
                  <p>
                  An increasing proportion of data today is generated by automated processes, sensors, and systems -- collectively, the Internet of Things (IoT). Efficiently analyzing and understanding IoT data at scale is remarkably challenging. However, as a result of its automated origins, the highly structured and regular nature of IoT data in turn enables a new class of optimizations for data management systems.
                  </p>
                  <p>
                  To address this opportunity, we have developed MacroBase, a system for large-scale, user-friendly IoT data management. MacroBase provides turn-key analysis of IoT data streams by automatically recognizing and summarizing interesting patterns and trends -- in effect, executing a complex outlier detection, explanation, and ranking pipeline. To facilitate efficient, accurate operation, MacroBase implements cross-layer optimizations across robust estimation, pattern mining, and sketching procedures. As a result, MacroBase can analyze over 300K events per second -- on a single core.
                  </p>
               </div>
             </td>
         </tr>
         <tr>
            <td>10:45-11:00</td>
            <td>Kamil Bajda-Pawlikowski (Teradata) <i>Architecture of Presto, an Open Source SQL Engine</i> 
            <br/>
            <a href="javascript: toggleVisibility ('#pa1B')">Click to toggle abstract.</a> 
               <div id="pa1B" class="abstract" style="display: none;">
                  <p>
                  Presto is an open source distributed SQL engine, originally developed by Facebook for their massive Hadoop data warehouse. Earlier this year, Teradata has joined the Presto community and announced a multi-year roadmap to accelerate Presto development to make it ready for enterprise users. This talk will cover Presto architecture and discuss the technology behind the project.
                  </p>
               </div>
             </td>
         </tr>
         <tr>
            <td>11:00-11:15</td>
            <td>Rodica Neamtu (WPI) <i>Interactive Online Exploration of Time Series Data sets</i> 
            <br/>
            <a href="javascript: toggleVisibility ('#pa1C')">Click to toggle abstract.</a> 
               <div id="pa1C" class="abstract" style="display: none;">
                  <p>
                  Analyzing similarities in time series data is critical for many applications ranging from financial planning to policy making. We propose a novel paradigm called Online Exploration of Time Series or ONEX that by leveraging a powerful one-time pre-processing step builds a hierarchical abstract model to achieve interactive time series analytics.
                  <p>
                  The pre-processing step compresses the raw data into a knowledge base encoding critical similarity relationships among time series. ONEX enables further exploration of these relationships and establishes a similarity-centric panorama of time series data sets. We theoretically and empirically compare the ONEX efficiency and query response times to other techniques based on Dynamic Time Warping and embedding-based frameworks for subsequence matching. In addition to being competitive with and in most cases faster than other methods, our approach offers a rich set of novel exploratory mining requests giving users deep insights into the data sets.
                  </p>
               </div>
             </td>
         </tr>
         <tr>
            <td>11:15-11:30</td>
            <td> Yongjoo Park (U Mich) <i>Database Learning: Towards a database that becomes smarter every time</i> 
            <br/>
            <a href="javascript: toggleVisibility ('#pa1D')">Click to toggle abstract.</a> 
               <div id="pa1D" class="abstract" style="display: none;">
                  <p>
                  Database learning is the first database system that uses its internal model for processing time-sensitive approximate analytic SQL queries, where the internal model is learned from past queries and their computed answers. Compared to view-based approaches, model-driven query answering mechanism enables flexible reuse of past computations, providing great improvements over sample-based approximate database systems. In this talk, we present database learning's system overview, technical contributions, and experimental results using TPC-H benchmark datasets.
                  </p>
               </div>
             </td>
         </tr>

         <tr>
            <td>11:30-11:45</td>
            <td>Michael R Anderson (U Mich) <i>Faster Feature Engineering by Approximate Evaluation </i> 
            <br/>
            <a href="javascript: toggleVisibility ('#pa1E')">Click to toggle abstract.</a> 
               <div id="pa1E" class="abstract" style="display: none;">
                  <p>
                  The application of machine learning to large datasets has become a vital component of many important and sophisticated software systems built today. Such trained systems are often based on supervised learning tasks that require features, or extracted signals that distill complicated raw data objects into a small number of salient values. A trained system's success depends on the quality of its features.
                  <p>
                  Unfortunately, feature engineering—writing code that turns raw data objects into feature vectors suitable for a machine learning algorithm—is tedious and time-consuming. Because big data inputs are so diverse, feature engineering is a trial-and-error process with many small, iterative code changes. Because the inputs are so large, each code change can involve time-consuming data processing. We introduce Zombie, a data-centric system that accelerates feature engineering through intelligent input selection, optimizing the inner loop of the feature engineering process.
                  </p>
               </div>
             </td>
         </tr>



         <tr>
            <td colspan="2">&nbsp;</td>
         </tr>



         <tr>
            <td>11:50-12:50</td>
            <td><strong>Lunch (Outside 32-123)</strong></td>
         </tr>
         <tr>
            <td colspan="2">&nbsp;</td>
         </tr>
         <tr>
            <td>1:00-1:50</td>
            <td><strong>Keynote2: </strong>
               Le Cong (MIT). 
               <i>Novel Genome Engineering Tools Based on CRISPR-Cas 
               System and Their Application and Intersection with Genomics Analysis
                </i> <br/>
                <a href="javascript: toggleVisibility ('#keynote2A')">Click to toggle bio.</a>
               <div id="keynote2A" class="abstract" style="display: none;">
                  <h3>Bio</h3>
                  <p>
                  Dr. Le Cong obtained his B.S. with highest honor from Tsinghua University, his Ph.D. from Harvard University, and is currently a Postdoctoral Associate at the Broad Institute of MIT and Harvard. He completed thesis work mainly in Dr. Feng Zhang’s lab, where he did pioneering studies on genetic and epigenetic engineering, focusing on developing technologies based on TALE and CRISPR-Cas systems and adapting them for gene therapy applications. His work was the first to demonstrate CRISPR-Cas9 genome editing in eukaryotic cells (Cong et al. Science. 2013), and is co-inventor of multiple patents on related technologies. He is now working on single-cell genomics and systems biology in Dr. Aviv Regev's group. He has published in many high-impact journals including Science, Nature, Cell, Nature Biotechnology, and written for Springer’s Book Series Neuromethods and Methods in Molecular Biology. He is recipient of multiple awards and fellowships including the HHMI International Student Research Fellowship, the CRI Irvington Postdoctoral Fellowships Award, among others.
                  </p>
               </div>
            </td>
         </tr>










         <tr class="success">
            <td colspan="2" align="center"><strong>Session 2: Graphs and DB Design</strong></td>
         </tr>
         <tr>
            <td>2:00-2:15</td>
            <td>Xiangyao Yu (MIT) <i>TicToc: Time Traveling Optimistic Concurrency Control </i> 
            <br/>
            <a href="javascript: toggleVisibility ('#pa2A')">Click to toggle abstract.</a> 
               <div id="pa2A" class="abstract" style="display: none;">
                  <p>
                  </p>
               </div>
             </td>
         </tr>

         <tr>
            <td>2:15-2:30</td>
            <td>Jose M Faleiro (Yale) <i>Multi-versioning in Main-memory Databases: Limitations and Opportunities</i> 
            <br/>
            <a href="javascript: toggleVisibility ('#pa2B')">Click to toggle abstract.</a> 
               <div id="pa2B" class="abstract" style="display: none;">
                  <p>
                  </p>
               </div>
             </td>
         </tr>

         <tr>
            <td>2:30-2:45</td>
            <td>Hideaki Kimura (HP) <i>FOEDUS: Transaction Processing on 1,000 Cores and NVRAM </i> 
            <br/>
            <a href="javascript: toggleVisibility ('#pa2C')">Click to toggle abstract.</a> 
               <div id="pa2C" class="abstract" style="display: none;">
                  <p>
                  This talk describes two projects at Hewlett Packard Enterprise.
                  <p>

                  The first part will summarize \textbf{The Machine}~\cite{tm},
                  an ambitious project for which HP has been devoting
                  75\% of its research activity to redefine
                  hardware/software stacks in future servers.
                  <p>

                  The second part will present \textbf{FOEDUS}~\cite{foedus},
                  an OLTP engine developed from scratch for 1,000 cores and NVRAM,
                  which achieved 17 million TPC-C TPS (100x of the current world record).
                  <p>

                  Both projects would be of great interest to the NEDB community.
                  The talk will call for involvement from academia.
                  </p>
               </div>
             </td>
         </tr>

         <tr>
            <td>2:45-3:00</td>
            <td>Tim Kraska (Brown) <i>Making Distributed Transaction Scale </i> 
            <br/>
            <a href="javascript: toggleVisibility ('#pa2D')">Click to toggle abstract.</a> 
               <div id="pa2D" class="abstract" style="display: none;">
                  <p>
                  Common wisdom is, that distributed transactions do not
                  scale. In this talk, we will first show that this
                  used to be true as the network was a major bottleneck. 
                  However, with the next generation
                  of high-performance RDMA-capable networks this
                  no longer holds true. RDMA significantly reduces the network
                  overhead per message and the new Infiniband standards
                  allow to achieve network transfer rates similar to the
                  main memory bandwidth. Yet, simply upgrading an existing
                  database cluster with Infiniband without redesigning
                  the system does not necessarily improve its performance. In
                  fact, the performance can degrade. In this talk, we show
                  why this is the case and present a new database system for
                  transaction processing over high-speed RDMA-capable networks.
                  Our earlier experiments show, that our new design is
                  able to achieve almost 2 million distributed transactions
                  per second over an 8-node clusters compared roughly 30,000
                  distributed transaction using more traditional transaction
                  approaches.
                  </p>
               </div>
             </td>
         </tr>


         <tr>
            <td>3:00-3:10</td>
            <td>Coffee Break</td>
         </tr>
         <tr class="success">
            <td colspan="2" align="center"><strong>Session 3: Big Data and Real-time Analytics</strong></td>
         </tr>
         <tr>
            <td>3:15-3:30</td>
            <td>Vikash Mansinghka (MIT) <i>Query the probable implications of statistical data with BayesDB </i> 
            <br/>
            <a href="javascript: toggleVisibility ('#pa3A)">Click to toggle abstract.</a> 
               <div id="pa3A class="abstract" style="display: none;">
                  <p>
                  Is it possible to make statistical inference broadly accessible to non-statisticians without sacrificing mathematical rigor or inference quality? Here we present BayesDB, a probabilistic programming platform that aims to enable users to query the probable implications of their data as directly as SQL databases enable them to query the data itself. BayesDB has been applied to diverse problems involving public and proprietary data, including the diagnosis and treatment of malnutrition, in collaboration with the Bill & Melinda Gates Foundation, and analysis of wire fraud and fixed income trading, in collaboration with JP Morgan. Here we sketch two applications on public data sources: cleaning and exploring an open-access database of Earth satellites and analyzing a salary survey.
                  </p>
               </div>
             </td>
         </tr>

         <tr>
            <td>3:30-3:45</td>
            <td>Michael Gubanov (UT San Antonio) <i>Hybrid: A Large-scale Linear-Relational Database Management System</i> 
            <br/>
            <a href="javascript: toggleVisibility ('#pa3B')">Click to toggle abstract.</a> 
               <div id="pa3B class="abstract" style="display: none;">
                  <p>
                  Natively supporting linear algebra inside of a database system provides important abstractions for modern analytics-matrices and vectors-and at the same time avoids the ``move-your-data nightmare''. A database programmer can implement complex algorithms using these abstractions, yielding both improved productivity and significant performance gains. 
                  <p>
                  Unfortunately, no existing RDBMS has native support for matrix and vector data types. Array databases largely forgo the relational model. Solutions such as Matlab/R are difficult to scale and require prohibitive data movement conversion costs.
                  <p>
                  Here we describe and evaluate our work in progress on a scalable system that natively supports both relational and linear algebras. It is built on top of SimSQL - a distributed stochastic analytics engine. Its optimizer enables large-scale linear-relational computation by choosing a minimal-cost plan. Our experiments justify its critical role at scale and demonstrate significant performance gains.
                  </p>
               </div>
             </td>
         </tr>

         <tr>
            <td>3:45-4:00</td>
            <td> Siddharth Samsi (Lincoln Labs) <i>D4M and Large Array Databases for Management and Analysis of Large Biomedical Imaging Data </i> 
            <br/>
            <a href="javascript: toggleVisibility ('#pa3C')">Click to toggle abstract.</a> 
               <div id="pa3C" class="abstract" style="display: none;">
                  <p>
                  In this paper, we will present a new set of tools that leverage the D4M (Dynamic Distributed Dimensional Data Model) toolbox for analyzing giga-voxel biomedical datasets. By combining SciDB and the D4M toolbox, we demonstrate that we can access large volumetric data and perform large-scale bioinformatics analytics efficiently and interactively. We show that it is possible to achieve an ingest rate of 2.8 million entries per second for importing large datasets into SciDB. These tools provide more efficient ways to access random sub-volumes of massive datasets and to process the information that typically cannot be loaded into memory. This work describes the D4M and SciDB tools that we developed and presents the initial performance results.
                  </p>
               </div>
             </td>
         </tr>

         <tr>
            <td>4:00-4:15</td>
            <td>Paul Brown (Paradigm4) <i>Convolution is a Database Problem!  </i> 
            <br/>
            <a href="javascript: toggleVisibility ('#pa3D')">Click to toggle abstract.</a> 
               <div id="pa3D" class="abstract" style="display: none;">
                  <p>
                  Among the most exciting developments in contemporary
                  software are systems for the processing and analysis of very
                  large image collections [16]. In this talk, we will propose
                  that integrating these methods with scalable and reliable
                  data management is an important challenge in DBMS research
                  and development. We provide a brief introduction to
                  some of these algorithms and review two inter-related problem
                  areas; image processing, and deep learning. Then we
                  move on to explain the role data management has to play
                  in bringing these techniques out of narrow and relatively
                  specialized domains. Our investigation of these use cases
                  further motivates the inclusion of matrix operations within
                  a scalable DBMS platform.
                  </p>
               </div>
             </td>
         </tr>

         <tr>
            <td>4:15-4:30</td>
            <td>Vijay Gadepally (MIT) <i>The BigDAWG Architecture </i> 
            <br/>
            <a href="javascript: toggleVisibility ('#pa3E')">Click to toggle abstract.</a> 
               <div id="pa3E" class="abstract" style="display: none;">
                  <p>
                  BigDAWG is a polystore system designed to work on complex problems that naturally span across different processing or storage engines. BigDAWG provides an architecture that supports diverse database systems working with different data models, support for the competing notions of location transparency and semantic completeness via islands of information and a middleware that provides a uniform multi-island interface. In this article, we describe the current architecture of BigDAWG, its application on the MIMIC II medical dataset, and our plans for the mechanics of cross-system queries. During the presentation, we will also deliver a brief demonstration of the current version of BigDAWG.
                  </p>
               </div>
             </td>
         </tr>

         <tr>
            <td colspan="2">&nbsp;</td>
         </tr>
         <tr>
            <td>4:30 PM</td>
            <td><a href="#posters"><strong>Poster Session</strong></a> and Appetizers / Drinks (Building 32, R&D Commons, 4th Floor, Gates Tower)</td>
         </tr>
         <tr>
            <td>6:30 PM</td>
            <td>Adjourn</td>
          </tr>
      </tbody>
   </table>
</div>
