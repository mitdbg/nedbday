<div class="content-container">
   <h1>
         North East Database Day 2018 <br />
         <small>Friday January 19th, 2018</small><br/>
         <small>MIT CSAIL Building 32 Room 123</small>
   </h1>
   <p>The poster session will include drinks and appetizers. It will be held in Building 32, R&D Commons, 4th Floor, Gates Tower. Take either set of elevators to the 4th floor.</p>
   <h3>Information for Poster Presenters</h3>
   <p>We will be able to provide poster board, easels, and mounting supplies for you.
      You will need to print your poster and bring it with you to the conference. We recommend posters to be either A0, A1, or ANSI D or E sizes  (either 24" by 32" or 36" by 42"). 
      Our poster boards are large enough to accommodate any of these (4' x 6'). 
      You may orient your poster as you see fit. If you prefer to print out individual 8.5" x 11" pages, our board will be large enough to 
      accommodate about 12 pages. However, we recommend making a single poster -- if you don't have access to a large format printer Fedex/Kinkos can print your poster for you.
   </p>
   <p>We will have a storage area for poster tubes at the conference. You will also be able to set up your posters from 8:00 AM Friday morning, if you would like to put them up early.</p>
   <h3>List of accepted posters (in addition to posters presented for talks):</h3>

<table class="table table-bordered posterTable">
<tbody>



<tr>
<td>
Andrew Crotty*, Brown University; Alex Galakatos, Brown University; Tim Kraska, MIT
<br/>
<strong>
MgBench: A Benchmark for Systems that Manage Machine-generated Data
</strong><br/>
<a href="javascript: toggleVisibility ('#po1')">Toggle Abstract</a>
<div id="po1" class="abstract" style="display: none;">
<p>
We are currently in the midst of an explosion of growth in the number of data-generating devices, ranging from simple wearable technology to complex autonomous vehicles. These devices are producing massive volumes of machine-generated data at unprecedented rates, but how well do we understand the unique challenges and opportunities surrounding this emerging data management problem? Through extensive conversations with a wide variety of industry professionals, we identified the key characteristics of these workloads. Based on these conversations, we analyze three case studies containing real-world data and queries that we believe are representative of common machine-generated data analysis tasks. These case studies form the basis of MgBench, a new benchmark for systems that manage machine-generated data. Using this benchmark, we are evaluating how well existing solutions perform under this realistic and diverse set of workloads.
</p>
</div>
</td>
</tr>


<tr>
<td>
Xiao Qin*, Worcester Polytechnic Institute; Lei Cao, MIT; Elke Rundensteiner, WPI; Samuel Madden, MIT
<br/>
<strong>
Kernel Density Estimation-based Local Outlier Detection over Large Data Streams
</strong><br/>
<a href="javascript: toggleVisibility ('#po2')">Toggle Abstract</a>
<div id="po2" class="abstract" style="display: none;">
<p>
Local outlier techniques have been proven to be effective for detecting outliers in skewed datasets. However, existing methods are not well equipped to support modern high-velocity data streams due to the high complexity detection algorithms and their volatility to data updates. To tackle these shortcomings, we propose new local outlier semantics that leverage kernel density estimation (KDE) to effectively detect outliers from streaming data. A strategy to continuously detect top-N KDE-based local outliers over streams is also designed, called KELOS - the first linear time complexity streaming local outlier detection approach. The experimental evaluation demonstrates that KELOS is up to 3 orders of magnitude faster than the alternative solutions, while still being highly effective in detecting local outliers from streaming data.
</p>
</div>
</td>
</tr>


<tr>
<td>
Ramoza Ahsan*, WPI; Muzammil Bashir, worcester polytechnic instituite; Rodica Neamtu, Worcester Polytechnic Institute; Elke Rundensteiner, WPI; Gabor Sarkozy, worcester polytechnic instituite
<br/>
<strong>
Correlation-based Time Series Analytics
</strong><br/>
<a href="javascript: toggleVisibility ('#po3')">Toggle Abstract</a>
<div id="po3" class="abstract" style="display: none;">
<p>
Correlation analytics among time series data is critical for a wide range of applications from stock analysis to weather forecasting. Yet correlation analytics, especially when searching for correlations at the granularity of subsequences, is known to be prohibitively costly for large data sets. Our proposed framework, called CORAL tackles this challenge by adopting a preprocess-once and query-many-times paradigm. We develop a formal model that establishes a mapping between the popular non-metric Pearson correlation with the metric Euclidean distance. This allows CORAL to compress the raw time series into Euclidean-based clusters taking advantage of the ED triangle inequality. These clusters are augmented by a compact overlay graph encoding correlation relationships to support a variety of operations. Experimental evaluation on real datasets demonstrate that CORAL is several times faster than the state-of-the-art systems while still rendering high-accuracy results.
</p>
</div>
</td>
</tr>


<tr>
<td>
Yizhou Yan*, Worcester Polytechnic Institute; Lei Cao, MIT; Samuel Madden, MIT; Elke Rundensteiner, WPI
<br/>
<strong>
SWIFT: Mining Representative Patterns from Large Event Streams
</strong><br/>
<a href="javascript: toggleVisibility ('#po4')">Toggle Abstract</a>
<div id="po4" class="abstract" style="display: none;">
<p>
Modern Internet of Things (IoT) applications generate large volume, high velocity event streams. In this paper, we present a new approach called SWIFT to efficiently discover a compact set of representative patterns from such IoT event streams for the continuous monitoring of typical and abnormal system behaviors. First, SWIFT features a new stream pattern mining semantics. Second, we design the SWIFT algorithm that continuously produces this representative pattern set upon event arrival. We also develop the batch update strategy, called B-SWIFT, that scales to high velocity event streams. Third, SWIFT compactly maintains the patterns mined in the history by leveraging the multiple time granularity tilted-time window. Therefore SWIFT efficiently supports exploratory analytics requests essential for monitoring the changes of patterns and their trends. Our experimental evaluation demonstrates the effectiveness and its efficiency in handling large-volume high-velocity event streams.
</p>
</div>
</td>
</tr>


<tr>
<td>
Yeounoh Chung*, Brown university; Emanuel Zgraggen, Brown University; Zeyuan Shang, Brown University; Benedetto Buratti, Brown University; Robert Zeleznik, Brown University; Tim Kraska, MIT; Carsten Binnig, TU Darmstadt; Eli Upfal, Brown University; Dylan Ebert, Brown University; Andries van Dam, Brown University
<br/>
<strong>
QuIC-M: A System for Quality-Aware Interactive Curation of Models
</strong><br/>
<a href="javascript: toggleVisibility ('#po5')">Toggle Abstract</a>
<div id="po5" class="abstract" style="display: none;">
<p>
Democratizing Data Science requires a fundamental rethinking of the way data analytics and model discovery is done. Available tools for analyzing massive data sets and curating machine learning models are limited in a number of fundamental ways. First, existing tools require well-trained data scientists to select the appropriate techniques to build models and to evaluate their outcomes. Second, existing tools require heavy data preparation steps and are often too slow to give interactive feedback to domain experts int he model building process, severely limiting the possible interactions. Third, current tools do not provide adequate analysis of statistical risk factors int he model development. In this work, we present QuIC-M, an interactive human-in-the-loop data exploration and model building suite. The goal is to enable domain experts to build models an order of magnitude faster than an ML expert, with a model quality comparable to the expert solution.
</p>
</div>
</td>
</tr>


<tr>
<td>
Oscar Moll*, MIT; Samuel Madden, MIT; Michael Stonebraker, MIT; Vijay Gadepally, MIT Lincoln Laboratory
<br/>
<strong>
Learning to sample moving camera videos
</strong><br/>
<a href="javascript: toggleVisibility ('#po6')">Toggle Abstract</a>
<div id="po6" class="abstract" style="display: none;">
<p>
Moving cameras such as dashboard cameras and camera drones can collect large amounts of video data in a short time. Thanks to recent advances in computer vision, it is possible to automate some analyses of this new data source. For example, a city may want to make an inventory of street sign locations and traffic lights using video from moving vehicles. Despite advances in vision technology, data volume and processing cost stand in the way of democratizing mining such datasets. In this project we leverage the time continuous nature of the data as well as the multimodality of the sensors used, to generate data skipping strategies. We demonstrate the data skipping savings enabled by this technique on a traffic light mapping task using dash cam data.
</p>
</div>
</td>
</tr>


<tr>
<td>
Chunyao Song, UMass Lowell; Xuanming Liu, UMass Lowell; Tingjian Ge*, University of Massachusetts, Lowell
<br/>
<strong>
Top-k Frequent Items and Item Frequency Tracking over Sliding Windows of Any Sizes
</strong><br/>
<a href="javascript: toggleVisibility ('#po7')">Toggle Abstract</a>
<div id="po7" class="abstract" style="display: none;">
<p>
Many big data applications today require querying highly dynamic and large-scale data streams for top-k frequent items in the most recent window of any specified size at any time. This is a challenging problem. We show that our novel solution is not only accurate, but it also one to two orders of magnitude faster than previous approaches. Moreover, its memory footprint grows only logarithmically with the window size, rather than linearly as in previous work. Our comprehensive experiments over real-world datasets show that our solution is very effective and scalable. In addition, we devise a concise and efficient solution to a related problem of tracking the frequency of selected items, improving upon previous work by twenty to thirty times in model conciseness while providing the same accuracy and efficiency.
</p>
</div>
</td>
</tr>


<tr>
<td>
Abhishek Roy*, University of Massachusetts Amherst
<br/>
<strong>
Massively Parallel Processing of Whole Genome Sequence Data
</strong><br/>
<a href="javascript: toggleVisibility ('#po8')">Toggle Abstract</a>
<div id="po8" class="abstract" style="display: none;">
<p>
This work presents a joint effort between a group of computer scientists and bioinformaticians towards a general big data platform for genome analysis pipelines. Our platform, called Gesall, is based on the new Wrapper Technology that supports existing genomic data analysis programs in their native forms, without having to rewrite them. To do so, our system provides several layers of software, including a new Genome Data Parallel Toolkit (GDPT), which can be used to "wrap" existing data analysis programs. This platform offers a concrete context for evaluating big data technology for genomics: we report on super-linear speedup and sublinear speedup for various tasks, as well as the reasons why a parallel program could produce different results from those of a serial program.
</p>
</div>
</td>
</tr>


<tr>
<td>
Tabassum Kakar*, Worcester Polytechnic Institute; Xiao Qin, WPI; Andrew Schade, Worcester Polytechnic Institute; Brian Zylich, Worcester Polytechnic Institute; Brian McCarthy, Worcester Polytechnic Institute; Huy Quoc Tran, Worcester Polytechnic Institute; Elke Rundensteiner, WPI; Lane Harrison, Worcester Polytechnic Institute
<br/>
<strong>
DIVA: Towards Validation of Hypothesized Drug-Drug Interactions via Visual Analysis
</strong><br/>
<a href="javascript: toggleVisibility ('#po9')">Toggle Abstract</a>
<div id="po9" class="abstract" style="display: none;">
<p>
Severe adverse drug reactions (ADRs) caused by drug-drug interactions are a major public health concern. Currently, drug-drug interaction related adverse reaction (DIAR) signals are detected manually by reviewing ADR reports collected via post-marketing drug surveillance systems. Sifting through hundreds of daily ADR reports poses challenge on drug safety analysts who attempt to form and validate a hypothesis about a DIAR signal. Candidate DIAR signals generated by data mining techniques are huge in size and need human intervention to be validated to be a true signal warranting action. In this work, we propose a visual analytics tool, DIVA, that aligns with the drug safety analysts’ workflow of signal screening and detection to support them to systematically generate, screen and verify hypothesised DIAR signals. Both our case studies and drug analysts’ interviews demonstrate the effectiveness of DIVA in supporting in the exploration and analysis of new critical DIAR signals.
</p>
</div>
</td>
</tr>


<tr>
<td>
Yeounoh Chung*, Brown university; Tim Kraska, MIT; Steven Whang, Google Research; Neoklis Polyzotis, Google
<br/>
<strong>
SliceFinder: Automated Data Slicing for Model Debugging
</strong><br/>
<a href="javascript: toggleVisibility ('#po10')">Toggle Abstract</a>
<div id="po10" class="abstract" style="display: none;">
<p>
As machine learning (ML) systems become democratized, helping users easily debug their models becomes increasingly important. Yet current data tools are still primitive when it comes to helping users trace model performance problems all the way to the data. We focus on the particular problem of data slicing, which identifies subsets of the training data where the model performs poorly. Unlike general techniques (e.g., clustering) that can find arbitrary slices, our goal is to find interpretable slices (which are easier to take action compared to arbitrary subsets) that are problematic and large. We propose SliceFinder, which is an interactive framework for identifying such slices using statistical techniques. The slices can be used to diagnose model fairness and fraud detection where intuitively describing slices is necessary.
</p>
</div>
</td>
</tr>


<tr>
<td>
Yoshinori Matsunobu*, Facebook; Herman Lee, Facebook
<br/>
<strong>
MyRocks -- Space and Write optimized MySQL database
</strong><br/>
<a href="javascript: toggleVisibility ('#po11')">Toggle Abstract</a>
<div id="po11" class="abstract" style="display: none;">
<p>
Last year, we introduced MyRocks, our new MySQL database engine, with the goal of improving space and write efficiency beyond what was possible with compressed InnoDB. Our objective was to migrate one of our main databases (UDB) from compressed InnoDB to MyRocks and reduce the amount of storage and number of servers used by half. We carefully planned and implemented the migration from InnoDB to MyRocks for our UDB tier, which manages data about Facebook’s social graph. We completed the migration last August, and successfully cut our storage usage in half. This poster describes MyRocks overview, comparison to InnoDB, and future plans.
</p>
</div>
</td>
</tr>


<tr>
<td>
Ryan Marcus*, Brandeis University; Olga Papaemmanouil, Brandeis University
<br/>
<strong>
An End-to-End Economic Method for Cloud Database Fragmentation, Replication, and Provisioning
</strong><br/>
<a href="javascript: toggleVisibility ('#po12')">Toggle Abstract</a>
<div id="po12" class="abstract" style="display: none;">
<p>
Distributed data management systems often operate on "elastic" clusters that can scale up/down on demand. These systems face numerous challenges, including data fragmentation, replication, and cluster sizing. This poster showcases NashDB, a data distribution and provisioning framework that relies on an economic model to automatically balance the supply and demand of resources like fragments, replicas, and cluster nodes. NashDB aims to adapt its decisions to query priorities and shifting workloads, while avoiding underutilized cluster nodes and redundant replicas. This poster demonstrates how NashDB identifies configurations that balance resource demand to resource usage cost, and transitions the DBMS to new configurations with minimum data transfer overhead.
</p>
</div>
</td>
</tr>


<tr>
<td>
Ligia Nistor*, Oracle
<br/>
<strong>
Apache Spark as a back end for a distributed query engine (poster)
</strong><br/>
<a href="javascript: toggleVisibility ('#po13')">Toggle Abstract</a>
<div id="po13" class="abstract" style="display: none;">
<p>
This abstract describes the setup of a cluster of 3 machines that use Hadoop, Yarn, Apache Spark and spark-jobserver to run distributed queries on a dataset of 1 million records and the results that I got. spark-jobserver For each query being run in Apache Spark there is a startup delay of about 20 seconds. For our scenario of using Spark SQL as the back end, we could have a long running background job in Spark and then we won't have to wait the 20 seconds for each query. We used spark-jobserver for this. The advantages are that it is open source, it runs jobs in their own Contexts or share 1 context among jobs. I ran 3 Explore queries on a cluster of 3 powerful machines with spark-jobserver installed on one of them, on a 1 million records dataset. I ran each of the 3 queries (one example of such a query is SELECT COUNT(DISTINCT geocode_geo_city) AS "geocode_geo_city" FROM taxi) a few times and the results are going to be presented in the poster.
</p>
</div>
</td>
</tr>


<tr>
<td>
Cuong Nguyen*, WPI; Charles Lovering, WPI; Rodica Neamtu, Worcester Polytechnic Institute
<br/>
<strong>
Ranked Time Series Matching by Interleaving Similarity Distances
</strong><br/>
<a href="javascript: toggleVisibility ('#po14')">Toggle Abstract</a>
<div id="po14" class="abstract" style="display: none;">
<p>
Our proposed method K-ONEX, uses a preprocess-once and query-many-times paradigm that interleaves the inexpensive Euclidean distance with the robust Dynamic Time Warping (DTW), to retrieve the k most similar matches to a given sample sequence. We first reduce the data cardinality by generating Euclidean-based groups of similar time series. Then these groups are further explored using the elastic DTW to find similar sequences of any length and temporal alignment. Our evaluation shows that our framework is 2-3 orders of magnitude faster than benchmark methods while achieving 100% accuracy by exploring on average less than 0.5% of the sequences in each dataset.
</p>
</div>
</td>
</tr>


<tr>
<td>
Niv Dayan*, Harvard University; Manos Athanassoulis, Harvard SEAS; Stratos Idreos, Harvard
<br/>
<strong>
Monkey: Optimal Navigable Key-Value Store
</strong><br/>
<a href="javascript: toggleVisibility ('#po15')">Toggle Abstract</a>
<div id="po15" class="abstract" style="display: none;">
<p>
We present Monkey, an LSM-based key-value store that strikes the optimal balance between the costs of updates and lookups with any given main memory budget. The insight is that worst-case lookup cost is proportional to the sum of the false positive rates of the Bloom filters across all levels of the LSM-tree. Monkey allocates memory to filters across different levels so as to minimize this sum. We show analytically that Monkey reduces the asymptotic complexity of the worst-case lookup I/O cost, and we verify empirically using an implementation on top of LevelDB that Monkey reduces lookup latency by an increasing margin as the data volume grows. Furthermore, we map the LSM-tree design space onto a closed-form model that enables co-tuning the merge policy, the buffer size and the filters' false positive rates to trade among lookup cost, update cost and/or main memory, depending on the workload, the dataset, and the underlying hardware.
</p>
</div>
</td>
</tr>


<tr>
<td>
Marcel Kost*, Carnegie Mellon University / Karlsruhe Institute of Technology
<br/>
<strong>
Using an Interpreter to Hide the Latency of Query Compilation in an In-Memory Database Managment System
</strong><br/>
<a href="javascript: toggleVisibility ('#po16')">Toggle Abstract</a>
<div id="po16" class="abstract" style="display: none;">
<p>
Compiling queries on the fly speeds up execution in an in-memory DBMS by 50-100x. The compilation step itself, however, can be quite significant (20-100ms). It is a constant factor that is independent from the query runtime. This extra latency is noticeable and makes up most of the overall request time, especially for queries with a short running time. One solution to this problem is to hide this latency by immediately running the query and start processing tuples before its compilation finishes. One can achieve this by interpreting the intermediate language that was generated from the query plan, and which is fed into the compiler. This offers several advantages compared to simply walking the query plan. In this poster, we explore this option, discuss how to implement, and the problems that one must overcome in the context of the Peloton DBMS being built at Carnegie Mellon University.
</p>
</div>
</td>
</tr>


<tr>
<td>
Allison Rozet*, WPI
<br/>
<strong>
Muse: Multi-level Shared Event Trend Aggregation
</strong><br/>
<a href="javascript: toggleVisibility ('#po17')">Toggle Abstract</a>
<div id="po17" class="abstract" style="display: none;">
<p>
Stream processing applications execute large workloads of Kleene pattern queries against high-rate streams to aggregate event trends, that is, event sequences of any length. Given a workload, the state-of-the-art approach processes each query independently and misses sharing opportunities. We are the first to introduce shared processing of these complex pattern queries with optimizations at two levels: static query-level optimizations at compile-time and adaptive instance-level optimizations at runtime. Our Multi-level Shared Event Trend Aggregation (Muse) approach shares incremental trend aggregation tasks maximally among queries by uniquely exploiting “just-in-time” sharing opportunities. The Sharing Analyzer embeds sharing decision points into the Muse Runtime Substrate, which then guides runtime execution driven by dynamic stream characteristics. We present implementation details and preliminary experimental results that demonstrate the effectiveness of our approach.
</p>
</div>
</td>
</tr>


<tr>
<td>
liang zhang*, WPI; Mohamed Eltabakh, Worcester Polytechnic Institute; Elke Rundensteiner, WPI
<br/>
<strong>
TARDIS: Distributed Indexing Framework for Big Time Series Data
</strong><br/>
<a href="javascript: toggleVisibility ('#po18')">Toggle Abstract</a>
<div id="po18" class="abstract" style="display: none;">
<p>
Indexing plays a critical role in speeding up time series similarity queries on which most of time series data mining algorithms rely. However, the state-of-art techniques fall short in leveraging the full power of modern distributed systems to efficiently construct an index over billions of time series data. Here we propose TARDIS indexing framework composed of a global centralized index and a local distributed index. TARDIS efficiently supports exact-match, kNN, and range search queries over big time series data. It not only reduces the depth and the size of the index tree significantly, but also maintains the similarity relationship more effectively. Results show that over a 1.0 Billion time series, TARDIS un-clustered index is 60% faster than the state-of-the-art systems, whereas clustered index is 83% faster. Moreover, the average response time of exact-match queries is decreased by 50%, and the accuracy of the KNN-approximate queries has increased from 3% to 40%.
</p>
</div>
</td>
</tr>


<tr>
<td>
Yi Lu*, MIT; Samuel Madden, MIT
<br/>
<strong>
Building Fast, Adaptive and Highly Available Databases with Multi-master Replication
</strong><br/>
<a href="javascript: toggleVisibility ('#po19')">Toggle Abstract</a>
<div id="po19" class="abstract" style="display: none;">
<p>
In this poster, we present R-Store, a new fully replicated distributed in-memory database with multi-master replication. By replicating data on every node, but mastering each record on a different node, R-Store is able to efficiently run both highly partitionable workloads and workloads that involve many non-partitionable transactions. The key idea is a new phase-switching algorithm where cross-partition transactions are deferred, and mastership for the entire database is switched to a single node, which can execute these transactions without the use of expensive coordination protocols like two-phase commit. Because the system is fully replicated, this phase-switching can be done at almost no cost. Full-replication also allows R-Store to quickly and dynamically adapt to changing and skewed workloads by re-mastering records on the fly.
</p>
</div>
</td>
</tr>


<tr>
<td>
Leonhard Spiegelberg*, Brown University; Zeyuan Shang, Brown University; Ani Kristo, Brown University; Tim Kraska, MIT
<br/>
<strong>
How to extract, load and transform data faster?
</strong><br/>
<a href="javascript: toggleVisibility ('#po20')">Toggle Abstract</a>
<div id="po20" class="abstract" style="display: none;">
<p>
We propose a new ETL (Extract, Load, Transform) framework that enables data engineers and scientists to access data faster while developing data pipelines in a more robust way. Our vision is based on three key ideas: First, we want to optimize parsing of textual data by generating most-efficient code through various optimization tricks by lowering predicates, via data statistics and through knowledge of the full query plan. Second, we want to infer a normal case using sampling for which we explicitly optimize. Third, we want to avoid loss of computation time due to job failure by acknowledging data anomalies and being able to restore computation in updated jobs. Furthermore, we believe that easy-to-use languages like Python which provide a high-level of abstraction are the future. Hence, preliminary analysis of the data allows us to compile efficient query plans of a UDF (User Defined Functions) based workflow to native code using LLVM.
</p>
</div>
</td>
</tr>


<tr>
<td>
Ani Kristo*, Brown University; Tim Kraska, MIT
<br/>
<strong>
A compilation tool to reduce inference time for small neural networks
</strong><br/>
<a href="javascript: toggleVisibility ('#po21')">Toggle Abstract</a>
<div id="po21" class="abstract" style="display: none;">
<p>
With the increasing popularity and use of neural networks, many developers turn to TensorFlow™ to perform their deep learning-related tasks. In the context of database systems, one of the newest applications is the use of learned indexes as models that can predict the position of the records, given a lookup key. In order for them to outperform the B-tree index structures, they need to make predictions extremely fast. Therefore, we are looking into some ways of accelerating TensorFlow™ model inference by compiling the trained neural networks. The compiler supports matrix multiplication, ReLU and ReLU-like activation functions and the basic arithmetic operations with automatic broadcasting. The preliminary results demonstrate an average of 80x speedup for the entire execution process and 150x for the inference subroutine alone. This is a work in progress to enable more optimizations, target GPU accelerators and support a wider range of neural network architectures.
</p>
</div>
</td>
</tr>


<tr>
<td>
Charles Lovering*, WPI; Cuong Nguyen, WPI; Rodica Neamtu, Worcester Polytechnic Institute
<br/>
<strong>
Ranked Time Series Matching by Interleaving Similarity Distances
</strong><br/>
<a href="javascript: toggleVisibility ('#po22')">Toggle Abstract</a>
<div id="po22" class="abstract" style="display: none;">
<p>
Our proposed method K-ONEX uses a preprocess-once and query-many-times paradigm that interleaves the inexpensive Euclidean distance with the robust Dynamic Time Warping (DTW), to retrieve the k most similar matches to a given sample sequence. We first reduce the data cardinality by generating Euclidean-based groups of similar time series. Then these groups are further explored using the elastic DTW to find similar sequences of any length and temporal alignment. Our evaluation shows the our framework is 2-3 orders of magnitude faster than benchmark methods while achieving 100% accuracy by exploring on average less than 0.5% of the sequences in each dataset.
</p>
</div>
</td>
</tr>


<tr>
<td>
Xiaofeng Yang*, Northeastern University; Wolfgang Gatterbauer, Northeastern University; Mirek Riedewald, Northeastern University
<br/>
<strong>
"Any-k" Evaluation of Conjunctive Queries
</strong><br/>
<a href="javascript: toggleVisibility ('#po23')">Toggle Abstract</a>
<div id="po23" class="abstract" style="display: none;">
<p>
Conjunctive queries are common in databases and can also be used to express sub-graph patterns in large graphs. When querying big data, instead of all answers, a user might be satisfied with the (few) most important ones. Without prior knowledge on how many answers the user will request, the algorithm has to be able to incrementally return the next most relevant answers on demand. We call this an "any-k" algorithm to differentiate it from top-k, where k is given up front. In this poster, we introduce a framework for "any-k" evaluation on conjunctive queries. We propose an algorithm for acyclic queries, analyze its complexity, and show experimental results. Then we discuss the challenge to generalize it to queries with cycles, presenting preliminary results for a length-4 cycle query.
</p>
</div>
</td>
</tr>


<tr>
<td>
Vijay Gadepally*, MIT Lincoln Laboratory
<br/>
<strong>
Hyperscaling Network Analysis with D4M on the MIT SuperCloud
</strong><br/>
<a href="javascript: toggleVisibility ('#po24')">Toggle Abstract</a>
<div id="po24" class="abstract" style="display: none;">
<p>
Detecting anomalous behavior in network traffic is a major challenge due to the volume and velocity of network traffic. For example, a 10 GigE connection can generate over 50 MB/s of packet headers. For global network providers, this challenge can be amplified by many orders of magnitude. Development of novel computer network traffic analytics requires: high level programming environments, massive amount of packet capture (PCAP) data, and diverse data products for “at scale” algorithm pipeline development.
</p>
</div>
</td>
</tr>


<tr>
<td>
Hyungsik Kim*, DePaul University; Alexander Rasin, DePaul University; Robert Cole, DePaul University; Jane Clelang-Huang, University of Notre Dame
<br/>
<strong>
Query Plans for Software Traceability Analysis
</strong><br/>
<a href="javascript: toggleVisibility ('#po25')">Toggle Abstract</a>
<div id="po25" class="abstract" style="display: none;">
<p>
In safety-critical domain, overlooking the impact of change in requirements or code may cause a catastrophic event. Trace links connecting software artifacts can be utilized to ensure the quality of software, evaluate impact of a change in a component or requirements, identify necessary resources while developing or maintaining a system. However, the data characteristics of software projects diﬀer from a typical database setting – and as a result traditional query optimizers choose sub-optimal plans. In this poster, we evaluate a new strategy to represent and evaluate query execution plans for the software traceability domain. We incorporate a fanout statistic to accurately estimate join cardinality and evaluate our approach using Oracle and an H2-based prototype database we developed. Our results show that using graph-based plans and fanout join cardinality estimations generated through the algorithms we have developed, we were able to signiﬁcantly improve the resulting query plans.
</p>
</div>
</td>
</tr>


<tr>
<td>
Matthew Mucklo*, MITS
<br/>
<strong>
Ease of use and Upcoming API federation for the BigDAWG Polystore
</strong><br/>
<a href="javascript: toggleVisibility ('#po26')">Toggle Abstract</a>
<div id="po26" class="abstract" style="display: none;">
<p>
Under the direction of Vijay Gadepally of MIT's Lincoln Labs, and Professor Samuel Madden, BigDAWG is presently being extend in two ways. The first to be discussed is the Ease of Use enhancements to the BigDAWG administrative interface allowing for researchers and other users to more easily interact with the system without extensive experience with command line tools such as curl, etc. The second to be discussed is the upcoming changes to allow BigDAWG to federate data from API-based sources.
</p>
</div>
</td>
</tr>


<tr>
<td>
Michael Gubanov*, University of Texas
<br/>
<strong>
Hybrid.Poly: An Interactive Large-scale In-memory Analytical Polystore
</strong><br/>
<a href="javascript: toggleVisibility ('#po27')">Toggle Abstract</a>
<div id="po27" class="abstract" style="display: none;">
<p>
Anecdotal evidence suggests that {\em Variety} of Big data is one of the most challenging problems in Computer Science research today \cite{nist}. It is a significant impediment for Big data users who simply want to access all relevant data regardless of its shape, format, size, and a back-end system used to store it. Here we describe Hybrid.Poly -- a consolidated parallel in-memory Polystore engine and several popular analytical workloads, implemented as queries.
</p>
</div>
</td>
</tr>


<tr>
<td>
Michael Gubanov*, University of Texas
<br/>
<strong>
Hybrid.ai: An AI-Augmented Search Engine for Large-scale Structured Data
</strong><br/>
<a href="javascript: toggleVisibility ('#po28')">Toggle Abstract</a>
<div id="po28" class="abstract" style="display: none;">
<p>
Variety of Big data is a significant impediment for anyone who wants to search inside a large-scale structured dataset. For example, there are millions of tables available on the Web, but the most relevant search result does not necessarily match the keyword-query exactly due to a variety of ways to represent the same information. Here we describe and evaluate Hybrid.ai, an AI-augmented search engine for large-scale structured data that automatically clusters the corpus and routes the user query to the most relevant cluster to return the best search results. Hybrid.ai employs a new unsupervised scalable clustering algorithm that automatically generates and trains machine learning classifiers capable of recognizing similar data objects. It has broader applications beyond this search-engine to any structured data corpora.
</p>
</div>
</td>
</tr>


<tr>
<td>
Amir Rahimzadeh Ilkhechi*, Brown University; Nathaniel Weir, Brown University; P. Ajie Utama, Brown University; Carsten Binnig, TU Darmstadt; Ugur Cetintamel, Brown University
<br/>
<strong>
DBPal: Natural language-based data exploration
</strong><br/>
<a href="javascript: toggleVisibility ('#po29')">Toggle Abstract</a>
<div id="po29" class="abstract" style="display: none;">
<p>
A significant amount of data is still stored in relational databases. While structured data is easier to query and often has less ambiguity compared to unstructured data (e.g., plain text), it is often designed to be employed by technical users. In this work, we introduce DBPal which is a tool designed to be utilized by users who want to explore a given database using their natural language and visualization provided by DBPal. DBPal is designed to support only relational databases at the moment. The longterm features that are designed to be offered by DBPal include content visualization, automatic entity-relationship extraction, and natural language-based exploration. The latter is in its final stages of development and therefore, the focus of our presentation. This component includes natural language to SQL (NL2SQL) translation, text predictor, and stateful conversation handler (still under development). Our experiment results show that DBPal outperforms state-of-the-art translators.
</p>
</div>
</td>
</tr>


<tr>
<td>
Erfan Zamanian*, Brown University; Tim Kraska, Brown University; Carsten Binnig, TU Darmstadt
<br/>
<strong>
Data Partitioning for Distributed Transactions on Modern Networks
</strong><br/>
<a href="javascript: toggleVisibility ('#po30')">Toggle Abstract</a>
<div id="po30" class="abstract" style="display: none;">
<p>
The advent of new generation of networks has a profound impact on how transaction systems should be designed. We showed in a previous work that RDMA features, low latency and high bandwidth of these networks enable us to build scalable database systems that do not depend on data locality to deliver scalability and needed performance, as the communication overhead, which was previously hindering the performance of distributed databases, is now drastically reduced or completely gone. Here in this work, we analyze what locality really means in modern systems and how it can be leveraged efficiently. To this end, we will make the case that the optimal partitioning goal is shifted toward a different objective, which is minimizing contention. We propose a holistic design to data partitioning and transaction processing that aims to achieve this goal. The main intuition behind our design is to distinguish hot records from cold ones, both when partitioning and executing transactions.
</p>
</div>
</td>
</tr>


<tr>
<td>
Priya Deshpande*, DePaul University; Alexander Rasin, DePaul University
<br/>
<strong>
IRIS: Data Integration and Query Expansion Technique for Radiology Search Engine
</strong><br/>
<a href="javascript: toggleVisibility ('#po31')">Toggle Abstract</a>
<div id="po31" class="abstract" style="display: none;">
<p>
Teaching files play an important role in radiologist’s diagnosis process with many publically available sources and in-house teaching file repositories. However, these sources are highly heterogeneous and difficult to combine in practice. The Integrated Radiological Image Search (IRIS) engine integrates multiple radiology data sources into a single repository with query term augmentation using RadLex and SNOMED CT ontologies. As IRIS search produced many matches, we also integrated a search relevance algorithm. We believe that such search engine should be tailored to radiologist needs, providing an understanding of natural language such as negation statements, substituting search term synonyms, correctly interpreting adjectives and considering the structure of source text. We propose to further expand IRIS search engine by incorporating search context based search and Image based search.
</p>
</div>
</td>
</tr>


<tr>
<td>
Priya Deshpande*, DePaul University
<br/>
<strong>
Big Data Integration Requirement of Today’s Healthcare System
</strong><br/>
<a href="javascript: toggleVisibility ('#po32')">Toggle Abstract</a>
<div id="po32" class="abstract" style="display: none;">
<p>
Current situation in this digitized world generates the requirement of big data integration and analysis. Healthcare records generate petabytes of big data in a single day. The data is heterogeneous in nature, captured in different files and formats, varies from hospital to hospital. If we integrate this data and provide meaningful information to medical community, we can improve the overall quality of patient care. Our research focused on integration of health records. To start with integration of healthcare data sources, we developed Integrated Radiological Image search (IRIS) engine, which could be a good example of a publicly available data integration system in healthcare domain. IRIS provided support for medical ontologies, which would help radiologists to interpret clinical reports by considering context behind the search queries. Integrated healthcare system would help doctors to make faster, accurate and more reliable decisions about diagnosis of diseases.
</p>
</div>
</td>
</tr>


<tr>
<td>
Eric Metcalf*, Brown University; Stan Zdonik, Brown University; John Meehan, Brown University; Nesime Tatbul, Intel Labs and MIT
<br/>
<strong>
Fault Tolerance through Reliable Data Ingestion for IOT Applications
</strong><br/>
<a href="javascript: toggleVisibility ('#po33')">Toggle Abstract</a>
<div id="po33" class="abstract" style="display: none;">
<p>
In recent years, IOT applications have continued to grow and take the world by storm. With the growth of these devices that are producing data, as a society we are starting to rely heavily on the data that these devices are producing. For example, self-driving cars are collecting terabytes of data per day from multiple sensors and are using that data to make life critical decisions. At the same time the demand of the sensors is growing and these sensors need to address the following challenges including environmental challenges, signal loss, power loss, and erroneous measurements. In these situations, it is vital that data management systems are resilient to incorrect or missing data produced by the sensors. That same self-driving car could miss predict an obstacle because of an incorrect value or a missing value causing the car to crash. See the submitted file with complete abstract.
</p>
</div>
</td>
</tr>


<tr>
<td>
Dong Deng*, MIT
<br/>
<strong>
Entity Consolidation: The Golden Record Problem
</strong><br/>
<a href="javascript: toggleVisibility ('#po34')">Toggle Abstract</a>
<div id="po34" class="abstract" style="display: none;">
<p>
Entity consolidation is the process of merging a cluster of duplicate records into a “golden record”, which contains the canonical value for each attribute. In real-world, this task is driven by user-written rules. To improve the scalability, we propose a new solution. It first automatically mines candidate matching rules (substring pairs that could be replaced by each other, such as Avenue ↔ Ave) from the clusters, and then groups these rules into sets with common characteristics (such as 9 ↔ 9th and 6 ↔ 6th) to allow humans to verify them in bulk. The common characteristics are based on both the syntax of the rules and the programs that describe how the two substrings transform to each other. Next, the human-approved rule groups are applied to merge the duplicate attribute values in the clusters. We shown that by only asking a small number of yes/no questions, the solution was able to produce comparable precision to, and much higher recall than, solutions based on human-written rules
</p>
</div>
</td>
</tr>


<tr>
<td>
Eden Zik*, Vertica; Ben Vandiver, Vertica
<br/>
<strong>
Eventual Durability for Cloud Databases with Ephemeral Nodes
</strong><br/>
<a href="javascript: toggleVisibility ('#po35')">Toggle Abstract</a>
<div id="po35" class="abstract" style="display: none;">
<p>
Users of cloud databases seeking low prices often utilize “spot instances”, which are terminated as soon as market price exceeds the price paid by the user. However, their ephemeral nature poses a serious challenge for database consistency. We present a technique involving the storage of catalog metadata in a cloud object store (such as S3), which, while slower than local storage, is significantly more durable. This provides metadata persistence amidst catastrophic cluster-wide failures, which are especially common in cloud deployments utilizing spot instances.

We present a mechanism for “catalog sync” - a relaxed (and configurable) but consistent durability guarantee for database metadata (“the catalog”) that does not affect commit performance. By asynchronously persisting a catalog snapshot and its latest consistent version, we can guarantee that cluster termination will preserve all metadata up to a tolerable interval in the past, and enable a cloud database to be used in a completely ephemeral context where local storage on any node needs not be preserved past instance termination. Although the mechanism is asynchronous and performed in parallel by all nodes, the cluster-wide catalog snapshot present on shared storage is consistent with respect to each shard of the data.

Furthermore, we describe how this mechanism allows for the “revival” of a database from shared storage on a new node set without re-defining schemas or reloading data - a cluster is created by simply pointing a group of empty “promiscuous” nodes (which can be spot instances) to an existing S3 bucket containing a catalog snapshot.

Finally, we demonstrate the effectiveness of our technique using the Vertica analytic database.
</p>
</div>
</td>
</tr>


<tr>
<td>
Gourab Mitra*, University at Buffalo; Oliver Kennedy, University at Buffalo, SUNY
<br/>
<strong>
Data Synthesis for automatically generating Smartphone Database Benchmarks
</strong><br/>
<a href="javascript: toggleVisibility ('#po36')">Toggle Abstract</a>
<div id="po36" class="abstract" style="display: none;">
<p>
To build applications which evolve with changing requirements,it is important to make the correct choice of database to power them. One of the ways to do that is to use a benchmark which are domain specific. But when compared to the variety of applications that use databases, benchmarks are far less in number. It is important to characterize an application in order to prepare a relevant benchmark. An application’s database query log provides a source to infer it’s behaviour. Most benchmarking suites employ data generators to synthetically provide data. These data generators rely on a formal specification of the complete database schema. In this paper, we discuss ongoing research on design of a methodology to infer the schema from a smartphone database query log. This information could be translated into schema model to drive data generators. We envision that this method could be be used to synthetically generate data for automatic generation of benchmarks for smartphone databases.
</p>
</div>
</td>
</tr>


<tr>
<td>
William Spoth*, University at Buffalo; Oliver Kennedy, University at Buffalo, SUNY
<br/>
<strong>
MESS: Meta-data Extraction System for Schemas
</strong><br/>
<a href="javascript: toggleVisibility ('#po37')">Toggle Abstract</a>
<div id="po37" class="abstract" style="display: none;">
<p>
Data is often collected first and cleaned later as an after thought. It is common for essential information to the curation process to be embedded into file names, time stamps, or other file system meta-data. This meta-data then needs to be painstakingly incorporated during cleaning, or even worse, sometimes by another person. When the data collection process begins, users have a general sense of what their ideal data looks like, MESS is aimed at making that a reality. MESS is a structured system for extracting meta-data and imposing logical groupings of data that mimics common data collection practices directly from the source. MESS aids in the cleaning and planning process and outputs a view that would typically be the starting point for workload queries.
</p>
</div>
</td>
</tr>


<tr>
<td>
Prakhar Ojha, Indian Institute of Science; Paul Langton, Northeastern University; Wolfgang Gatterbauer, Northeastern University
<br/>
<strong>
How are classes related to each other? Scalable Compatibility Estimation in Large Network Data
</strong><br/>
<a href="javascript: toggleVisibility ('#po38')">Toggle Abstract</a>
<div id="po38" class="abstract" style="display: none;">
<p>
Belief Propagation (BP) and various linearizations have been widely used for predicting labels in network data where assumptions of label similarity between neighbors (“homophily”) do not hold. To perform the labeling, these methods rely on domain experts to specify the relative compatibilities between classes, and two open questions are: “Where do the compatibilities come from?” and “Does correctness of compatibilities actually matter for labeling accuracy?” This poster answers the ﬁrst question by developing scalable and accurate estimators that can learn the compatibilities from extremely sparsely labeled networks (e.g., < 0.1% labeled nodes) in a fraction of the time it takes to later label the remaining nodes. In other words, we suggest an end-to-end solution for within-network classiﬁcation: we estimate all necessary parameters on the same network for which we later infer labels of unlabeled nodes, and thus remove the need for prior domain knowledge.
</p>
</div>
</td>
</tr>


<tr>
<td>
Xinyan Deng*, Northeastern University; Wolfgang Gatterbauer, Northeastern University; Mirek Riedewald, Northeastern University
<br/>
<strong>
Evaluating of Conjunctive Queries with self-joins with Probabilistic Databases
</strong><br/>
<a href="javascript: toggleVisibility ('#po39')">Toggle Abstract</a>
<div id="po39" class="abstract" style="display: none;">
<p>
We focus on evaluating arbitrary conjunctive queries with probabilistic databases. While previous work showed that calculating the probability of some known-to-be #P-hard self-join free conjunctive queries can be approximated by carefully setting the probability of the dissociated expressions, the same technique does not directly work for self-join conjunctive queries. We introduce a novel way to give guaranteed lower and upper bounds of the probability of an example hard query with a standard relational database.
</p>
</div>
</td>
</tr>


<tr>
<td>
Lin Ma*, Carnegie Mellon University; Dana Van Aken, Carnegie Mellon University; Ahmed Hefny, Carnegie Mellon University; Gustavo Mezerhane, Carnegie Mellon University; Andrew Pavlo, Carnegie Mellon University; Geoff Gordon, 
<br/>
<strong>
Query-based Workload Forecasting for Self-Driving Database Management Systems
</strong><br/>
<a href="javascript: toggleVisibility ('#po40')">Toggle Abstract</a>
<div id="po40" class="abstract" style="display: none;">
<p>
The first step towards an autonomous database management system (DBMS) is the ability to model the target application’s workload. This is necessary to allow the system anticipate future workload needs and select the proper optimizations promptly. Previous forecasting techniques have focused on modeling the resource utilization of the queries. Such metrics, however, will change as the database physical design and the hardware resources change over time, thereby invalidating the forecasting result. We present a robust forecasting framework that allows a DBMS to predict the expected arrival rate of queries in the future based on historical data. We also present a clustering-based technique for reducing the total number of forecasting models that the system must train with minimal loss in accuracy. We implemented our models in an external controller for PostgreSQL and MySQL, and demonstrate their effectiveness in automatically building the optimal indexes for real-world database workloads.
</p>
</div>
</td>
</tr>


<tr>
<td>
Gokhan Kul*, University at Buffalo
<br/>
<strong>
Incoming Query Prediction on Mobile Databases with Probabilistic NFA
</strong><br/>
<a href="javascript: toggleVisibility ('#po41')">Toggle Abstract</a>
<div id="po41" class="abstract" style="display: none;">
<p>
Mobile databases are the statutory backbones of many applications on smartphones. Their performance depends on the performance of the underlying data sources. However, these databases and the querying engines in the applications are usually uncontrolled, not properly designed, and not tuned for optimal performance. In this paper, we propose a probabilistic non-deterministic automata (PNFA) based methodology for pre-fetching and caching information through incoming workload prediction. We use a given workload to construct a PNFA that models the k-length prefixes and probabilities of suffixes that follows them. Then, we use the PNFA to predict possible incoming queries based on the probabilities associated with each suffix. We believe this methodology is very suitable for mobile databases since PNFAs are easy to construct, easily modifiable to use on-the-fly, and memory efficient.
</p>
</div>
</td>
</tr>


<tr>
<td>
Philipp Eichmann*, Brown University
<br/>
<strong>
IDEBench: A Benchmark for Interactive Data Exploration Systems
</strong><br/>
<a href="javascript: toggleVisibility ('#po42')">Toggle Abstract</a>
<div id="po42" class="abstract" style="display: none;">
<p>
Traditional benchmarks are not suitable for evaluating modern interactive data exploration (IDE) systems where most queries are ad-hoc and built incrementally. With this work we present a novel benchmark called IDEBench, which includes a workload generator to simulate different exploration behaviors of users as well as a data generator that is based on real-world data sets. It defines a set of new metrics to better cover the question of how efficiently users can gain insights from a new data set. As a second contribution, we discuss the results of an extensive study of evaluating IDE systems using our benchmark. The study includes two commercial systems as well as two research prototypes and one traditional analytical database system.
</p>
</div>
</td>
</tr>


<tr>
<td>
Anil Shanbhag*, MIT
<br/>
<strong>
The next frontier for high performance analytics: GPUs
</strong><br/>
<a href="javascript: toggleVisibility ('#po43')">Toggle Abstract</a>
<div id="po43" class="abstract" style="display: none;">
<p>
In this work, we examine the true nature of the performance benefit of GPU-based database. To do so, we examine the performance of different query operators individually on GPUs to address these issues. We present performance characteristics of all basic operators like Select, Join, etc on GPU vs equivalent optimal implementations on CPU. We present novel algorithms to do join and top-k efficiently on the GPU that avoid the ’paradigm’ tax and make full use of GPU hardware. We combine these optimized implementations to build a prototype GPU-based database we call Luna. Luna run on the Star Schema benchmark can give 11x improvement in throughput and latency and be 3.5x more cost-efficient (measured as $’s to run workload).
</p>
</div>
</td>
</tr>


<tr>
<td>
Noura Alghamdi *, WPI; Abdulaziz Alajaji, WPI; Elke Rundensteiner, WPI; Mohamed Y. Eltabakh, Worcester Polytechnic Institute
<br/>
<strong>
DLSM: Distributed Long-Subsequence Matching in a Large- Scale Time Series Database.
</strong><br/>
<a href="javascript: toggleVisibility ('#po44')">Toggle Abstract</a>
<div id="po44" class="abstract" style="display: none;">
<p>
Subsequence matching in large time series databases is critical for supporting data mining from predictions to hypothesis testing. However, state-of-the-art systems are not effective at coping with tera-byte data sets composed of billions of time series. Moreover, the efficiency of these systems decreases as the length of the subsequences exceeds 128 data points. To tackle this challenge, we propose distributed solution framework, called DLSM, for indexing subsequences.
</p>
</div>
</td>
</tr>


<tr>
<td>
Andy Vidan*, Composable Analytics, Inc.
<br/>
<strong>
Curation of Live Data Sets using Dataflow Programming
</strong><br/>
<a href="javascript: toggleVisibility ('#po45')">Toggle Abstract</a>
<div id="po45" class="abstract" style="display: none;">
<p>
We describe dataflow programming and its ability to facilitate distributed data and query integration, allowing users to ingest an unlimited number of information sources in a variety of formats and integrate them using a flow-based application. In this way, entire datasets do not need to be pulled, since only aggregated or selected data can be transferred. Processes and queries can then also be sent to other data systems, rather than sending the data to the process. Throughout the talk, we will demonstrate several dataflow‐based applications using a platform called Composable Analytics, originally developed at MIT Lincoln Laboratory. Examples will showcase database queries, spreadsheet readers, web services, and other ingestion workflows for social media analysis and other applications.
</p>
</div>
</td>
</tr>





</tbody>
</table>

</div>
